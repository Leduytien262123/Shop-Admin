import { x as requireValid, y as requireSemver, z as requireSatisfies, A as requireValid$1, B as requireClean, g as getDefaultExportFromCjs } from '../shared/taze.BALSlAAn.mjs';
import require$$0 from 'path';
import require$$0$1 from 'url';
import require$$2$1 from 'stream';
import require$$1$2 from 'os';
import { r as requireLib$c, a as requireCommonjs$1, b as requireNpa, c as requireLib$d } from '../shared/taze.DrL2fBmQ.mjs';
import require$$2$2 from 'node:path';
import require$$1$3 from 'node:os';
import require$$5 from 'node:fs/promises';
import require$$0$2 from 'node:url';
import { r as requirePromiseRetry, a as requireCommonjs$2, b as requireCommonjs$3 } from '../shared/taze.BQWAtX8r.mjs';
import require$$0$5 from 'child_process';
import require$$0$4 from 'fs';
import require$$0$3 from 'fs/promises';
import require$$4 from 'node:fs';
import process$1 from 'node:process';

function commonjsRequire(path) {
	throw new Error('Could not dynamically require "' + path + '". Please configure the dynamicRequireTargets or/and ignoreDynamicRequires option of @rollup/plugin-commonjs appropriately for this require call to work.');
}

var commonjs = {};

var hasRequiredCommonjs;

function requireCommonjs () {
	if (hasRequiredCommonjs) return commonjs;
	hasRequiredCommonjs = 1;
	Object.defineProperty(commonjs, "__esModule", { value: true });
	commonjs.walkUp = void 0;
	const path_1 = require$$0;
	const walkUp = function* (path) {
	    for (path = (0, path_1.resolve)(path); path;) {
	        yield path;
	        const pp = (0, path_1.dirname)(path);
	        if (pp === path) {
	            break;
	        }
	        else {
	            path = pp;
	        }
	    }
	};
	commonjs.walkUp = walkUp;
	
	return commonjs;
}

var ini;
var hasRequiredIni;

function requireIni () {
	if (hasRequiredIni) return ini;
	hasRequiredIni = 1;
	const { hasOwnProperty } = Object.prototype;

	const encode = (obj, opt = {}) => {
	  if (typeof opt === 'string') {
	    opt = { section: opt };
	  }
	  opt.align = opt.align === true;
	  opt.newline = opt.newline === true;
	  opt.sort = opt.sort === true;
	  opt.whitespace = opt.whitespace === true || opt.align === true;
	  // The `typeof` check is required because accessing the `process` directly fails on browsers.
	  /* istanbul ignore next */
	  opt.platform = opt.platform || (typeof process !== 'undefined' && process.platform);
	  opt.bracketedArray = opt.bracketedArray !== false;

	  /* istanbul ignore next */
	  const eol = opt.platform === 'win32' ? '\r\n' : '\n';
	  const separator = opt.whitespace ? ' = ' : '=';
	  const children = [];

	  const keys = opt.sort ? Object.keys(obj).sort() : Object.keys(obj);

	  let padToChars = 0;
	  // If aligning on the separator, then padToChars is determined as follows:
	  // 1. Get the keys
	  // 2. Exclude keys pointing to objects unless the value is null or an array
	  // 3. Add `[]` to array keys
	  // 4. Ensure non empty set of keys
	  // 5. Reduce the set to the longest `safe` key
	  // 6. Get the `safe` length
	  if (opt.align) {
	    padToChars = safe(
	      (
	        keys
	          .filter(k => obj[k] === null || Array.isArray(obj[k]) || typeof obj[k] !== 'object')
	          .map(k => Array.isArray(obj[k]) ? `${k}[]` : k)
	      )
	        .concat([''])
	        .reduce((a, b) => safe(a).length >= safe(b).length ? a : b)
	    ).length;
	  }

	  let out = '';
	  const arraySuffix = opt.bracketedArray ? '[]' : '';

	  for (const k of keys) {
	    const val = obj[k];
	    if (val && Array.isArray(val)) {
	      for (const item of val) {
	        out += safe(`${k}${arraySuffix}`).padEnd(padToChars, ' ') + separator + safe(item) + eol;
	      }
	    } else if (val && typeof val === 'object') {
	      children.push(k);
	    } else {
	      out += safe(k).padEnd(padToChars, ' ') + separator + safe(val) + eol;
	    }
	  }

	  if (opt.section && out.length) {
	    out = '[' + safe(opt.section) + ']' + (opt.newline ? eol + eol : eol) + out;
	  }

	  for (const k of children) {
	    const nk = splitSections(k, '.').join('\\.');
	    const section = (opt.section ? opt.section + '.' : '') + nk;
	    const child = encode(obj[k], {
	      ...opt,
	      section,
	    });
	    if (out.length && child.length) {
	      out += eol;
	    }

	    out += child;
	  }

	  return out
	};

	function splitSections (str, separator) {
	  var lastMatchIndex = 0;
	  var lastSeparatorIndex = 0;
	  var nextIndex = 0;
	  var sections = [];

	  do {
	    nextIndex = str.indexOf(separator, lastMatchIndex);

	    if (nextIndex !== -1) {
	      lastMatchIndex = nextIndex + separator.length;

	      if (nextIndex > 0 && str[nextIndex - 1] === '\\') {
	        continue
	      }

	      sections.push(str.slice(lastSeparatorIndex, nextIndex));
	      lastSeparatorIndex = nextIndex + separator.length;
	    }
	  } while (nextIndex !== -1)

	  sections.push(str.slice(lastSeparatorIndex));

	  return sections
	}

	const decode = (str, opt = {}) => {
	  opt.bracketedArray = opt.bracketedArray !== false;
	  const out = Object.create(null);
	  let p = out;
	  let section = null;
	  //          section          |key      = value
	  const re = /^\[([^\]]*)\]\s*$|^([^=]+)(=(.*))?$/i;
	  const lines = str.split(/[\r\n]+/g);
	  const duplicates = {};

	  for (const line of lines) {
	    if (!line || line.match(/^\s*[;#]/) || line.match(/^\s*$/)) {
	      continue
	    }
	    const match = line.match(re);
	    if (!match) {
	      continue
	    }
	    if (match[1] !== undefined) {
	      section = unsafe(match[1]);
	      if (section === '__proto__') {
	        // not allowed
	        // keep parsing the section, but don't attach it.
	        p = Object.create(null);
	        continue
	      }
	      p = out[section] = out[section] || Object.create(null);
	      continue
	    }
	    const keyRaw = unsafe(match[2]);
	    let isArray;
	    if (opt.bracketedArray) {
	      isArray = keyRaw.length > 2 && keyRaw.slice(-2) === '[]';
	    } else {
	      duplicates[keyRaw] = (duplicates?.[keyRaw] || 0) + 1;
	      isArray = duplicates[keyRaw] > 1;
	    }
	    const key = isArray && keyRaw.endsWith('[]')
	      ? keyRaw.slice(0, -2) : keyRaw;

	    if (key === '__proto__') {
	      continue
	    }
	    const valueRaw = match[3] ? unsafe(match[4]) : true;
	    const value = valueRaw === 'true' ||
	      valueRaw === 'false' ||
	      valueRaw === 'null' ? JSON.parse(valueRaw)
	      : valueRaw;

	    // Convert keys with '[]' suffix to an array
	    if (isArray) {
	      if (!hasOwnProperty.call(p, key)) {
	        p[key] = [];
	      } else if (!Array.isArray(p[key])) {
	        p[key] = [p[key]];
	      }
	    }

	    // safeguard against resetting a previously defined
	    // array by accidentally forgetting the brackets
	    if (Array.isArray(p[key])) {
	      p[key].push(value);
	    } else {
	      p[key] = value;
	    }
	  }

	  // {a:{y:1},"a.b":{x:2}} --> {a:{y:1,b:{x:2}}}
	  // use a filter to return the keys that have to be deleted.
	  const remove = [];
	  for (const k of Object.keys(out)) {
	    if (!hasOwnProperty.call(out, k) ||
	      typeof out[k] !== 'object' ||
	      Array.isArray(out[k])) {
	      continue
	    }

	    // see if the parent section is also an object.
	    // if so, add it to that, and mark this one for deletion
	    const parts = splitSections(k, '.');
	    p = out;
	    const l = parts.pop();
	    const nl = l.replace(/\\\./g, '.');
	    for (const part of parts) {
	      if (part === '__proto__') {
	        continue
	      }
	      if (!hasOwnProperty.call(p, part) || typeof p[part] !== 'object') {
	        p[part] = Object.create(null);
	      }
	      p = p[part];
	    }
	    if (p === out && nl === l) {
	      continue
	    }

	    p[nl] = out[k];
	    remove.push(k);
	  }
	  for (const del of remove) {
	    delete out[del];
	  }

	  return out
	};

	const isQuoted = val => {
	  return (val.startsWith('"') && val.endsWith('"')) ||
	    (val.startsWith("'") && val.endsWith("'"))
	};

	const safe = val => {
	  if (
	    typeof val !== 'string' ||
	    val.match(/[=\r\n]/) ||
	    val.match(/^\[/) ||
	    (val.length > 1 && isQuoted(val)) ||
	    val !== val.trim()
	  ) {
	    return JSON.stringify(val)
	  }
	  return val.split(';').join('\\;').split('#').join('\\#')
	};

	const unsafe = val => {
	  val = (val || '').trim();
	  if (isQuoted(val)) {
	    // remove the single quotes before calling JSON.parse
	    if (val.charAt(0) === "'") {
	      val = val.slice(1, -1);
	    }
	    try {
	      val = JSON.parse(val);
	    } catch {
	      // ignore errors
	    }
	  } else {
	    // walk the val to find the first not-escaped ; character
	    let esc = false;
	    let unesc = '';
	    for (let i = 0, l = val.length; i < l; i++) {
	      const c = val.charAt(i);
	      if (esc) {
	        if ('\\;#'.indexOf(c) !== -1) {
	          unesc += c;
	        } else {
	          unesc += '\\' + c;
	        }

	        esc = false;
	      } else if (';#'.indexOf(c) !== -1) {
	        break
	      } else if (c === '\\') {
	        esc = true;
	      } else {
	        unesc += c;
	      }
	    }
	    if (esc) {
	      unesc += '\\';
	    }

	    return unesc.trim()
	  }
	  return val
	};

	ini = {
	  parse: decode,
	  decode,
	  stringify: encode,
	  encode,
	  safe,
	  unsafe,
	};
	return ini;
}

var nopt = {exports: {}};

var lib$b;
var hasRequiredLib$b;

function requireLib$b () {
	if (hasRequiredLib$b) return lib$b;
	hasRequiredLib$b = 1;
	lib$b = abbrev;

	function abbrev (...args) {
	  let list = args.length === 1 || Array.isArray(args[0]) ? args[0] : args;

	  for (let i = 0, l = list.length; i < l; i++) {
	    list[i] = typeof list[i] === 'string' ? list[i] : String(list[i]);
	  }

	  // sort them lexicographically, so that they're next to their nearest kin
	  list = list.sort(lexSort);

	  // walk through each, seeing how much it has in common with the next and previous
	  const abbrevs = {};
	  let prev = '';
	  for (let ii = 0, ll = list.length; ii < ll; ii++) {
	    const current = list[ii];
	    const next = list[ii + 1] || '';
	    let nextMatches = true;
	    let prevMatches = true;
	    if (current === next) {
	      continue
	    }
	    let j = 0;
	    const cl = current.length;
	    for (; j < cl; j++) {
	      const curChar = current.charAt(j);
	      nextMatches = nextMatches && curChar === next.charAt(j);
	      prevMatches = prevMatches && curChar === prev.charAt(j);
	      if (!nextMatches && !prevMatches) {
	        j++;
	        break
	      }
	    }
	    prev = current;
	    if (j === cl) {
	      abbrevs[current] = current;
	      continue
	    }
	    for (let a = current.slice(0, j); j <= cl; j++) {
	      abbrevs[a] = current;
	      a += current.charAt(j);
	    }
	  }
	  return abbrevs
	}

	function lexSort (a, b) {
	  return a === b ? 0 : a > b ? 1 : -1
	}
	return lib$b;
}

/* istanbul ignore next */

var debug;
var hasRequiredDebug;

function requireDebug () {
	if (hasRequiredDebug) return debug;
	hasRequiredDebug = 1;
	debug = process.env.DEBUG_NOPT || process.env.NOPT_DEBUG
	  // eslint-disable-next-line no-console
	  ? (...a) => console.error(...a)
	  : () => {};
	return debug;
}

var typeDefs$1;
var hasRequiredTypeDefs$1;

function requireTypeDefs$1 () {
	if (hasRequiredTypeDefs$1) return typeDefs$1;
	hasRequiredTypeDefs$1 = 1;
	const url = require$$0$1;
	const path = require$$0;
	const Stream = require$$2$1.Stream;
	const os = require$$1$2;
	const debug = requireDebug();

	function validateString (data, k, val) {
	  data[k] = String(val);
	}

	function validatePath (data, k, val) {
	  if (val === true) {
	    return false
	  }
	  if (val === null) {
	    return true
	  }

	  val = String(val);

	  const isWin = process.platform === 'win32';
	  const homePattern = isWin ? /^~(\/|\\)/ : /^~\//;
	  const home = os.homedir();

	  if (home && val.match(homePattern)) {
	    data[k] = path.resolve(home, val.slice(2));
	  } else {
	    data[k] = path.resolve(val);
	  }
	  return true
	}

	function validateNumber (data, k, val) {
	  debug('validate Number %j %j %j', k, val, isNaN(val));
	  if (isNaN(val)) {
	    return false
	  }
	  data[k] = +val;
	}

	function validateDate (data, k, val) {
	  const s = Date.parse(val);
	  debug('validate Date %j %j %j', k, val, s);
	  if (isNaN(s)) {
	    return false
	  }
	  data[k] = new Date(val);
	}

	function validateBoolean (data, k, val) {
	  if (typeof val === 'string') {
	    if (!isNaN(val)) {
	      val = !!(+val);
	    } else if (val === 'null' || val === 'false') {
	      val = false;
	    } else {
	      val = true;
	    }
	  } else {
	    val = !!val;
	  }
	  data[k] = val;
	}

	function validateUrl (data, k, val) {
	  // Changing this would be a breaking change in the npm cli
	  /* eslint-disable-next-line node/no-deprecated-api */
	  val = url.parse(String(val));
	  if (!val.host) {
	    return false
	  }
	  data[k] = val.href;
	}

	function validateStream (data, k, val) {
	  if (!(val instanceof Stream)) {
	    return false
	  }
	  data[k] = val;
	}

	typeDefs$1 = {
	  String: { type: String, validate: validateString },
	  Boolean: { type: Boolean, validate: validateBoolean },
	  url: { type: url, validate: validateUrl },
	  Number: { type: Number, validate: validateNumber },
	  path: { type: path, validate: validatePath },
	  Stream: { type: Stream, validate: validateStream },
	  Date: { type: Date, validate: validateDate },
	  Array: { type: Array },
	};
	return typeDefs$1;
}

var noptLib;
var hasRequiredNoptLib;

function requireNoptLib () {
	if (hasRequiredNoptLib) return noptLib;
	hasRequiredNoptLib = 1;
	const abbrev = requireLib$b();
	const debug = requireDebug();
	const defaultTypeDefs = requireTypeDefs$1();

	const hasOwn = (o, k) => Object.prototype.hasOwnProperty.call(o, k);

	const getType = (k, { types, dynamicTypes }) => {
	  let hasType = hasOwn(types, k);
	  let type = types[k];
	  if (!hasType && typeof dynamicTypes === 'function') {
	    const matchedType = dynamicTypes(k);
	    if (matchedType !== undefined) {
	      type = matchedType;
	      hasType = true;
	    }
	  }
	  return [hasType, type]
	};

	const isTypeDef = (type, def) => def && type === def;
	const hasTypeDef = (type, def) => def && type.indexOf(def) !== -1;
	const doesNotHaveTypeDef = (type, def) => def && !hasTypeDef(type, def);

	function nopt (args, {
	  types,
	  shorthands,
	  typeDefs,
	  invalidHandler, // opt is configured but its value does not validate against given type
	  unknownHandler, // opt is not configured
	  abbrevHandler, // opt is being expanded via abbrev
	  typeDefault,
	  dynamicTypes,
	} = {}) {
	  debug(types, shorthands, args, typeDefs);

	  const data = {};
	  const argv = {
	    remain: [],
	    cooked: args,
	    original: args.slice(0),
	  };

	  parse(args, data, argv.remain, {
	    typeDefs, types, dynamicTypes, shorthands, unknownHandler, abbrevHandler,
	  });

	  // now data is full
	  clean(data, { types, dynamicTypes, typeDefs, invalidHandler, typeDefault });
	  data.argv = argv;

	  Object.defineProperty(data.argv, 'toString', {
	    value: function () {
	      return this.original.map(JSON.stringify).join(' ')
	    },
	    enumerable: false,
	  });

	  return data
	}

	function clean (data, {
	  types = {},
	  typeDefs = {},
	  dynamicTypes,
	  invalidHandler,
	  typeDefault,
	} = {}) {
	  const StringType = typeDefs.String?.type;
	  const NumberType = typeDefs.Number?.type;
	  const ArrayType = typeDefs.Array?.type;
	  const BooleanType = typeDefs.Boolean?.type;
	  const DateType = typeDefs.Date?.type;

	  const hasTypeDefault = typeof typeDefault !== 'undefined';
	  if (!hasTypeDefault) {
	    typeDefault = [false, true, null];
	    if (StringType) {
	      typeDefault.push(StringType);
	    }
	    if (ArrayType) {
	      typeDefault.push(ArrayType);
	    }
	  }

	  const remove = {};

	  Object.keys(data).forEach((k) => {
	    if (k === 'argv') {
	      return
	    }
	    let val = data[k];
	    debug('val=%j', val);
	    const isArray = Array.isArray(val);
	    let [hasType, rawType] = getType(k, { types, dynamicTypes });
	    let type = rawType;
	    if (!isArray) {
	      val = [val];
	    }
	    if (!type) {
	      type = typeDefault;
	    }
	    if (isTypeDef(type, ArrayType)) {
	      type = typeDefault.concat(ArrayType);
	    }
	    if (!Array.isArray(type)) {
	      type = [type];
	    }

	    debug('val=%j', val);
	    debug('types=', type);
	    val = val.map((v) => {
	      // if it's an unknown value, then parse false/true/null/numbers/dates
	      if (typeof v === 'string') {
	        debug('string %j', v);
	        v = v.trim();
	        if ((v === 'null' && ~type.indexOf(null))
	            || (v === 'true' &&
	               (~type.indexOf(true) || hasTypeDef(type, BooleanType)))
	            || (v === 'false' &&
	               (~type.indexOf(false) || hasTypeDef(type, BooleanType)))) {
	          v = JSON.parse(v);
	          debug('jsonable %j', v);
	        } else if (hasTypeDef(type, NumberType) && !isNaN(v)) {
	          debug('convert to number', v);
	          v = +v;
	        } else if (hasTypeDef(type, DateType) && !isNaN(Date.parse(v))) {
	          debug('convert to date', v);
	          v = new Date(v);
	        }
	      }

	      if (!hasType) {
	        if (!hasTypeDefault) {
	          return v
	        }
	        // if the default type has been passed in then we want to validate the
	        // unknown data key instead of bailing out earlier. we also set the raw
	        // type which is passed to the invalid handler so that it can be
	        // determined if during validation if it is unknown vs invalid
	        rawType = typeDefault;
	      }

	      // allow `--no-blah` to set 'blah' to null if null is allowed
	      if (v === false && ~type.indexOf(null) &&
	          !(~type.indexOf(false) || hasTypeDef(type, BooleanType))) {
	        v = null;
	      }

	      const d = {};
	      d[k] = v;
	      debug('prevalidated val', d, v, rawType);
	      if (!validate(d, k, v, rawType, { typeDefs })) {
	        if (invalidHandler) {
	          invalidHandler(k, v, rawType, data);
	        } else if (invalidHandler !== false) {
	          debug('invalid: ' + k + '=' + v, rawType);
	        }
	        return remove
	      }
	      debug('validated v', d, v, rawType);
	      return d[k]
	    }).filter((v) => v !== remove);

	    // if we allow Array specifically, then an empty array is how we
	    // express 'no value here', not null.  Allow it.
	    if (!val.length && doesNotHaveTypeDef(type, ArrayType)) {
	      debug('VAL HAS NO LENGTH, DELETE IT', val, k, type.indexOf(ArrayType));
	      delete data[k];
	    } else if (isArray) {
	      debug(isArray, data[k], val);
	      data[k] = val;
	    } else {
	      data[k] = val[0];
	    }

	    debug('k=%s val=%j', k, val, data[k]);
	  });
	}

	function validate (data, k, val, type, { typeDefs } = {}) {
	  const ArrayType = typeDefs?.Array?.type;
	  // arrays are lists of types.
	  if (Array.isArray(type)) {
	    for (let i = 0, l = type.length; i < l; i++) {
	      if (isTypeDef(type[i], ArrayType)) {
	        continue
	      }
	      if (validate(data, k, val, type[i], { typeDefs })) {
	        return true
	      }
	    }
	    delete data[k];
	    return false
	  }

	  // an array of anything?
	  if (isTypeDef(type, ArrayType)) {
	    return true
	  }

	  // Original comment:
	  // NaN is poisonous.  Means that something is not allowed.
	  // New comment: Changing this to an isNaN check breaks a lot of tests.
	  // Something is being assumed here that is not actually what happens in
	  // practice.  Fixing it is outside the scope of getting linting to pass in
	  // this repo. Leaving as-is for now.
	  /* eslint-disable-next-line no-self-compare */
	  if (type !== type) {
	    debug('Poison NaN', k, val, type);
	    delete data[k];
	    return false
	  }

	  // explicit list of values
	  if (val === type) {
	    debug('Explicitly allowed %j', val);
	    data[k] = val;
	    return true
	  }

	  // now go through the list of typeDefs, validate against each one.
	  let ok = false;
	  const types = Object.keys(typeDefs);
	  for (let i = 0, l = types.length; i < l; i++) {
	    debug('test type %j %j %j', k, val, types[i]);
	    const t = typeDefs[types[i]];
	    if (t && (
	      (type && type.name && t.type && t.type.name) ?
	        (type.name === t.type.name) :
	        (type === t.type)
	    )) {
	      const d = {};
	      ok = t.validate(d, k, val) !== false;
	      val = d[k];
	      if (ok) {
	        data[k] = val;
	        break
	      }
	    }
	  }
	  debug('OK? %j (%j %j %j)', ok, k, val, types[types.length - 1]);

	  if (!ok) {
	    delete data[k];
	  }
	  return ok
	}

	function parse (args, data, remain, {
	  types = {},
	  typeDefs = {},
	  shorthands = {},
	  dynamicTypes,
	  unknownHandler,
	  abbrevHandler,
	} = {}) {
	  const StringType = typeDefs.String?.type;
	  const NumberType = typeDefs.Number?.type;
	  const ArrayType = typeDefs.Array?.type;
	  const BooleanType = typeDefs.Boolean?.type;

	  debug('parse', args, data, remain);

	  const abbrevs = abbrev(Object.keys(types));
	  debug('abbrevs=%j', abbrevs);
	  const shortAbbr = abbrev(Object.keys(shorthands));

	  for (let i = 0; i < args.length; i++) {
	    let arg = args[i];
	    debug('arg', arg);

	    if (arg.match(/^-{2,}$/)) {
	      // done with keys.
	      // the rest are args.
	      remain.push.apply(remain, args.slice(i + 1));
	      args[i] = '--';
	      break
	    }
	    let hadEq = false;
	    if (arg.charAt(0) === '-' && arg.length > 1) {
	      const at = arg.indexOf('=');
	      if (at > -1) {
	        hadEq = true;
	        const v = arg.slice(at + 1);
	        arg = arg.slice(0, at);
	        args.splice(i, 1, arg, v);
	      }

	      // see if it's a shorthand
	      // if so, splice and back up to re-parse it.
	      const shRes = resolveShort(arg, shortAbbr, abbrevs, { shorthands, abbrevHandler });
	      debug('arg=%j shRes=%j', arg, shRes);
	      if (shRes) {
	        args.splice.apply(args, [i, 1].concat(shRes));
	        if (arg !== shRes[0]) {
	          i--;
	          continue
	        }
	      }
	      arg = arg.replace(/^-+/, '');
	      let no = null;
	      while (arg.toLowerCase().indexOf('no-') === 0) {
	        no = !no;
	        arg = arg.slice(3);
	      }

	      // abbrev includes the original full string in its abbrev list
	      if (abbrevs[arg] && abbrevs[arg] !== arg) {
	        if (abbrevHandler) {
	          abbrevHandler(arg, abbrevs[arg]);
	        } else if (abbrevHandler !== false) {
	          debug(`abbrev: ${arg} -> ${abbrevs[arg]}`);
	        }
	        arg = abbrevs[arg];
	      }

	      let [hasType, argType] = getType(arg, { types, dynamicTypes });
	      let isTypeArray = Array.isArray(argType);
	      if (isTypeArray && argType.length === 1) {
	        isTypeArray = false;
	        argType = argType[0];
	      }

	      let isArray = isTypeDef(argType, ArrayType) ||
	        isTypeArray && hasTypeDef(argType, ArrayType);

	      // allow unknown things to be arrays if specified multiple times.
	      if (!hasType && hasOwn(data, arg)) {
	        if (!Array.isArray(data[arg])) {
	          data[arg] = [data[arg]];
	        }
	        isArray = true;
	      }

	      let val;
	      let la = args[i + 1];

	      const isBool = typeof no === 'boolean' ||
	        isTypeDef(argType, BooleanType) ||
	        isTypeArray && hasTypeDef(argType, BooleanType) ||
	        (typeof argType === 'undefined' && !hadEq) ||
	        (la === 'false' &&
	         (argType === null ||
	          isTypeArray && ~argType.indexOf(null)));

	      if (typeof argType === 'undefined') {
	        // la is going to unexpectedly be parsed outside the context of this arg
	        const hangingLa = !hadEq && la && !la?.startsWith('-') && !['true', 'false'].includes(la);
	        if (unknownHandler) {
	          if (hangingLa) {
	            unknownHandler(arg, la);
	          } else {
	            unknownHandler(arg);
	          }
	        } else if (unknownHandler !== false) {
	          debug(`unknown: ${arg}`);
	          if (hangingLa) {
	            debug(`unknown: ${la} parsed as normal opt`);
	          }
	        }
	      }

	      if (isBool) {
	        // just set and move along
	        val = !no;
	        // however, also support --bool true or --bool false
	        if (la === 'true' || la === 'false') {
	          val = JSON.parse(la);
	          la = null;
	          if (no) {
	            val = !val;
	          }
	          i++;
	        }

	        // also support "foo":[Boolean, "bar"] and "--foo bar"
	        if (isTypeArray && la) {
	          if (~argType.indexOf(la)) {
	            // an explicit type
	            val = la;
	            i++;
	          } else if (la === 'null' && ~argType.indexOf(null)) {
	            // null allowed
	            val = null;
	            i++;
	          } else if (!la.match(/^-{2,}[^-]/) &&
	                      !isNaN(la) &&
	                      hasTypeDef(argType, NumberType)) {
	            // number
	            val = +la;
	            i++;
	          } else if (!la.match(/^-[^-]/) && hasTypeDef(argType, StringType)) {
	            // string
	            val = la;
	            i++;
	          }
	        }

	        if (isArray) {
	          (data[arg] = data[arg] || []).push(val);
	        } else {
	          data[arg] = val;
	        }

	        continue
	      }

	      if (isTypeDef(argType, StringType)) {
	        if (la === undefined) {
	          la = '';
	        } else if (la.match(/^-{1,2}[^-]+/)) {
	          la = '';
	          i--;
	        }
	      }

	      if (la && la.match(/^-{2,}$/)) {
	        la = undefined;
	        i--;
	      }

	      val = la === undefined ? true : la;
	      if (isArray) {
	        (data[arg] = data[arg] || []).push(val);
	      } else {
	        data[arg] = val;
	      }

	      i++;
	      continue
	    }
	    remain.push(arg);
	  }
	}

	const SINGLES = Symbol('singles');
	const singleCharacters = (arg, shorthands) => {
	  let singles = shorthands[SINGLES];
	  if (!singles) {
	    singles = Object.keys(shorthands).filter((s) => s.length === 1).reduce((l, r) => {
	      l[r] = true;
	      return l
	    }, {});
	    shorthands[SINGLES] = singles;
	    debug('shorthand singles', singles);
	  }
	  const chrs = arg.split('').filter((c) => singles[c]);
	  return chrs.join('') === arg ? chrs : null
	};

	function resolveShort (arg, ...rest) {
	  const { abbrevHandler, types = {}, shorthands = {} } = rest.length ? rest.pop() : {};
	  const shortAbbr = rest[0] ?? abbrev(Object.keys(shorthands));
	  const abbrevs = rest[1] ?? abbrev(Object.keys(types));

	  // handle single-char shorthands glommed together, like
	  // npm ls -glp, but only if there is one dash, and only if
	  // all of the chars are single-char shorthands, and it's
	  // not a match to some other abbrev.
	  arg = arg.replace(/^-+/, '');

	  // if it's an exact known option, then don't go any further
	  if (abbrevs[arg] === arg) {
	    return null
	  }

	  // if it's an exact known shortopt, same deal
	  if (shorthands[arg]) {
	    // make it an array, if it's a list of words
	    if (shorthands[arg] && !Array.isArray(shorthands[arg])) {
	      shorthands[arg] = shorthands[arg].split(/\s+/);
	    }

	    return shorthands[arg]
	  }

	  // first check to see if this arg is a set of single-char shorthands
	  const chrs = singleCharacters(arg, shorthands);
	  if (chrs) {
	    return chrs.map((c) => shorthands[c]).reduce((l, r) => l.concat(r), [])
	  }

	  // if it's an arg abbrev, and not a literal shorthand, then prefer the arg
	  if (abbrevs[arg] && !shorthands[arg]) {
	    return null
	  }

	  // if it's an abbr for a shorthand, then use that
	  // exact match has already happened so we don't need to account for that here
	  if (shortAbbr[arg]) {
	    if (abbrevHandler) {
	      abbrevHandler(arg, shortAbbr[arg]);
	    } else if (abbrevHandler !== false) {
	      debug(`abbrev: ${arg} -> ${shortAbbr[arg]}`);
	    }
	    arg = shortAbbr[arg];
	  }

	  // make it an array, if it's a list of words
	  if (shorthands[arg] && !Array.isArray(shorthands[arg])) {
	    shorthands[arg] = shorthands[arg].split(/\s+/);
	  }

	  return shorthands[arg]
	}

	noptLib = {
	  nopt,
	  clean,
	  parse,
	  validate,
	  resolveShort,
	  typeDefs: defaultTypeDefs,
	};
	return noptLib;
}

var hasRequiredNopt;

function requireNopt () {
	if (hasRequiredNopt) return nopt.exports;
	hasRequiredNopt = 1;
	(function (module, exports) {
		const lib = requireNoptLib();
		const defaultTypeDefs = requireTypeDefs$1();

		// This is the version of nopt's API that requires setting typeDefs and invalidHandler
		// on the required `nopt` object since it is a singleton. To not do a breaking change
		// an API that requires all options be passed in is located in `nopt-lib.js` and
		// exported here as lib.
		// TODO(breaking): make API only work in non-singleton mode

		module.exports = exports = nopt;
		exports.clean = clean;
		exports.typeDefs = defaultTypeDefs;
		exports.lib = lib;

		function nopt (types, shorthands, args = process.argv, slice = 2) {
		  return lib.nopt(args.slice(slice), {
		    types: types || {},
		    shorthands: shorthands || {},
		    typeDefs: exports.typeDefs,
		    invalidHandler: exports.invalidHandler,
		    unknownHandler: exports.unknownHandler,
		    abbrevHandler: exports.abbrevHandler,
		  })
		}

		function clean (data, types, typeDefs = exports.typeDefs) {
		  return lib.clean(data, {
		    types: types || {},
		    typeDefs,
		    invalidHandler: exports.invalidHandler,
		    unknownHandler: exports.unknownHandler,
		    abbrevHandler: exports.abbrevHandler,
		  })
		} 
	} (nopt, nopt.exports));
	return nopt.exports;
}

var typeDefs = {exports: {}};

var umask;
var hasRequiredUmask;

function requireUmask () {
	if (hasRequiredUmask) return umask;
	hasRequiredUmask = 1;
	const parse = val => {
	  // this is run via nopt and parse field where everything is
	  // converted to a string first, ignoring coverage for now
	  // instead of figuring out what is happening under the hood in nopt
	  // istanbul ignore else
	  if (typeof val === 'string') {
	    if (/^0o?[0-7]+$/.test(val)) {
	      return parseInt(val.replace(/^0o?/, ''), 8)
	    } else if (/^[1-9][0-9]*$/.test(val)) {
	      return parseInt(val, 10)
	    } else {
	      throw new Error(`invalid umask value: ${val}`)
	    }
	  } else {
	    if (typeof val !== 'number') {
	      throw new Error(`invalid umask value: ${val}`)
	    }
	    val = Math.floor(val);
	    if (val < 0 || val > 511) {
	      throw new Error(`invalid umask value: ${val}`)
	    }
	    return val
	  }
	};

	const validate = (data, k, val) => {
	  try {
	    data[k] = parse(val);
	    return true
	  } catch (er) {
	    return false
	  }
	};

	umask = { parse, validate };
	return umask;
}

var hasRequiredTypeDefs;

function requireTypeDefs () {
	if (hasRequiredTypeDefs) return typeDefs.exports;
	hasRequiredTypeDefs = 1;
	(function (module) {
		const nopt = requireNopt();

		const { validate: validateUmask } = requireUmask();

		class Umask {}
		class Semver {}
		const semverValid = requireValid();
		const validateSemver = (data, k, val) => {
		  const valid = semverValid(val);
		  if (!valid) {
		    return false
		  }
		  data[k] = valid;
		};

		const noptValidatePath = nopt.typeDefs.path.validate;
		const validatePath = (data, k, val) => {
		  if (typeof val !== 'string') {
		    return false
		  }
		  return noptValidatePath(data, k, val)
		};

		// add descriptions so we can validate more usefully
		module.exports = {
		  ...nopt.typeDefs,
		  semver: {
		    type: Semver,
		    validate: validateSemver,
		    description: 'full valid SemVer string',
		  },
		  Umask: {
		    type: Umask,
		    validate: validateUmask,
		    description: 'octal number in range 0o000..0o777 (0..511)',
		  },
		  url: {
		    ...nopt.typeDefs.url,
		    description: 'full url with "http://"',
		  },
		  path: {
		    ...nopt.typeDefs.path,
		    validate: validatePath,
		    description: 'valid filesystem path',
		  },
		  Number: {
		    ...nopt.typeDefs.Number,
		    description: 'numeric value',
		  },
		  Boolean: {
		    ...nopt.typeDefs.Boolean,
		    description: 'boolean value (true or false)',
		  },
		  Date: {
		    ...nopt.typeDefs.Date,
		    description: 'valid Date string',
		  },
		};

		// TODO: make nopt less of a global beast so this kludge isn't necessary
		nopt.typeDefs = module.exports; 
	} (typeDefs));
	return typeDefs.exports;
}

var nerfDart;
var hasRequiredNerfDart;

function requireNerfDart () {
	if (hasRequiredNerfDart) return nerfDart;
	hasRequiredNerfDart = 1;
	const { URL } = require$$0$2;

	/**
	 * Maps a URL to an identifier.
	 *
	 * Name courtesy schiffertronix media LLC, a New Jersey corporation
	 *
	 * @param {String} uri The URL to be nerfed.
	 *
	 * @returns {String} A nerfed URL.
	 */
	nerfDart = (url) => {
	  const parsed = new URL(url);
	  const from = `${parsed.protocol}//${parsed.host}${parsed.pathname}`;
	  const rel = new URL('.', from);
	  const res = `//${rel.host}${rel.pathname}`;
	  return res
	};
	return nerfDart;
}

var envReplace;
var hasRequiredEnvReplace;

function requireEnvReplace () {
	if (hasRequiredEnvReplace) return envReplace;
	hasRequiredEnvReplace = 1;
	// replace any ${ENV} values with the appropriate environ.

	const envExpr = /(?<!\\)(\\*)\$\{([^${}]+)\}/g;

	envReplace = (f, env) => f.replace(envExpr, (orig, esc, name) => {
	  const val = env[name] !== undefined ? env[name] : `$\{${name}}`;

	  // consume the escape chars that are relevant.
	  if (esc.length % 2) {
	    return orig.slice((esc.length + 1) / 2)
	  }

	  return (esc.slice(esc.length / 2)) + val
	});
	return envReplace;
}

var parseField_1;
var hasRequiredParseField;

function requireParseField () {
	if (hasRequiredParseField) return parseField_1;
	hasRequiredParseField = 1;
	// Parse a field, coercing it to the best type available.
	const typeDefs = requireTypeDefs();
	const envReplace = requireEnvReplace();
	const { resolve } = require$$2$2;

	const { parse: umaskParse } = requireUmask();

	const parseField = (f, key, opts, listElement = false) => {
	  if (typeof f !== 'string' && !Array.isArray(f)) {
	    return f
	  }

	  const { platform, types, home, env } = opts;

	  // type can be array or a single thing.  coerce to array.
	  const typeList = new Set([].concat(types[key]));
	  const isPath = typeList.has(typeDefs.path.type);
	  const isBool = typeList.has(typeDefs.Boolean.type);
	  const isString = isPath || typeList.has(typeDefs.String.type);
	  const isUmask = typeList.has(typeDefs.Umask.type);
	  const isNumber = typeList.has(typeDefs.Number.type);
	  const isList = !listElement && typeList.has(Array);
	  const isDate = typeList.has(typeDefs.Date.type);

	  if (Array.isArray(f)) {
	    return !isList ? f : f.map(field => parseField(field, key, opts, true))
	  }

	  // now we know it's a string
	  f = f.trim();

	  // list types get put in the environment separated by double-\n
	  // usually a single \n would suffice, but ca/cert configs can contain
	  // line breaks and multiple entries.
	  if (isList) {
	    return parseField(f.split('\n\n'), key, opts)
	  }

	  // --foo is like --foo=true for boolean types
	  if (isBool && !isString && f === '') {
	    return true
	  }

	  // string types can be the string 'true', 'false', etc.
	  // otherwise, parse these values out
	  if (!isString && !isPath && !isNumber) {
	    switch (f) {
	      case 'true': return true
	      case 'false': return false
	      case 'null': return null
	      case 'undefined': return undefined
	    }
	  }

	  f = envReplace(f, env);

	  if (isDate) {
	    return new Date(f)
	  }

	  if (isPath) {
	    const homePattern = platform === 'win32' ? /^~(\/|\\)/ : /^~\//;
	    if (homePattern.test(f) && home) {
	      f = resolve(home, f.slice(2));
	    } else {
	      f = resolve(f);
	    }
	  }

	  if (isUmask) {
	    try {
	      return umaskParse(f)
	    } catch (er) {
	      // let it warn later when we validate
	      return f
	    }
	  }

	  if (isNumber && !isNaN(f)) {
	    f = +f;
	  }

	  return f
	};

	parseField_1 = parseField;
	return parseField_1;
}

var setEnvs_1;
var hasRequiredSetEnvs;

function requireSetEnvs () {
	if (hasRequiredSetEnvs) return setEnvs_1;
	hasRequiredSetEnvs = 1;
	// Set environment variables for any non-default configs,
	// so that they're already there when we run lifecycle scripts.
	//
	// See https://github.com/npm/rfcs/pull/90

	// Return the env key if this is a thing that belongs in the env.
	// Ie, if the key isn't a @scope, //nerf.dart, or _private,
	// and the value is a string or array.  Otherwise return false.
	const envKey = (key, val) => {
	  return !/^[/@_]/.test(key) &&
	    (typeof envVal(val) === 'string') &&
	      `npm_config_${key.replace(/-/g, '_').toLowerCase()}`
	};

	const envVal = val => Array.isArray(val) ? val.map(v => envVal(v)).join('\n\n')
	  : val === null || val === undefined || val === false ? ''
	  : typeof val === 'object' ? null
	  : String(val);

	const sameConfigValue = (def, val) =>
	  !Array.isArray(val) || !Array.isArray(def) ? def === val
	  : sameArrayValue(def, val);

	const sameArrayValue = (def, val) => {
	  if (def.length !== val.length) {
	    return false
	  }

	  for (let i = 0; i < def.length; i++) {
	    /* istanbul ignore next - there are no array configs where the default
	     * is not an empty array, so this loop is a no-op, but it's the correct
	     * thing to do if we ever DO add a config like that. */
	    if (def[i] !== val[i]) {
	      return false
	    }
	  }
	  return true
	};

	const setEnv = (env, rawKey, rawVal) => {
	  const val = envVal(rawVal);
	  const key = envKey(rawKey, val);
	  if (key && val !== null) {
	    env[key] = val;
	  }
	};

	const setEnvs = (config) => {
	  // This ensures that all npm config values that are not the defaults are
	  // shared appropriately with child processes, without false positives.
	  const {
	    env,
	    defaults,
	    definitions,
	    list: [cliConf, envConf],
	  } = config;

	  env.INIT_CWD = process.cwd();

	  // if the key is deprecated, skip it always.
	  // if the key is the default value,
	  //   if the environ is NOT the default value,
	  //     set the environ
	  //   else skip it, it's fine
	  // if the key is NOT the default value,
	  //   if the env is setting it, then leave it (already set)
	  //   otherwise, set the env
	  const cliSet = new Set(Object.keys(cliConf));
	  const envSet = new Set(Object.keys(envConf));
	  for (const key in cliConf) {
	    const { deprecated, envExport = true } = definitions[key] || {};
	    if (deprecated || envExport === false) {
	      continue
	    }

	    if (sameConfigValue(defaults[key], cliConf[key])) {
	      // config is the default, if the env thought different, then we
	      // have to set it BACK to the default in the environment.
	      if (!sameConfigValue(envConf[key], cliConf[key])) {
	        setEnv(env, key, cliConf[key]);
	      }
	    } else {
	      // config is not the default.  if the env wasn't the one to set
	      // it that way, then we have to put it in the env
	      if (!(envSet.has(key) && !cliSet.has(key))) {
	        setEnv(env, key, cliConf[key]);
	      }
	    }
	  }

	  // also set some other common nice envs that we want to rely on
	  env.HOME = config.home;
	  env.npm_config_global_prefix = config.globalPrefix;
	  env.npm_config_local_prefix = config.localPrefix;
	  if (cliConf.editor) {
	    env.EDITOR = cliConf.editor;
	  }

	  // note: this doesn't afect the *current* node process, of course, since
	  // it's already started, but it does affect the options passed to scripts.
	  if (cliConf['node-options']) {
	    env.NODE_OPTIONS = cliConf['node-options'];
	  }
	  env.npm_execpath = config.npmBin;
	  env.NODE = env.npm_node_execpath = config.execPath;
	};

	setEnvs_1 = setEnvs;
	return setEnvs_1;
}

var errors$1;
var hasRequiredErrors$1;

function requireErrors$1 () {
	if (hasRequiredErrors$1) return errors$1;
	hasRequiredErrors$1 = 1;

	class ErrInvalidAuth extends Error {
	  constructor (problems) {
	    let message = 'Invalid auth configuration found: ';
	    message += problems.map((problem) => {
	      // istanbul ignore else
	      if (problem.action === 'delete') {
	        return `\`${problem.key}\` is not allowed in ${problem.where} config`
	      } else if (problem.action === 'rename') {
	        return `\`${problem.from}\` must be renamed to \`${problem.to}\` in ${problem.where} config`
	      }
	    }).join(', ');
	    message += '\nPlease run `npm config fix` to repair your configuration.`';
	    super(message);
	    this.code = 'ERR_INVALID_AUTH';
	    this.problems = problems;
	  }
	}

	errors$1 = {
	  ErrInvalidAuth,
	};
	return errors$1;
}

var typeDescription_1;
var hasRequiredTypeDescription;

function requireTypeDescription () {
	if (hasRequiredTypeDescription) return typeDescription_1;
	hasRequiredTypeDescription = 1;
	// return the description of the valid values of a field
	// returns a string for one thing, or an array of descriptions
	const typeDefs = requireTypeDefs();
	const typeDescription = t => {
	  if (!t || typeof t !== 'function' && typeof t !== 'object') {
	    return t
	  }

	  if (Array.isArray(t)) {
	    return t.map(t => typeDescription(t))
	  }

	  for (const { type, description } of Object.values(typeDefs)) {
	    if (type === t) {
	      return description || type
	    }
	  }

	  return t
	};
	typeDescription_1 = t => [].concat(typeDescription(t)).filter(t => t !== undefined);
	return typeDescription_1;
}

var lib$a;
var hasRequiredLib$a;

function requireLib$a () {
	if (hasRequiredLib$a) return lib$a;
	hasRequiredLib$a = 1;

	const INDENT = Symbol.for('indent');
	const NEWLINE = Symbol.for('newline');

	const DEFAULT_NEWLINE = '\n';
	const DEFAULT_INDENT = '  ';
	const BOM = /^\uFEFF/;

	// only respect indentation if we got a line break, otherwise squash it
	// things other than objects and arrays aren't indented, so ignore those
	// Important: in both of these regexps, the $1 capture group is the newline
	// or undefined, and the $2 capture group is the indent, or undefined.
	const FORMAT = /^\s*[{[]((?:\r?\n)+)([\s\t]*)/;
	const EMPTY = /^(?:\{\}|\[\])((?:\r?\n)+)?$/;

	// Node 20 puts single quotes around the token and a comma after it
	const UNEXPECTED_TOKEN = /^Unexpected token '?(.)'?(,)? /i;

	const hexify = (char) => {
	  const h = char.charCodeAt(0).toString(16).toUpperCase();
	  return `0x${h.length % 2 ? '0' : ''}${h}`
	};

	// Remove byte order marker. This catches EF BB BF (the UTF-8 BOM)
	// because the buffer-to-string conversion in `fs.readFileSync()`
	// translates it to FEFF, the UTF-16 BOM.
	const stripBOM = (txt) => String(txt).replace(BOM, '');

	const makeParsedError = (msg, parsing, position = 0) => ({
	  message: `${msg} while parsing ${parsing}`,
	  position,
	});

	const parseError = (e, txt, context = 20) => {
	  let msg = e.message;

	  if (!txt) {
	    return makeParsedError(msg, 'empty string')
	  }

	  const badTokenMatch = msg.match(UNEXPECTED_TOKEN);
	  const badIndexMatch = msg.match(/ position\s+(\d+)/i);

	  if (badTokenMatch) {
	    msg = msg.replace(
	      UNEXPECTED_TOKEN,
	      `Unexpected token ${JSON.stringify(badTokenMatch[1])} (${hexify(badTokenMatch[1])})$2 `
	    );
	  }

	  let errIdx;
	  if (badIndexMatch) {
	    errIdx = +badIndexMatch[1];
	  } else /* istanbul ignore next - doesnt happen in Node 22 */ if (
	    msg.match(/^Unexpected end of JSON.*/i)
	  ) {
	    errIdx = txt.length - 1;
	  }

	  if (errIdx == null) {
	    return makeParsedError(msg, `'${txt.slice(0, context * 2)}'`)
	  }

	  const start = errIdx <= context ? 0 : errIdx - context;
	  const end = errIdx + context >= txt.length ? txt.length : errIdx + context;
	  const slice = `${start ? '...' : ''}${txt.slice(start, end)}${end === txt.length ? '' : '...'}`;

	  return makeParsedError(
	    msg,
	    `${txt === slice ? '' : 'near '}${JSON.stringify(slice)}`,
	    errIdx
	  )
	};

	class JSONParseError extends SyntaxError {
	  constructor (er, txt, context, caller) {
	    const metadata = parseError(er, txt, context);
	    super(metadata.message);
	    Object.assign(this, metadata);
	    this.code = 'EJSONPARSE';
	    this.systemError = er;
	    Error.captureStackTrace(this, caller || this.constructor);
	  }

	  get name () {
	    return this.constructor.name
	  }

	  set name (n) {}

	  get [Symbol.toStringTag] () {
	    return this.constructor.name
	  }
	}

	const parseJson = (txt, reviver) => {
	  const result = JSON.parse(txt, reviver);
	  if (result && typeof result === 'object') {
	    // get the indentation so that we can save it back nicely
	    // if the file starts with {" then we have an indent of '', ie, none
	    // otherwise, pick the indentation of the next line after the first \n If the
	    // pattern doesn't match, then it means no indentation. JSON.stringify ignores
	    // symbols, so this is reasonably safe. if the string is '{}' or '[]', then
	    // use the default 2-space indent.
	    const match = txt.match(EMPTY) || txt.match(FORMAT) || [null, '', ''];
	    result[NEWLINE] = match[1] ?? DEFAULT_NEWLINE;
	    result[INDENT] = match[2] ?? DEFAULT_INDENT;
	  }
	  return result
	};

	const parseJsonError = (raw, reviver, context) => {
	  const txt = stripBOM(raw);
	  try {
	    return parseJson(txt, reviver)
	  } catch (e) {
	    if (typeof raw !== 'string' && !Buffer.isBuffer(raw)) {
	      const msg = Array.isArray(raw) && raw.length === 0 ? 'an empty array' : String(raw);
	      throw Object.assign(
	        new TypeError(`Cannot parse ${msg}`),
	        { code: 'EJSONPARSE', systemError: e }
	      )
	    }
	    throw new JSONParseError(e, txt, context, parseJsonError)
	  }
	};

	lib$a = parseJsonError;
	parseJsonError.JSONParseError = JSONParseError;
	parseJsonError.noExceptions = (raw, reviver) => {
	  try {
	    return parseJson(stripBOM(raw), reviver)
	  } catch {
	    // no exceptions
	  }
	};
	return lib$a;
}

var updateDependencies_1;
var hasRequiredUpdateDependencies;

function requireUpdateDependencies () {
	if (hasRequiredUpdateDependencies) return updateDependencies_1;
	hasRequiredUpdateDependencies = 1;
	const depTypes = new Set([
	  'dependencies',
	  'optionalDependencies',
	  'devDependencies',
	  'peerDependencies',
	]);

	// sort alphabetically all types of deps for a given package
	const orderDeps = (content) => {
	  for (const type of depTypes) {
	    if (content && content[type]) {
	      content[type] = Object.keys(content[type])
	        .sort((a, b) => a.localeCompare(b, 'en'))
	        .reduce((res, key) => {
	          res[key] = content[type][key];
	          return res
	        }, {});
	    }
	  }
	  return content
	};

	const updateDependencies = ({ content, originalContent }) => {
	  const pkg = orderDeps({
	    ...content,
	  });

	  // optionalDependencies don't need to be repeated in two places
	  if (pkg.dependencies) {
	    if (pkg.optionalDependencies) {
	      for (const name of Object.keys(pkg.optionalDependencies)) {
	        delete pkg.dependencies[name];
	      }
	    }
	  }

	  const result = { ...originalContent };

	  // loop through all types of dependencies and update package json pkg
	  for (const type of depTypes) {
	    if (pkg[type]) {
	      result[type] = pkg[type];
	    }

	    // prune empty type props from resulting object
	    const emptyDepType =
	      pkg[type]
	      && typeof pkg === 'object'
	      && Object.keys(pkg[type]).length === 0;
	    if (emptyDepType) {
	      delete result[type];
	    }
	  }

	  // if original package.json had dep in peerDeps AND deps, preserve that.
	  const { dependencies: origProd, peerDependencies: origPeer } =
	    originalContent || {};
	  const { peerDependencies: newPeer } = result;
	  if (origProd && origPeer && newPeer) {
	    // we have original prod/peer deps, and new peer deps
	    // copy over any that were in both in the original
	    for (const name of Object.keys(origPeer)) {
	      if (origProd[name] !== undefined && newPeer[name] !== undefined) {
	        result.dependencies = result.dependencies || {};
	        result.dependencies[name] = newPeer[name];
	      }
	    }
	  }

	  return result
	};

	updateDependencies.knownKeys = depTypes;

	updateDependencies_1 = updateDependencies;
	return updateDependencies_1;
}

var updateScripts_1;
var hasRequiredUpdateScripts;

function requireUpdateScripts () {
	if (hasRequiredUpdateScripts) return updateScripts_1;
	hasRequiredUpdateScripts = 1;
	const updateScripts = ({ content, originalContent = {} }) => {
	  const newScripts = content.scripts;

	  if (!newScripts) {
	    return originalContent
	  }

	  // validate scripts content being appended
	  const hasInvalidScripts = () =>
	    Object.entries(newScripts)
	      .some(([key, value]) =>
	        typeof key !== 'string' || typeof value !== 'string');
	  if (hasInvalidScripts()) {
	    throw Object.assign(
	      new TypeError(
	        'package.json scripts should be a key-value pair of strings.'),
	      { code: 'ESCRIPTSINVALID' }
	    )
	  }

	  return {
	    ...originalContent,
	    scripts: {
	      ...newScripts,
	    },
	  }
	};

	updateScripts_1 = updateScripts;
	return updateScripts_1;
}

var updateWorkspaces_1;
var hasRequiredUpdateWorkspaces;

function requireUpdateWorkspaces () {
	if (hasRequiredUpdateWorkspaces) return updateWorkspaces_1;
	hasRequiredUpdateWorkspaces = 1;
	const updateWorkspaces = ({ content, originalContent = {} }) => {
	  const newWorkspaces = content.workspaces;

	  if (!newWorkspaces) {
	    return originalContent
	  }

	  // validate workspaces content being appended
	  const hasInvalidWorkspaces = () =>
	    newWorkspaces.some(w => !(typeof w === 'string'));
	  if (!newWorkspaces.length || hasInvalidWorkspaces()) {
	    throw Object.assign(
	      new TypeError('workspaces should be an array of strings.'),
	      { code: 'EWORKSPACESINVALID' }
	    )
	  }

	  return {
	    ...originalContent,
	    workspaces: [
	      ...newWorkspaces,
	    ],
	  }
	};

	updateWorkspaces_1 = updateWorkspaces;
	return updateWorkspaces_1;
}

var cjs = {};

var posix = {};

var hasRequiredPosix;

function requirePosix () {
	if (hasRequiredPosix) return posix;
	hasRequiredPosix = 1;
	/**
	 * This is the Posix implementation of isexe, which uses the file
	 * mode and uid/gid values.
	 *
	 * @module
	 */
	Object.defineProperty(posix, "__esModule", { value: true });
	posix.sync = posix.isexe = void 0;
	const fs_1 = require$$0$4;
	const promises_1 = require$$0$3;
	/**
	 * Determine whether a path is executable according to the mode and
	 * current (or specified) user and group IDs.
	 */
	const isexe = async (path, options = {}) => {
	    const { ignoreErrors = false } = options;
	    try {
	        return checkStat(await (0, promises_1.stat)(path), options);
	    }
	    catch (e) {
	        const er = e;
	        if (ignoreErrors || er.code === 'EACCES')
	            return false;
	        throw er;
	    }
	};
	posix.isexe = isexe;
	/**
	 * Synchronously determine whether a path is executable according to
	 * the mode and current (or specified) user and group IDs.
	 */
	const sync = (path, options = {}) => {
	    const { ignoreErrors = false } = options;
	    try {
	        return checkStat((0, fs_1.statSync)(path), options);
	    }
	    catch (e) {
	        const er = e;
	        if (ignoreErrors || er.code === 'EACCES')
	            return false;
	        throw er;
	    }
	};
	posix.sync = sync;
	const checkStat = (stat, options) => stat.isFile() && checkMode(stat, options);
	const checkMode = (stat, options) => {
	    const myUid = options.uid ?? process.getuid?.();
	    const myGroups = options.groups ?? process.getgroups?.() ?? [];
	    const myGid = options.gid ?? process.getgid?.() ?? myGroups[0];
	    if (myUid === undefined || myGid === undefined) {
	        throw new Error('cannot get uid or gid');
	    }
	    const groups = new Set([myGid, ...myGroups]);
	    const mod = stat.mode;
	    const uid = stat.uid;
	    const gid = stat.gid;
	    const u = parseInt('100', 8);
	    const g = parseInt('010', 8);
	    const o = parseInt('001', 8);
	    const ug = u | g;
	    return !!(mod & o ||
	        (mod & g && groups.has(gid)) ||
	        (mod & u && uid === myUid) ||
	        (mod & ug && myUid === 0));
	};
	
	return posix;
}

var win32 = {};

var hasRequiredWin32;

function requireWin32 () {
	if (hasRequiredWin32) return win32;
	hasRequiredWin32 = 1;
	/**
	 * This is the Windows implementation of isexe, which uses the file
	 * extension and PATHEXT setting.
	 *
	 * @module
	 */
	Object.defineProperty(win32, "__esModule", { value: true });
	win32.sync = win32.isexe = void 0;
	const fs_1 = require$$0$4;
	const promises_1 = require$$0$3;
	/**
	 * Determine whether a path is executable based on the file extension
	 * and PATHEXT environment variable (or specified pathExt option)
	 */
	const isexe = async (path, options = {}) => {
	    const { ignoreErrors = false } = options;
	    try {
	        return checkStat(await (0, promises_1.stat)(path), path, options);
	    }
	    catch (e) {
	        const er = e;
	        if (ignoreErrors || er.code === 'EACCES')
	            return false;
	        throw er;
	    }
	};
	win32.isexe = isexe;
	/**
	 * Synchronously determine whether a path is executable based on the file
	 * extension and PATHEXT environment variable (or specified pathExt option)
	 */
	const sync = (path, options = {}) => {
	    const { ignoreErrors = false } = options;
	    try {
	        return checkStat((0, fs_1.statSync)(path), path, options);
	    }
	    catch (e) {
	        const er = e;
	        if (ignoreErrors || er.code === 'EACCES')
	            return false;
	        throw er;
	    }
	};
	win32.sync = sync;
	const checkPathExt = (path, options) => {
	    const { pathExt = process.env.PATHEXT || '' } = options;
	    const peSplit = pathExt.split(';');
	    if (peSplit.indexOf('') !== -1) {
	        return true;
	    }
	    for (let i = 0; i < peSplit.length; i++) {
	        const p = peSplit[i].toLowerCase();
	        const ext = path.substring(path.length - p.length).toLowerCase();
	        if (p && ext === p) {
	            return true;
	        }
	    }
	    return false;
	};
	const checkStat = (stat, path, options) => stat.isFile() && checkPathExt(path, options);
	
	return win32;
}

var options = {};

var hasRequiredOptions;

function requireOptions () {
	if (hasRequiredOptions) return options;
	hasRequiredOptions = 1;
	Object.defineProperty(options, "__esModule", { value: true });
	
	return options;
}

var hasRequiredCjs;

function requireCjs () {
	if (hasRequiredCjs) return cjs;
	hasRequiredCjs = 1;
	(function (exports) {
		var __createBinding = (cjs && cjs.__createBinding) || (Object.create ? (function(o, m, k, k2) {
		    if (k2 === undefined) k2 = k;
		    var desc = Object.getOwnPropertyDescriptor(m, k);
		    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
		      desc = { enumerable: true, get: function() { return m[k]; } };
		    }
		    Object.defineProperty(o, k2, desc);
		}) : (function(o, m, k, k2) {
		    if (k2 === undefined) k2 = k;
		    o[k2] = m[k];
		}));
		var __setModuleDefault = (cjs && cjs.__setModuleDefault) || (Object.create ? (function(o, v) {
		    Object.defineProperty(o, "default", { enumerable: true, value: v });
		}) : function(o, v) {
		    o["default"] = v;
		});
		var __importStar = (cjs && cjs.__importStar) || function (mod) {
		    if (mod && mod.__esModule) return mod;
		    var result = {};
		    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
		    __setModuleDefault(result, mod);
		    return result;
		};
		var __exportStar = (cjs && cjs.__exportStar) || function(m, exports) {
		    for (var p in m) if (p !== "default" && !Object.prototype.hasOwnProperty.call(exports, p)) __createBinding(exports, m, p);
		};
		Object.defineProperty(exports, "__esModule", { value: true });
		exports.sync = exports.isexe = exports.posix = exports.win32 = void 0;
		const posix = __importStar(requirePosix());
		exports.posix = posix;
		const win32 = __importStar(requireWin32());
		exports.win32 = win32;
		__exportStar(requireOptions(), exports);
		const platform = process.env._ISEXE_TEST_PLATFORM_ || process.platform;
		const impl = platform === 'win32' ? win32 : posix;
		/**
		 * Determine whether a path is executable on the current platform.
		 */
		exports.isexe = impl.isexe;
		/**
		 * Synchronously determine whether a path is executable on the
		 * current platform.
		 */
		exports.sync = impl.sync;
		
	} (cjs));
	return cjs;
}

var lib$9;
var hasRequiredLib$9;

function requireLib$9 () {
	if (hasRequiredLib$9) return lib$9;
	hasRequiredLib$9 = 1;
	const { isexe, sync: isexeSync } = requireCjs();
	const { join, delimiter, sep, posix } = require$$0;

	const isWindows = process.platform === 'win32';

	// used to check for slashed in commands passed in. always checks for the posix
	// seperator on all platforms, and checks for the current separator when not on
	// a posix platform. don't use the isWindows check for this since that is mocked
	// in tests but we still need the code to actually work when called. that is also
	// why it is ignored from coverage.
	/* istanbul ignore next */
	const rSlash = new RegExp(`[${posix.sep}${sep === posix.sep ? '' : sep}]`.replace(/(\\)/g, '\\$1'));
	const rRel = new RegExp(`^\\.${rSlash.source}`);

	const getNotFoundError = (cmd) =>
	  Object.assign(new Error(`not found: ${cmd}`), { code: 'ENOENT' });

	const getPathInfo = (cmd, {
	  path: optPath = process.env.PATH,
	  pathExt: optPathExt = process.env.PATHEXT,
	  delimiter: optDelimiter = delimiter,
	}) => {
	  // If it has a slash, then we don't bother searching the pathenv.
	  // just check the file itself, and that's it.
	  const pathEnv = cmd.match(rSlash) ? [''] : [
	    // windows always checks the cwd first
	    ...(isWindows ? [process.cwd()] : []),
	    ...(optPath || /* istanbul ignore next: very unusual */ '').split(optDelimiter),
	  ];

	  if (isWindows) {
	    const pathExtExe = optPathExt ||
	      ['.EXE', '.CMD', '.BAT', '.COM'].join(optDelimiter);
	    const pathExt = pathExtExe.split(optDelimiter).flatMap((item) => [item, item.toLowerCase()]);
	    if (cmd.includes('.') && pathExt[0] !== '') {
	      pathExt.unshift('');
	    }
	    return { pathEnv, pathExt, pathExtExe }
	  }

	  return { pathEnv, pathExt: [''] }
	};

	const getPathPart = (raw, cmd) => {
	  const pathPart = /^".*"$/.test(raw) ? raw.slice(1, -1) : raw;
	  const prefix = !pathPart && rRel.test(cmd) ? cmd.slice(0, 2) : '';
	  return prefix + join(pathPart, cmd)
	};

	const which = async (cmd, opt = {}) => {
	  const { pathEnv, pathExt, pathExtExe } = getPathInfo(cmd, opt);
	  const found = [];

	  for (const envPart of pathEnv) {
	    const p = getPathPart(envPart, cmd);

	    for (const ext of pathExt) {
	      const withExt = p + ext;
	      const is = await isexe(withExt, { pathExt: pathExtExe, ignoreErrors: true });
	      if (is) {
	        if (!opt.all) {
	          return withExt
	        }
	        found.push(withExt);
	      }
	    }
	  }

	  if (opt.all && found.length) {
	    return found
	  }

	  if (opt.nothrow) {
	    return null
	  }

	  throw getNotFoundError(cmd)
	};

	const whichSync = (cmd, opt = {}) => {
	  const { pathEnv, pathExt, pathExtExe } = getPathInfo(cmd, opt);
	  const found = [];

	  for (const pathEnvPart of pathEnv) {
	    const p = getPathPart(pathEnvPart, cmd);

	    for (const ext of pathExt) {
	      const withExt = p + ext;
	      const is = isexeSync(withExt, { pathExt: pathExtExe, ignoreErrors: true });
	      if (is) {
	        if (!opt.all) {
	          return withExt
	        }
	        found.push(withExt);
	      }
	    }
	  }

	  if (opt.all && found.length) {
	    return found
	  }

	  if (opt.nothrow) {
	    return null
	  }

	  throw getNotFoundError(cmd)
	};

	lib$9 = which;
	which.sync = whichSync;
	return lib$9;
}

var _escape;
var hasRequired_escape;

function require_escape () {
	if (hasRequired_escape) return _escape;
	hasRequired_escape = 1;

	// eslint-disable-next-line max-len
	// this code adapted from: https://blogs.msdn.microsoft.com/twistylittlepassagesallalike/2011/04/23/everyone-quotes-command-line-arguments-the-wrong-way/
	const cmd = (input, doubleEscape) => {
	  if (!input.length) {
	    return '""'
	  }

	  let result;
	  if (!/[ \t\n\v"]/.test(input)) {
	    result = input;
	  } else {
	    result = '"';
	    for (let i = 0; i <= input.length; ++i) {
	      let slashCount = 0;
	      while (input[i] === '\\') {
	        ++i;
	        ++slashCount;
	      }

	      if (i === input.length) {
	        result += '\\'.repeat(slashCount * 2);
	        break
	      }

	      if (input[i] === '"') {
	        result += '\\'.repeat(slashCount * 2 + 1);
	        result += input[i];
	      } else {
	        result += '\\'.repeat(slashCount);
	        result += input[i];
	      }
	    }
	    result += '"';
	  }

	  // and finally, prefix shell meta chars with a ^
	  result = result.replace(/[ !%^&()<>|"]/g, '^$&');
	  if (doubleEscape) {
	    result = result.replace(/[ !%^&()<>|"]/g, '^$&');
	  }

	  return result
	};

	const sh = (input) => {
	  if (!input.length) {
	    return `''`
	  }

	  if (!/[\t\n\r "#$&'()*;<>?\\`|~]/.test(input)) {
	    return input
	  }

	  // replace single quotes with '\'' and wrap the whole result in a fresh set of quotes
	  const result = `'${input.replace(/'/g, `'\\''`)}'`
	    // if the input string already had single quotes around it, clean those up
	    .replace(/^(?:'')+(?!$)/, '')
	    .replace(/\\'''/g, `\\'`);

	  return result
	};

	_escape = {
	  cmd,
	  sh,
	};
	return _escape;
}

var lib$8;
var hasRequiredLib$8;

function requireLib$8 () {
	if (hasRequiredLib$8) return lib$8;
	hasRequiredLib$8 = 1;

	const { spawn } = require$$0$5;
	const os = require$$1$2;
	const which = requireLib$9();

	const escape = require_escape();

	// 'extra' object is for decorating the error a bit more
	const promiseSpawn = (cmd, args, opts = {}, extra = {}) => {
	  if (opts.shell) {
	    return spawnWithShell(cmd, args, opts, extra)
	  }

	  let resolve, reject;
	  const promise = new Promise((_resolve, _reject) => {
	    resolve = _resolve;
	    reject = _reject;
	  });

	  // Create error here so we have a more useful stack trace when rejecting
	  const closeError = new Error('command failed');

	  const stdout = [];
	  const stderr = [];

	  const getResult = (result) => ({
	    cmd,
	    args,
	    ...result,
	    ...stdioResult(stdout, stderr, opts),
	    ...extra,
	  });
	  const rejectWithOpts = (er, erOpts) => {
	    const resultError = getResult(erOpts);
	    reject(Object.assign(er, resultError));
	  };

	  const proc = spawn(cmd, args, opts);
	  promise.stdin = proc.stdin;
	  promise.process = proc;

	  proc.on('error', rejectWithOpts);

	  if (proc.stdout) {
	    proc.stdout.on('data', c => stdout.push(c));
	    proc.stdout.on('error', rejectWithOpts);
	  }

	  if (proc.stderr) {
	    proc.stderr.on('data', c => stderr.push(c));
	    proc.stderr.on('error', rejectWithOpts);
	  }

	  proc.on('close', (code, signal) => {
	    if (code || signal) {
	      rejectWithOpts(closeError, { code, signal });
	    } else {
	      resolve(getResult({ code, signal }));
	    }
	  });

	  return promise
	};

	const spawnWithShell = (cmd, args, opts, extra) => {
	  let command = opts.shell;
	  // if shell is set to true, we use a platform default. we can't let the core
	  // spawn method decide this for us because we need to know what shell is in use
	  // ahead of time so that we can escape arguments properly. we don't need coverage here.
	  if (command === true) {
	    // istanbul ignore next
	    command = process.platform === 'win32' ? process.env.ComSpec : 'sh';
	  }

	  const options = { ...opts, shell: false };
	  const realArgs = [];
	  let script = cmd;

	  // first, determine if we're in windows because if we are we need to know if we're
	  // running an .exe or a .cmd/.bat since the latter requires extra escaping
	  const isCmd = /(?:^|\\)cmd(?:\.exe)?$/i.test(command);
	  if (isCmd) {
	    let doubleEscape = false;

	    // find the actual command we're running
	    let initialCmd = '';
	    let insideQuotes = false;
	    for (let i = 0; i < cmd.length; ++i) {
	      const char = cmd.charAt(i);
	      if (char === ' ' && !insideQuotes) {
	        break
	      }

	      initialCmd += char;
	      if (char === '"' || char === "'") {
	        insideQuotes = !insideQuotes;
	      }
	    }

	    let pathToInitial;
	    try {
	      pathToInitial = which.sync(initialCmd, {
	        path: (options.env && findInObject(options.env, 'PATH')) || process.env.PATH,
	        pathext: (options.env && findInObject(options.env, 'PATHEXT')) || process.env.PATHEXT,
	      }).toLowerCase();
	    } catch (err) {
	      pathToInitial = initialCmd.toLowerCase();
	    }

	    doubleEscape = pathToInitial.endsWith('.cmd') || pathToInitial.endsWith('.bat');
	    for (const arg of args) {
	      script += ` ${escape.cmd(arg, doubleEscape)}`;
	    }
	    realArgs.push('/d', '/s', '/c', script);
	    options.windowsVerbatimArguments = true;
	  } else {
	    for (const arg of args) {
	      script += ` ${escape.sh(arg)}`;
	    }
	    realArgs.push('-c', script);
	  }

	  return promiseSpawn(command, realArgs, options, extra)
	};

	// open a file with the default application as defined by the user's OS
	const open = (_args, opts = {}, extra = {}) => {
	  const options = { ...opts, shell: true };
	  const args = [].concat(_args);

	  let platform = process.platform;
	  // process.platform === 'linux' may actually indicate WSL, if that's the case
	  // open the argument with sensible-browser which is pre-installed
	  // In WSL, set the default browser using, for example,
	  // export BROWSER="/mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe"
	  // or
	  // export BROWSER="/mnt/c/Program Files (x86)/Microsoft/Edge/Application/msedge.exe"
	  // To permanently set the default browser, add the appropriate entry to your shell's
	  // RC file, e.g. .bashrc or .zshrc.
	  if (platform === 'linux' && os.release().toLowerCase().includes('microsoft')) {
	    platform = 'wsl';
	    if (!process.env.BROWSER) {
	      return Promise.reject(
	        new Error('Set the BROWSER environment variable to your desired browser.'))
	    }
	  }

	  let command = options.command;
	  if (!command) {
	    if (platform === 'win32') {
	      // spawnWithShell does not do the additional os.release() check, so we
	      // have to force the shell here to make sure we treat WSL as windows.
	      options.shell = process.env.ComSpec;
	      // also, the start command accepts a title so to make sure that we don't
	      // accidentally interpret the first arg as the title, we stick an empty
	      // string immediately after the start command
	      command = 'start ""';
	    } else if (platform === 'wsl') {
	      command = 'sensible-browser';
	    } else if (platform === 'darwin') {
	      command = 'open';
	    } else {
	      command = 'xdg-open';
	    }
	  }

	  return spawnWithShell(command, args, options, extra)
	};
	promiseSpawn.open = open;

	const isPipe = (stdio = 'pipe', fd) => {
	  if (stdio === 'pipe' || stdio === null) {
	    return true
	  }

	  if (Array.isArray(stdio)) {
	    return isPipe(stdio[fd], fd)
	  }

	  return false
	};

	const stdioResult = (stdout, stderr, { stdioString = true, stdio }) => {
	  const result = {
	    stdout: null,
	    stderr: null,
	  };

	  // stdio is [stdin, stdout, stderr]
	  if (isPipe(stdio, 1)) {
	    result.stdout = Buffer.concat(stdout);
	    if (stdioString) {
	      result.stdout = result.stdout.toString().trim();
	    }
	  }

	  if (isPipe(stdio, 2)) {
	    result.stderr = Buffer.concat(stderr);
	    if (stdioString) {
	      result.stderr = result.stderr.toString().trim();
	    }
	  }

	  return result
	};

	// case insensitive lookup in an object
	const findInObject = (obj, key) => {
	  key = key.toLowerCase();
	  for (const objKey of Object.keys(obj).sort()) {
	    if (objKey.toLowerCase() === key) {
	      return obj[objKey]
	    }
	  }
	};

	lib$8 = promiseSpawn;
	return lib$8;
}

var errors;
var hasRequiredErrors;

function requireErrors () {
	if (hasRequiredErrors) return errors;
	hasRequiredErrors = 1;
	const maxRetry = 3;

	class GitError extends Error {
	  shouldRetry () {
	    return false
	  }
	}

	class GitConnectionError extends GitError {
	  constructor () {
	    super('A git connection error occurred');
	  }

	  shouldRetry (number) {
	    return number < maxRetry
	  }
	}

	class GitPathspecError extends GitError {
	  constructor () {
	    super('The git reference could not be found');
	  }
	}

	class GitUnknownError extends GitError {
	  constructor () {
	    super('An unknown git error occurred');
	  }
	}

	errors = {
	  GitConnectionError,
	  GitPathspecError,
	  GitUnknownError,
	};
	return errors;
}

var makeError_1;
var hasRequiredMakeError;

function requireMakeError () {
	if (hasRequiredMakeError) return makeError_1;
	hasRequiredMakeError = 1;
	const {
	  GitConnectionError,
	  GitPathspecError,
	  GitUnknownError,
	} = requireErrors();

	const connectionErrorRe = new RegExp([
	  'remote error: Internal Server Error',
	  'The remote end hung up unexpectedly',
	  'Connection timed out',
	  'Operation timed out',
	  'Failed to connect to .* Timed out',
	  'Connection reset by peer',
	  'SSL_ERROR_SYSCALL',
	  'The requested URL returned error: 503',
	].join('|'));

	const missingPathspecRe = /pathspec .* did not match any file\(s\) known to git/;

	function makeError (er) {
	  const message = er.stderr;
	  let gitEr;
	  if (connectionErrorRe.test(message)) {
	    gitEr = new GitConnectionError(message);
	  } else if (missingPathspecRe.test(message)) {
	    gitEr = new GitPathspecError(message);
	  } else {
	    gitEr = new GitUnknownError(message);
	  }
	  return Object.assign(gitEr, er)
	}

	makeError_1 = makeError;
	return makeError_1;
}

var opts = {exports: {}};

var hasRequiredOpts;

function requireOpts () {
	if (hasRequiredOpts) return opts.exports;
	hasRequiredOpts = 1;
	const fs = require$$4;
	const os = require$$1$3;
	const path = require$$2$2;
	const ini = requireIni();

	const gitConfigPath = path.join(os.homedir(), '.gitconfig');

	let cachedConfig = null;

	// Function to load and cache the git config
	const loadGitConfig = () => {
	  if (cachedConfig === null) {
	    try {
	      cachedConfig = {};
	      if (fs.existsSync(gitConfigPath)) {
	        const configContent = fs.readFileSync(gitConfigPath, 'utf-8');
	        cachedConfig = ini.parse(configContent);
	      }
	    } catch (error) {
	      cachedConfig = {};
	    }
	  }
	  return cachedConfig
	};

	const checkGitConfigs = () => {
	  const config = loadGitConfig();
	  return {
	    sshCommandSetInConfig: config?.core?.sshCommand !== undefined,
	    askPassSetInConfig: config?.core?.askpass !== undefined,
	  }
	};

	const sshCommandSetInEnv = process.env.GIT_SSH_COMMAND !== undefined;
	const askPassSetInEnv = process.env.GIT_ASKPASS !== undefined;
	const { sshCommandSetInConfig, askPassSetInConfig } = checkGitConfigs();

	// Values we want to set if they're not already defined by the end user
	// This defaults to accepting new ssh host key fingerprints
	const finalGitEnv = {
	  ...(askPassSetInEnv || askPassSetInConfig ? {} : {
	    GIT_ASKPASS: 'echo',
	  }),
	  ...(sshCommandSetInEnv || sshCommandSetInConfig ? {} : {
	    GIT_SSH_COMMAND: 'ssh -oStrictHostKeyChecking=accept-new',
	  }),
	};

	opts.exports = (opts = {}) => ({
	  stdioString: true,
	  ...opts,
	  shell: false,
	  env: opts.env || { ...finalGitEnv, ...process.env },
	});

	// Export the loadGitConfig function for testing
	opts.exports.loadGitConfig = loadGitConfig;
	return opts.exports;
}

var which_1;
var hasRequiredWhich;

function requireWhich () {
	if (hasRequiredWhich) return which_1;
	hasRequiredWhich = 1;
	const which = requireLib$9();

	let gitPath;
	try {
	  gitPath = which.sync('git');
	} catch {
	  // ignore errors
	}

	which_1 = (opts = {}) => {
	  if (opts.git) {
	    return opts.git
	  }
	  if (!gitPath || opts.git === false) {
	    return Object.assign(new Error('No git binary found in $PATH'), { code: 'ENOGIT' })
	  }
	  return gitPath
	};
	return which_1;
}

var spawn_1;
var hasRequiredSpawn;

function requireSpawn () {
	if (hasRequiredSpawn) return spawn_1;
	hasRequiredSpawn = 1;
	const spawn = requireLib$8();
	const promiseRetry = requirePromiseRetry();
	const { log } = requireLib$c();
	const makeError = requireMakeError();
	const makeOpts = requireOpts();

	spawn_1 = (gitArgs, opts = {}) => {
	  const whichGit = requireWhich();
	  const gitPath = whichGit(opts);

	  if (gitPath instanceof Error) {
	    return Promise.reject(gitPath)
	  }

	  // undocumented option, mostly only here for tests
	  const args = opts.allowReplace || gitArgs[0] === '--no-replace-objects'
	    ? gitArgs
	    : ['--no-replace-objects', ...gitArgs];

	  let retryOpts = opts.retry;
	  if (retryOpts === null || retryOpts === undefined) {
	    retryOpts = {
	      retries: opts.fetchRetries || 2,
	      factor: opts.fetchRetryFactor || 10,
	      maxTimeout: opts.fetchRetryMaxtimeout || 60000,
	      minTimeout: opts.fetchRetryMintimeout || 1000,
	    };
	  }
	  return promiseRetry((retryFn, number) => {
	    if (number !== 1) {
	      log.silly('git', `Retrying git command: ${
	        args.join(' ')} attempt # ${number}`);
	    }

	    return spawn(gitPath, args, makeOpts(opts))
	      .catch(er => {
	        const gitError = makeError(er);
	        if (!gitError.shouldRetry(number)) {
	          throw gitError
	        }
	        retryFn(gitError);
	      })
	  }, retryOpts)
	};
	return spawn_1;
}

var linesToRevs;
var hasRequiredLinesToRevs;

function requireLinesToRevs () {
	if (hasRequiredLinesToRevs) return linesToRevs;
	hasRequiredLinesToRevs = 1;
	// turn an array of lines from `git ls-remote` into a thing
	// vaguely resembling a packument, where docs are a resolved ref

	const semver = requireSemver();

	linesToRevs = lines => finish(lines.reduce(linesToRevsReducer, {
	  versions: {},
	  'dist-tags': {},
	  refs: {},
	  shas: {},
	}));

	const finish = revs => distTags(shaList(peelTags(revs)));

	// We can check out shallow clones on specific SHAs if we have a ref
	const shaList = revs => {
	  Object.keys(revs.refs).forEach(ref => {
	    const doc = revs.refs[ref];
	    if (!revs.shas[doc.sha]) {
	      revs.shas[doc.sha] = [ref];
	    } else {
	      revs.shas[doc.sha].push(ref);
	    }
	  });
	  return revs
	};

	// Replace any tags with their ^{} counterparts, if those exist
	const peelTags = revs => {
	  Object.keys(revs.refs).filter(ref => ref.endsWith('^{}')).forEach(ref => {
	    const peeled = revs.refs[ref];
	    const unpeeled = revs.refs[ref.replace(/\^\{\}$/, '')];
	    if (unpeeled) {
	      unpeeled.sha = peeled.sha;
	      delete revs.refs[ref];
	    }
	  });
	  return revs
	};

	const distTags = revs => {
	  // not entirely sure what situations would result in an
	  // ichabod repo, but best to be careful in Sleepy Hollow anyway
	  const HEAD = revs.refs.HEAD || /* istanbul ignore next */ {};
	  const versions = Object.keys(revs.versions);
	  versions.forEach(v => {
	    // simulate a dist-tags with latest pointing at the
	    // 'latest' branch if one exists and is a version,
	    // or HEAD if not.
	    const ver = revs.versions[v];
	    if (revs.refs.latest && ver.sha === revs.refs.latest.sha) {
	      revs['dist-tags'].latest = v;
	    } else if (ver.sha === HEAD.sha) {
	      revs['dist-tags'].HEAD = v;
	      if (!revs.refs.latest) {
	        revs['dist-tags'].latest = v;
	      }
	    }
	  });
	  return revs
	};

	const refType = ref => {
	  if (ref.startsWith('refs/tags/')) {
	    return 'tag'
	  }
	  if (ref.startsWith('refs/heads/')) {
	    return 'branch'
	  }
	  if (ref.startsWith('refs/pull/')) {
	    return 'pull'
	  }
	  if (ref === 'HEAD') {
	    return 'head'
	  }
	  // Could be anything, ignore for now
	  /* istanbul ignore next */
	  return 'other'
	};

	// return the doc, or null if we should ignore it.
	const lineToRevDoc = line => {
	  const split = line.trim().split(/\s+/, 2);
	  if (split.length < 2) {
	    return null
	  }

	  const sha = split[0].trim();
	  const rawRef = split[1].trim();
	  const type = refType(rawRef);

	  if (type === 'tag') {
	    // refs/tags/foo^{} is the 'peeled tag', ie the commit
	    // that is tagged by refs/tags/foo they resolve to the same
	    // content, just different objects in git's data structure.
	    // But, we care about the thing the tag POINTS to, not the tag
	    // object itself, so we only look at the peeled tag refs, and
	    // ignore the pointer.
	    // For now, though, we have to save both, because some tags
	    // don't have peels, if they were not annotated.
	    const ref = rawRef.slice('refs/tags/'.length);
	    return { sha, ref, rawRef, type }
	  }

	  if (type === 'branch') {
	    const ref = rawRef.slice('refs/heads/'.length);
	    return { sha, ref, rawRef, type }
	  }

	  if (type === 'pull') {
	    // NB: merged pull requests installable with #pull/123/merge
	    // for the merged pr, or #pull/123 for the PR head
	    const ref = rawRef.slice('refs/'.length).replace(/\/head$/, '');
	    return { sha, ref, rawRef, type }
	  }

	  if (type === 'head') {
	    const ref = 'HEAD';
	    return { sha, ref, rawRef, type }
	  }

	  // at this point, all we can do is leave the ref un-munged
	  return { sha, ref: rawRef, rawRef, type }
	};

	const linesToRevsReducer = (revs, line) => {
	  const doc = lineToRevDoc(line);

	  if (!doc) {
	    return revs
	  }

	  revs.refs[doc.ref] = doc;
	  revs.refs[doc.rawRef] = doc;

	  if (doc.type === 'tag') {
	    // try to pull a semver value out of tags like `release-v1.2.3`
	    // which is a pretty common pattern.
	    const match = !doc.ref.endsWith('^{}') &&
	      doc.ref.match(/v?(\d+\.\d+\.\d+(?:[-+].+)?)$/);
	    if (match && semver.valid(match[1], true)) {
	      revs.versions[semver.clean(match[1], true)] = doc;
	    }
	  }

	  return revs
	};
	return linesToRevs;
}

var revs;
var hasRequiredRevs;

function requireRevs () {
	if (hasRequiredRevs) return revs;
	hasRequiredRevs = 1;
	const spawn = requireSpawn();
	const { LRUCache } = /*@__PURE__*/ requireCommonjs$1();
	const linesToRevs = requireLinesToRevs();

	const revsCache = new LRUCache({
	  max: 100,
	  ttl: 5 * 60 * 1000,
	});

	revs = async (repo, opts = {}) => {
	  if (!opts.noGitRevCache) {
	    const cached = revsCache.get(repo);
	    if (cached) {
	      return cached
	    }
	  }

	  const { stdout } = await spawn(['ls-remote', repo], opts);
	  const revs = linesToRevs(stdout.trim().split('\n'));
	  revsCache.set(repo, revs);
	  return revs
	};
	return revs;
}

var utils = {};

var hasRequiredUtils;

function requireUtils () {
	if (hasRequiredUtils) return utils;
	hasRequiredUtils = 1;
	const isWindows = opts => (opts.fakePlatform || process.platform) === 'win32';

	utils.isWindows = isWindows;
	return utils;
}

var currentEnv;
var hasRequiredCurrentEnv;

function requireCurrentEnv () {
	if (hasRequiredCurrentEnv) return currentEnv;
	hasRequiredCurrentEnv = 1;
	const process = process$1;
	const nodeOs = require$$1$3;
	const fs = require$$4;

	function isMusl (file) {
	  return file.includes('libc.musl-') || file.includes('ld-musl-')
	}

	function os () {
	  return process.platform
	}

	function cpu () {
	  return process.arch
	}

	const LDD_PATH = '/usr/bin/ldd';
	function getFamilyFromFilesystem () {
	  try {
	    const content = fs.readFileSync(LDD_PATH, 'utf-8');
	    if (content.includes('musl')) {
	      return 'musl'
	    }
	    if (content.includes('GNU C Library')) {
	      return 'glibc'
	    }
	    return null
	  } catch {
	    return undefined
	  }
	}

	function getFamilyFromReport () {
	  const originalExclude = process.report.excludeNetwork;
	  process.report.excludeNetwork = true;
	  const report = process.report.getReport();
	  process.report.excludeNetwork = originalExclude;
	  if (report.header?.glibcVersionRuntime) {
	    family = 'glibc';
	  } else if (Array.isArray(report.sharedObjects) && report.sharedObjects.some(isMusl)) {
	    family = 'musl';
	  } else {
	    family = null;
	  }
	  return family
	}

	let family;
	function libc (osName) {
	  if (osName !== 'linux') {
	    return undefined
	  }
	  if (family === undefined) {
	    family = getFamilyFromFilesystem();
	    if (family === undefined) {
	      family = getFamilyFromReport();
	    }
	  }
	  return family
	}

	function devEngines (env = {}) {
	  const osName = env.os || os();
	  return {
	    cpu: {
	      name: env.cpu || cpu(),
	    },
	    libc: {
	      name: env.libc || libc(osName),
	    },
	    os: {
	      name: osName,
	      version: env.osVersion || nodeOs.release(),
	    },
	    packageManager: {
	      name: 'npm',
	      version: env.npmVersion,
	    },
	    runtime: {
	      name: 'node',
	      version: env.nodeVersion || process.version,
	    },
	  }
	}

	currentEnv = {
	  cpu,
	  libc,
	  os,
	  devEngines,
	};
	return currentEnv;
}

var devEngines;
var hasRequiredDevEngines;

function requireDevEngines () {
	if (hasRequiredDevEngines) return devEngines;
	hasRequiredDevEngines = 1;
	const satisfies = requireSatisfies();
	const validRange = requireValid$1();

	const recognizedOnFail = [
	  'ignore',
	  'warn',
	  'error',
	  'download',
	];

	const recognizedProperties = [
	  'name',
	  'version',
	  'onFail',
	];

	const recognizedEngines = [
	  'packageManager',
	  'runtime',
	  'cpu',
	  'libc',
	  'os',
	];

	/** checks a devEngine dependency */
	function checkDependency (wanted, current, opts) {
	  const { engine } = opts;

	  if ((typeof wanted !== 'object' || wanted === null) || Array.isArray(wanted)) {
	    throw new Error(`Invalid non-object value for "${engine}"`)
	  }

	  const properties = Object.keys(wanted);

	  for (const prop of properties) {
	    if (!recognizedProperties.includes(prop)) {
	      throw new Error(`Invalid property "${prop}" for "${engine}"`)
	    }
	  }

	  if (!properties.includes('name')) {
	    throw new Error(`Missing "name" property for "${engine}"`)
	  }

	  if (typeof wanted.name !== 'string') {
	    throw new Error(`Invalid non-string value for "name" within "${engine}"`)
	  }

	  if (typeof current.name !== 'string' || current.name === '') {
	    throw new Error(`Unable to determine "name" for "${engine}"`)
	  }

	  if (properties.includes('onFail')) {
	    if (typeof wanted.onFail !== 'string') {
	      throw new Error(`Invalid non-string value for "onFail" within "${engine}"`)
	    }
	    if (!recognizedOnFail.includes(wanted.onFail)) {
	      throw new Error(`Invalid onFail value "${wanted.onFail}" for "${engine}"`)
	    }
	  }

	  if (wanted.name !== current.name) {
	    return new Error(
	      `Invalid name "${wanted.name}" does not match "${current.name}" for "${engine}"`
	    )
	  }

	  if (properties.includes('version')) {
	    if (typeof wanted.version !== 'string') {
	      throw new Error(`Invalid non-string value for "version" within "${engine}"`)
	    }
	    if (typeof current.version !== 'string' || current.version === '') {
	      throw new Error(`Unable to determine "version" for "${engine}" "${wanted.name}"`)
	    }
	    if (validRange(wanted.version)) {
	      if (!satisfies(current.version, wanted.version, opts.semver)) {
	        return new Error(
	          // eslint-disable-next-line max-len
	          `Invalid semver version "${wanted.version}" does not match "${current.version}" for "${engine}"`
	        )
	      }
	    } else if (wanted.version !== current.version) {
	      return new Error(
	        `Invalid version "${wanted.version}" does not match "${current.version}" for "${engine}"`
	      )
	    }
	  }
	}

	/** checks devEngines package property and returns array of warnings / errors */
	function checkDevEngines (wanted, current = {}, opts = {}) {
	  if ((typeof wanted !== 'object' || wanted === null) || Array.isArray(wanted)) {
	    throw new Error(`Invalid non-object value for devEngines`)
	  }

	  const errors = [];

	  for (const engine of Object.keys(wanted)) {
	    if (!recognizedEngines.includes(engine)) {
	      throw new Error(`Invalid property "${engine}"`)
	    }
	    const dependencyAsAuthored = wanted[engine];
	    const dependencies = [dependencyAsAuthored].flat();
	    const currentEngine = current[engine] || {};

	    // this accounts for empty array eg { runtime: [] } and ignores it
	    if (dependencies.length === 0) {
	      continue
	    }

	    const depErrors = [];
	    for (const dep of dependencies) {
	      const result = checkDependency(dep, currentEngine, { ...opts, engine });
	      if (result) {
	        depErrors.push(result);
	      }
	    }

	    const invalid = depErrors.length === dependencies.length;

	    if (invalid) {
	      const lastDependency = dependencies[dependencies.length - 1];
	      let onFail = lastDependency.onFail || 'error';
	      if (onFail === 'download') {
	        onFail = 'error';
	      }

	      const err = Object.assign(new Error(`Invalid engine "${engine}"`), {
	        errors: depErrors,
	        engine,
	        isWarn: onFail === 'warn',
	        isError: onFail === 'error',
	        current: currentEngine,
	        required: dependencyAsAuthored,
	      });

	      errors.push(err);
	    }
	  }
	  return errors
	}

	devEngines = {
	  checkDevEngines,
	};
	return devEngines;
}

var lib$7;
var hasRequiredLib$7;

function requireLib$7 () {
	if (hasRequiredLib$7) return lib$7;
	hasRequiredLib$7 = 1;
	const semver = requireSemver();
	const currentEnv = requireCurrentEnv();
	const { checkDevEngines } = requireDevEngines();

	const checkEngine = (target, npmVer, nodeVer, force = false) => {
	  const nodev = force ? null : nodeVer;
	  const eng = target.engines;
	  const opt = { includePrerelease: true };
	  if (!eng) {
	    return
	  }

	  const nodeFail = nodev && eng.node && !semver.satisfies(nodev, eng.node, opt);
	  const npmFail = npmVer && eng.npm && !semver.satisfies(npmVer, eng.npm, opt);
	  if (nodeFail || npmFail) {
	    throw Object.assign(new Error('Unsupported engine'), {
	      pkgid: target._id,
	      current: { node: nodeVer, npm: npmVer },
	      required: eng,
	      code: 'EBADENGINE',
	    })
	  }
	};

	const checkPlatform = (target, force = false, environment = {}) => {
	  if (force) {
	    return
	  }

	  const os = environment.os || currentEnv.os();
	  const cpu = environment.cpu || currentEnv.cpu();
	  const libc = environment.libc || currentEnv.libc(os);

	  const osOk = target.os ? checkList(os, target.os) : true;
	  const cpuOk = target.cpu ? checkList(cpu, target.cpu) : true;
	  let libcOk = target.libc ? checkList(libc, target.libc) : true;
	  if (target.libc && !libc) {
	    libcOk = false;
	  }

	  if (!osOk || !cpuOk || !libcOk) {
	    throw Object.assign(new Error('Unsupported platform'), {
	      pkgid: target._id,
	      current: {
	        os,
	        cpu,
	        libc,
	      },
	      required: {
	        os: target.os,
	        cpu: target.cpu,
	        libc: target.libc,
	      },
	      code: 'EBADPLATFORM',
	    })
	  }
	};

	const checkList = (value, list) => {
	  if (typeof list === 'string') {
	    list = [list];
	  }
	  if (list.length === 1 && list[0] === 'any') {
	    return true
	  }
	  // match none of the negated values, and at least one of the
	  // non-negated values, if any are present.
	  let negated = 0;
	  let match = false;
	  for (const entry of list) {
	    const negate = entry.charAt(0) === '!';
	    const test = negate ? entry.slice(1) : entry;
	    if (negate) {
	      negated++;
	      if (value === test) {
	        return false
	      }
	    } else {
	      match = match || value === test;
	    }
	  }
	  return match || negated === list.length
	};

	lib$7 = {
	  checkEngine,
	  checkPlatform,
	  checkDevEngines,
	  currentEnv,
	};
	return lib$7;
}

var lib$6;
var hasRequiredLib$6;

function requireLib$6 () {
	if (hasRequiredLib$6) return lib$6;
	hasRequiredLib$6 = 1;
	// pass in a manifest with a 'bin' field here, and it'll turn it
	// into a properly santized bin object
	const { join, basename } = require$$0;

	const normalize = pkg =>
	  !pkg.bin ? removeBin(pkg)
	  : typeof pkg.bin === 'string' ? normalizeString(pkg)
	  : Array.isArray(pkg.bin) ? normalizeArray(pkg)
	  : typeof pkg.bin === 'object' ? normalizeObject(pkg)
	  : removeBin(pkg);

	const normalizeString = pkg => {
	  if (!pkg.name) {
	    return removeBin(pkg)
	  }
	  pkg.bin = { [pkg.name]: pkg.bin };
	  return normalizeObject(pkg)
	};

	const normalizeArray = pkg => {
	  pkg.bin = pkg.bin.reduce((acc, k) => {
	    acc[basename(k)] = k;
	    return acc
	  }, {});
	  return normalizeObject(pkg)
	};

	const removeBin = pkg => {
	  delete pkg.bin;
	  return pkg
	};

	const normalizeObject = pkg => {
	  const orig = pkg.bin;
	  const clean = {};
	  let hasBins = false;
	  Object.keys(orig).forEach(binKey => {
	    const base = join('/', basename(binKey.replace(/\\|:/g, '/'))).slice(1);

	    if (typeof orig[binKey] !== 'string' || !base) {
	      return
	    }

	    const binTarget = join('/', orig[binKey].replace(/\\/g, '/'))
	      .replace(/\\/g, '/').slice(1);

	    if (!binTarget) {
	      return
	    }

	    clean[base] = binTarget;
	    hasBins = true;
	  });

	  if (hasBins) {
	    pkg.bin = clean;
	  } else {
	    delete pkg.bin;
	  }

	  return pkg
	};

	lib$6 = normalize;
	return lib$6;
}

var lib$5;
var hasRequiredLib$5;

function requireLib$5 () {
	if (hasRequiredLib$5) return lib$5;
	hasRequiredLib$5 = 1;

	const npa = requireNpa();
	const semver = requireSemver();
	const { checkEngine } = requireLib$7();
	const normalizeBin = requireLib$6();

	const engineOk = (manifest, npmVersion, nodeVersion) => {
	  try {
	    checkEngine(manifest, npmVersion, nodeVersion);
	    return true
	  } catch (_) {
	    return false
	  }
	};

	const isBefore = (verTimes, ver, time) =>
	  !verTimes || !verTimes[ver] || Date.parse(verTimes[ver]) <= time;

	const avoidSemverOpt = { includePrerelease: true, loose: true };
	const shouldAvoid = (ver, avoid) =>
	  avoid && semver.satisfies(ver, avoid, avoidSemverOpt);

	const decorateAvoid = (result, avoid) =>
	  result && shouldAvoid(result.version, avoid)
	    ? { ...result, _shouldAvoid: true }
	    : result;

	const pickManifest = (packument, wanted, opts) => {
	  const {
	    defaultTag = 'latest',
	    before = null,
	    nodeVersion = process.version,
	    npmVersion = null,
	    includeStaged = false,
	    avoid = null,
	    avoidStrict = false,
	  } = opts;

	  const { name, time: verTimes } = packument;
	  const versions = packument.versions || {};

	  if (avoidStrict) {
	    const looseOpts = {
	      ...opts,
	      avoidStrict: false,
	    };

	    const result = pickManifest(packument, wanted, looseOpts);
	    if (!result || !result._shouldAvoid) {
	      return result
	    }

	    const caret = pickManifest(packument, `^${result.version}`, looseOpts);
	    if (!caret || !caret._shouldAvoid) {
	      return {
	        ...caret,
	        _outsideDependencyRange: true,
	        _isSemVerMajor: false,
	      }
	    }

	    const star = pickManifest(packument, '*', looseOpts);
	    if (!star || !star._shouldAvoid) {
	      return {
	        ...star,
	        _outsideDependencyRange: true,
	        _isSemVerMajor: true,
	      }
	    }

	    throw Object.assign(new Error(`No avoidable versions for ${name}`), {
	      code: 'ETARGET',
	      name,
	      wanted,
	      avoid,
	      before,
	      versions: Object.keys(versions),
	    })
	  }

	  const staged = (includeStaged && packument.stagedVersions &&
	    packument.stagedVersions.versions) || {};
	  const restricted = (packument.policyRestrictions &&
	    packument.policyRestrictions.versions) || {};

	  const time = before && verTimes ? +(new Date(before)) : Infinity;
	  const spec = npa.resolve(name, wanted || defaultTag);
	  const type = spec.type;
	  const distTags = packument['dist-tags'] || {};

	  if (type !== 'tag' && type !== 'version' && type !== 'range') {
	    throw new Error('Only tag, version, and range are supported')
	  }

	  // if the type is 'tag', and not just the implicit default, then it must
	  // be that exactly, or nothing else will do.
	  if (wanted && type === 'tag') {
	    const ver = distTags[wanted];
	    // if the version in the dist-tags is before the before date, then
	    // we use that.  Otherwise, we get the highest precedence version
	    // prior to the dist-tag.
	    if (isBefore(verTimes, ver, time)) {
	      return decorateAvoid(versions[ver] || staged[ver] || restricted[ver], avoid)
	    } else {
	      return pickManifest(packument, `<=${ver}`, opts)
	    }
	  }

	  // similarly, if a specific version, then only that version will do
	  if (wanted && type === 'version') {
	    const ver = semver.clean(wanted, { loose: true });
	    const mani = versions[ver] || staged[ver] || restricted[ver];
	    return isBefore(verTimes, ver, time) ? decorateAvoid(mani, avoid) : null
	  }

	  // ok, sort based on our heuristics, and pick the best fit
	  const range = type === 'range' ? wanted : '*';

	  // if the range is *, then we prefer the 'latest' if available
	  // but skip this if it should be avoided, in that case we have
	  // to try a little harder.
	  const defaultVer = distTags[defaultTag];
	  if (defaultVer &&
	      (range === '*' || semver.satisfies(defaultVer, range, { loose: true })) &&
	      !restricted[defaultVer] &&
	      !shouldAvoid(defaultVer, avoid)) {
	    const mani = versions[defaultVer];
	    const ok = mani &&
	      isBefore(verTimes, defaultVer, time) &&
	      engineOk(mani, npmVersion, nodeVersion) &&
	      !mani.deprecated &&
	      !staged[defaultVer];
	    if (ok) {
	      return mani
	    }
	  }

	  // ok, actually have to sort the list and take the winner
	  const allEntries = Object.entries(versions)
	    .concat(Object.entries(staged))
	    .concat(Object.entries(restricted))
	    .filter(([ver]) => isBefore(verTimes, ver, time));

	  if (!allEntries.length) {
	    throw Object.assign(new Error(`No versions available for ${name}`), {
	      code: 'ENOVERSIONS',
	      name,
	      type,
	      wanted,
	      before,
	      versions: Object.keys(versions),
	    })
	  }

	  const sortSemverOpt = { loose: true };
	  const entries = allEntries.filter(([ver]) =>
	    semver.satisfies(ver, range, { loose: true }))
	    .sort((a, b) => {
	      const [vera, mania] = a;
	      const [verb, manib] = b;
	      const notavoida = !shouldAvoid(vera, avoid);
	      const notavoidb = !shouldAvoid(verb, avoid);
	      const notrestra = !restricted[vera];
	      const notrestrb = !restricted[verb];
	      const notstagea = !staged[vera];
	      const notstageb = !staged[verb];
	      const notdepra = !mania.deprecated;
	      const notdeprb = !manib.deprecated;
	      const enginea = engineOk(mania, npmVersion, nodeVersion);
	      const engineb = engineOk(manib, npmVersion, nodeVersion);
	      // sort by:
	      // - not an avoided version
	      // - not restricted
	      // - not staged
	      // - not deprecated and engine ok
	      // - engine ok
	      // - not deprecated
	      // - semver
	      return (notavoidb - notavoida) ||
	        (notrestrb - notrestra) ||
	        (notstageb - notstagea) ||
	        ((notdeprb && engineb) - (notdepra && enginea)) ||
	        (engineb - enginea) ||
	        (notdeprb - notdepra) ||
	        semver.rcompare(vera, verb, sortSemverOpt)
	    });

	  return decorateAvoid(entries[0] && entries[0][1], avoid)
	};

	lib$5 = (packument, wanted, opts = {}) => {
	  const mani = pickManifest(packument, wanted, opts);
	  const picked = mani && normalizeBin(mani);
	  const policyRestrictions = packument.policyRestrictions;
	  const restricted = (policyRestrictions && policyRestrictions.versions) || {};

	  if (picked && !restricted[picked.version]) {
	    return picked
	  }

	  const { before = null, defaultTag = 'latest' } = opts;
	  const bstr = before ? new Date(before).toLocaleString() : '';
	  const { name } = packument;
	  const pckg = `${name}@${wanted}` +
	    (before ? ` with a date before ${bstr}` : '');

	  const isForbidden = picked && !!restricted[picked.version];
	  const polMsg = isForbidden ? policyRestrictions.message : '';

	  const msg = !isForbidden ? `No matching version found for ${pckg}.`
	    : `Could not download ${pckg} due to policy violations:\n${polMsg}`;

	  const code = isForbidden ? 'E403' : 'ETARGET';
	  throw Object.assign(new Error(msg), {
	    code,
	    type: npa.resolve(packument.name, wanted).type,
	    wanted,
	    versions: Object.keys(packument.versions ?? {}),
	    name,
	    distTags: packument['dist-tags'],
	    defaultTag,
	  })
	};
	return lib$5;
}

var clone_1;
var hasRequiredClone;

function requireClone () {
	if (hasRequiredClone) return clone_1;
	hasRequiredClone = 1;
	// The goal here is to minimize both git workload and
	// the number of refs we download over the network.
	//
	// Every method ends up with the checked out working dir
	// at the specified ref, and resolves with the git sha.

	// Only certain whitelisted hosts get shallow cloning.
	// Many hosts (including GHE) don't always support it.
	// A failed shallow fetch takes a LOT longer than a full
	// fetch in most cases, so we skip it entirely.
	// Set opts.gitShallow = true/false to force this behavior
	// one way or the other.
	const shallowHosts = new Set([
	  'github.com',
	  'gist.github.com',
	  'gitlab.com',
	  'bitbucket.com',
	  'bitbucket.org',
	]);
	// we have to use url.parse until we add the same shim that hosted-git-info has
	// to handle scp:// urls
	const { parse } = require$$0$1; // eslint-disable-line node/no-deprecated-api
	const path = require$$0;

	const getRevs = requireRevs();
	const spawn = requireSpawn();
	const { isWindows } = requireUtils();

	const pickManifest = requireLib$5();
	const fs = require$$0$3;

	clone_1 = (repo, ref = 'HEAD', target = null, opts = {}) =>
	  getRevs(repo, opts).then(revs => clone(
	    repo,
	    revs,
	    ref,
	    resolveRef(revs, ref, opts),
	    target || defaultTarget(repo, opts.cwd),
	    opts
	  ));

	const maybeShallow = (repo, opts) => {
	  if (opts.gitShallow === false || opts.gitShallow) {
	    return opts.gitShallow
	  }
	  return shallowHosts.has(parse(repo).host)
	};

	const defaultTarget = (repo, /* istanbul ignore next */ cwd = process.cwd()) =>
	  path.resolve(cwd, path.basename(repo.replace(/[/\\]?\.git$/, '')));

	const clone = (repo, revs, ref, revDoc, target, opts) => {
	  if (!revDoc) {
	    return unresolved(repo, ref, target, opts)
	  }
	  if (revDoc.sha === revs.refs.HEAD.sha) {
	    return plain(repo, revDoc, target, opts)
	  }
	  if (revDoc.type === 'tag' || revDoc.type === 'branch') {
	    return branch(repo, revDoc, target, opts)
	  }
	  return other(repo, revDoc, target, opts)
	};

	const resolveRef = (revs, ref, opts) => {
	  const { spec = {} } = opts;
	  ref = spec.gitCommittish || ref;
	  /* istanbul ignore next - will fail anyway, can't pull */
	  if (!revs) {
	    return null
	  }
	  if (spec.gitRange) {
	    return pickManifest(revs, spec.gitRange, opts)
	  }
	  if (!ref) {
	    return revs.refs.HEAD
	  }
	  if (revs.refs[ref]) {
	    return revs.refs[ref]
	  }
	  if (revs.shas[ref]) {
	    return revs.refs[revs.shas[ref][0]]
	  }
	  return null
	};

	// pull request or some other kind of advertised ref
	const other = (repo, revDoc, target, opts) => {
	  const shallow = maybeShallow(repo, opts);

	  const fetchOrigin = ['fetch', 'origin', revDoc.rawRef]
	    .concat(shallow ? ['--depth=1'] : []);

	  const git = (args) => spawn(args, { ...opts, cwd: target });
	  return fs.mkdir(target, { recursive: true })
	    .then(() => git(['init']))
	    .then(() => isWindows(opts)
	      ? git(['config', '--local', '--add', 'core.longpaths', 'true'])
	      : null)
	    .then(() => git(['remote', 'add', 'origin', repo]))
	    .then(() => git(fetchOrigin))
	    .then(() => git(['checkout', revDoc.sha]))
	    .then(() => updateSubmodules(target, opts))
	    .then(() => revDoc.sha)
	};

	// tag or branches.  use -b
	const branch = (repo, revDoc, target, opts) => {
	  const args = [
	    'clone',
	    '-b',
	    revDoc.ref,
	    repo,
	    target,
	    '--recurse-submodules',
	  ];
	  if (maybeShallow(repo, opts)) {
	    args.push('--depth=1');
	  }
	  if (isWindows(opts)) {
	    args.push('--config', 'core.longpaths=true');
	  }
	  return spawn(args, opts).then(() => revDoc.sha)
	};

	// just the head.  clone it
	const plain = (repo, revDoc, target, opts) => {
	  const args = [
	    'clone',
	    repo,
	    target,
	    '--recurse-submodules',
	  ];
	  if (maybeShallow(repo, opts)) {
	    args.push('--depth=1');
	  }
	  if (isWindows(opts)) {
	    args.push('--config', 'core.longpaths=true');
	  }
	  return spawn(args, opts).then(() => revDoc.sha)
	};

	const updateSubmodules = async (target, opts) => {
	  const hasSubmodules = await fs.stat(`${target}/.gitmodules`)
	    .then(() => true)
	    .catch(() => false);
	  if (!hasSubmodules) {
	    return null
	  }
	  return spawn([
	    'submodule',
	    'update',
	    '-q',
	    '--init',
	    '--recursive',
	  ], { ...opts, cwd: target })
	};

	const unresolved = (repo, ref, target, opts) => {
	  // can't do this one shallowly, because the ref isn't advertised
	  // but we can avoid checking out the working dir twice, at least
	  const lp = isWindows(opts) ? ['--config', 'core.longpaths=true'] : [];
	  const cloneArgs = ['clone', '--mirror', '-q', repo, target + '/.git'];
	  const git = (args) => spawn(args, { ...opts, cwd: target });
	  return fs.mkdir(target, { recursive: true })
	    .then(() => git(cloneArgs.concat(lp)))
	    .then(() => git(['init']))
	    .then(() => git(['checkout', ref]))
	    .then(() => updateSubmodules(target, opts))
	    .then(() => git(['rev-parse', '--revs-only', 'HEAD']))
	    .then(({ stdout }) => stdout.trim())
	};
	return clone_1;
}

var is;
var hasRequiredIs;

function requireIs () {
	if (hasRequiredIs) return is;
	hasRequiredIs = 1;
	// not an airtight indicator, but a good gut-check to even bother trying
	const { stat } = require$$0$3;
	is = ({ cwd = process.cwd() } = {}) =>
	  stat(cwd + '/.git').then(() => true, () => false);
	return is;
}

var find;
var hasRequiredFind;

function requireFind () {
	if (hasRequiredFind) return find;
	hasRequiredFind = 1;
	const is = requireIs();
	const { dirname } = require$$0;

	find = async ({ cwd = process.cwd(), root } = {}) => {
	  while (true) {
	    if (await is({ cwd })) {
	      return cwd
	    }
	    const next = dirname(cwd);
	    if (cwd === root || cwd === next) {
	      return null
	    }
	    cwd = next;
	  }
	};
	return find;
}

var isClean;
var hasRequiredIsClean;

function requireIsClean () {
	if (hasRequiredIsClean) return isClean;
	hasRequiredIsClean = 1;
	const spawn = requireSpawn();

	isClean = (opts = {}) =>
	  spawn(['status', '--porcelain=v1', '-uno'], opts)
	    .then(res => !res.stdout.trim().split(/\r?\n+/)
	      .map(l => l.trim()).filter(l => l).length);
	return isClean;
}

var lib$4;
var hasRequiredLib$4;

function requireLib$4 () {
	if (hasRequiredLib$4) return lib$4;
	hasRequiredLib$4 = 1;
	lib$4 = {
	  clone: requireClone(),
	  revs: requireRevs(),
	  spawn: requireSpawn(),
	  is: requireIs(),
	  find: requireFind(),
	  isClean: requireIsClean(),
	  errors: requireErrors(),
	};
	return lib$4;
}

const require$$1$1 = ["0BSD","3D-Slicer-1.0","AAL","ADSL","AFL-1.1","AFL-1.2","AFL-2.0","AFL-2.1","AFL-3.0","AGPL-1.0-only","AGPL-1.0-or-later","AGPL-3.0-only","AGPL-3.0-or-later","AMD-newlib","AMDPLPA","AML","AML-glslang","AMPAS","ANTLR-PD","ANTLR-PD-fallback","APAFML","APL-1.0","APSL-1.0","APSL-1.1","APSL-1.2","APSL-2.0","ASWF-Digital-Assets-1.0","ASWF-Digital-Assets-1.1","Abstyles","AdaCore-doc","Adobe-2006","Adobe-Display-PostScript","Adobe-Glyph","Adobe-Utopia","Afmparse","Aladdin","Apache-1.0","Apache-1.1","Apache-2.0","App-s2p","Arphic-1999","Artistic-1.0","Artistic-1.0-Perl","Artistic-1.0-cl8","Artistic-2.0","BSD-1-Clause","BSD-2-Clause","BSD-2-Clause-Darwin","BSD-2-Clause-Patent","BSD-2-Clause-Views","BSD-2-Clause-first-lines","BSD-3-Clause","BSD-3-Clause-Attribution","BSD-3-Clause-Clear","BSD-3-Clause-HP","BSD-3-Clause-LBNL","BSD-3-Clause-Modification","BSD-3-Clause-No-Military-License","BSD-3-Clause-No-Nuclear-License","BSD-3-Clause-No-Nuclear-License-2014","BSD-3-Clause-No-Nuclear-Warranty","BSD-3-Clause-Open-MPI","BSD-3-Clause-Sun","BSD-3-Clause-acpica","BSD-3-Clause-flex","BSD-4-Clause","BSD-4-Clause-Shortened","BSD-4-Clause-UC","BSD-4.3RENO","BSD-4.3TAHOE","BSD-Advertising-Acknowledgement","BSD-Attribution-HPND-disclaimer","BSD-Inferno-Nettverk","BSD-Protection","BSD-Source-Code","BSD-Source-beginning-file","BSD-Systemics","BSD-Systemics-W3Works","BSL-1.0","BUSL-1.1","Baekmuk","Bahyph","Barr","Beerware","BitTorrent-1.0","BitTorrent-1.1","Bitstream-Charter","Bitstream-Vera","BlueOak-1.0.0","Boehm-GC","Boehm-GC-without-fee","Borceux","Brian-Gladman-2-Clause","Brian-Gladman-3-Clause","C-UDA-1.0","CAL-1.0","CAL-1.0-Combined-Work-Exception","CATOSL-1.1","CC-BY-1.0","CC-BY-2.0","CC-BY-2.5","CC-BY-2.5-AU","CC-BY-3.0","CC-BY-3.0-AT","CC-BY-3.0-AU","CC-BY-3.0-DE","CC-BY-3.0-IGO","CC-BY-3.0-NL","CC-BY-3.0-US","CC-BY-4.0","CC-BY-NC-1.0","CC-BY-NC-2.0","CC-BY-NC-2.5","CC-BY-NC-3.0","CC-BY-NC-3.0-DE","CC-BY-NC-4.0","CC-BY-NC-ND-1.0","CC-BY-NC-ND-2.0","CC-BY-NC-ND-2.5","CC-BY-NC-ND-3.0","CC-BY-NC-ND-3.0-DE","CC-BY-NC-ND-3.0-IGO","CC-BY-NC-ND-4.0","CC-BY-NC-SA-1.0","CC-BY-NC-SA-2.0","CC-BY-NC-SA-2.0-DE","CC-BY-NC-SA-2.0-FR","CC-BY-NC-SA-2.0-UK","CC-BY-NC-SA-2.5","CC-BY-NC-SA-3.0","CC-BY-NC-SA-3.0-DE","CC-BY-NC-SA-3.0-IGO","CC-BY-NC-SA-4.0","CC-BY-ND-1.0","CC-BY-ND-2.0","CC-BY-ND-2.5","CC-BY-ND-3.0","CC-BY-ND-3.0-DE","CC-BY-ND-4.0","CC-BY-SA-1.0","CC-BY-SA-2.0","CC-BY-SA-2.0-UK","CC-BY-SA-2.1-JP","CC-BY-SA-2.5","CC-BY-SA-3.0","CC-BY-SA-3.0-AT","CC-BY-SA-3.0-DE","CC-BY-SA-3.0-IGO","CC-BY-SA-4.0","CC-PDDC","CC-PDM-1.0","CC-SA-1.0","CC0-1.0","CDDL-1.0","CDDL-1.1","CDL-1.0","CDLA-Permissive-1.0","CDLA-Permissive-2.0","CDLA-Sharing-1.0","CECILL-1.0","CECILL-1.1","CECILL-2.0","CECILL-2.1","CECILL-B","CECILL-C","CERN-OHL-1.1","CERN-OHL-1.2","CERN-OHL-P-2.0","CERN-OHL-S-2.0","CERN-OHL-W-2.0","CFITSIO","CMU-Mach","CMU-Mach-nodoc","CNRI-Jython","CNRI-Python","CNRI-Python-GPL-Compatible","COIL-1.0","CPAL-1.0","CPL-1.0","CPOL-1.02","CUA-OPL-1.0","Caldera","Caldera-no-preamble","Catharon","ClArtistic","Clips","Community-Spec-1.0","Condor-1.1","Cornell-Lossless-JPEG","Cronyx","Crossword","CrystalStacker","Cube","D-FSL-1.0","DEC-3-Clause","DL-DE-BY-2.0","DL-DE-ZERO-2.0","DOC","DRL-1.0","DRL-1.1","DSDP","DocBook-Schema","DocBook-Stylesheet","DocBook-XML","Dotseqn","ECL-1.0","ECL-2.0","EFL-1.0","EFL-2.0","EPICS","EPL-1.0","EPL-2.0","EUDatagrid","EUPL-1.0","EUPL-1.1","EUPL-1.2","Elastic-2.0","Entessa","ErlPL-1.1","Eurosym","FBM","FDK-AAC","FSFAP","FSFAP-no-warranty-disclaimer","FSFUL","FSFULLR","FSFULLRWD","FTL","Fair","Ferguson-Twofish","Frameworx-1.0","FreeBSD-DOC","FreeImage","Furuseth","GCR-docs","GD","GFDL-1.1-invariants-only","GFDL-1.1-invariants-or-later","GFDL-1.1-no-invariants-only","GFDL-1.1-no-invariants-or-later","GFDL-1.1-only","GFDL-1.1-or-later","GFDL-1.2-invariants-only","GFDL-1.2-invariants-or-later","GFDL-1.2-no-invariants-only","GFDL-1.2-no-invariants-or-later","GFDL-1.2-only","GFDL-1.2-or-later","GFDL-1.3-invariants-only","GFDL-1.3-invariants-or-later","GFDL-1.3-no-invariants-only","GFDL-1.3-no-invariants-or-later","GFDL-1.3-only","GFDL-1.3-or-later","GL2PS","GLWTPL","GPL-1.0-only","GPL-1.0-or-later","GPL-2.0-only","GPL-2.0-or-later","GPL-3.0-only","GPL-3.0-or-later","Giftware","Glide","Glulxe","Graphics-Gems","Gutmann","HIDAPI","HP-1986","HP-1989","HPND","HPND-DEC","HPND-Fenneberg-Livingston","HPND-INRIA-IMAG","HPND-Intel","HPND-Kevlin-Henney","HPND-MIT-disclaimer","HPND-Markus-Kuhn","HPND-Netrek","HPND-Pbmplus","HPND-UC","HPND-UC-export-US","HPND-doc","HPND-doc-sell","HPND-export-US","HPND-export-US-acknowledgement","HPND-export-US-modify","HPND-export2-US","HPND-merchantability-variant","HPND-sell-MIT-disclaimer-xserver","HPND-sell-regexpr","HPND-sell-variant","HPND-sell-variant-MIT-disclaimer","HPND-sell-variant-MIT-disclaimer-rev","HTMLTIDY","HaskellReport","Hippocratic-2.1","IBM-pibs","ICU","IEC-Code-Components-EULA","IJG","IJG-short","IPA","IPL-1.0","ISC","ISC-Veillard","ImageMagick","Imlib2","Info-ZIP","Inner-Net-2.0","InnoSetup","Intel","Intel-ACPI","Interbase-1.0","JPL-image","JPNIC","JSON","Jam","JasPer-2.0","Kastrup","Kazlib","Knuth-CTAN","LAL-1.2","LAL-1.3","LGPL-2.0-only","LGPL-2.0-or-later","LGPL-2.1-only","LGPL-2.1-or-later","LGPL-3.0-only","LGPL-3.0-or-later","LGPLLR","LOOP","LPD-document","LPL-1.0","LPL-1.02","LPPL-1.0","LPPL-1.1","LPPL-1.2","LPPL-1.3a","LPPL-1.3c","LZMA-SDK-9.11-to-9.20","LZMA-SDK-9.22","Latex2e","Latex2e-translated-notice","Leptonica","LiLiQ-P-1.1","LiLiQ-R-1.1","LiLiQ-Rplus-1.1","Libpng","Linux-OpenIB","Linux-man-pages-1-para","Linux-man-pages-copyleft","Linux-man-pages-copyleft-2-para","Linux-man-pages-copyleft-var","Lucida-Bitmap-Fonts","MIPS","MIT","MIT-0","MIT-CMU","MIT-Click","MIT-Festival","MIT-Khronos-old","MIT-Modern-Variant","MIT-Wu","MIT-advertising","MIT-enna","MIT-feh","MIT-open-group","MIT-testregex","MITNFA","MMIXware","MPEG-SSG","MPL-1.0","MPL-1.1","MPL-2.0","MPL-2.0-no-copyleft-exception","MS-LPL","MS-PL","MS-RL","MTLL","Mackerras-3-Clause","Mackerras-3-Clause-acknowledgment","MakeIndex","Martin-Birgmeier","McPhee-slideshow","Minpack","MirOS","Motosoto","MulanPSL-1.0","MulanPSL-2.0","Multics","Mup","NAIST-2003","NASA-1.3","NBPL-1.0","NCBI-PD","NCGL-UK-2.0","NCL","NCSA","NGPL","NICTA-1.0","NIST-PD","NIST-PD-fallback","NIST-Software","NLOD-1.0","NLOD-2.0","NLPL","NOSL","NPL-1.0","NPL-1.1","NPOSL-3.0","NRL","NTP","NTP-0","Naumen","NetCDF","Newsletr","Nokia","Noweb","O-UDA-1.0","OAR","OCCT-PL","OCLC-2.0","ODC-By-1.0","ODbL-1.0","OFFIS","OFL-1.0","OFL-1.0-RFN","OFL-1.0-no-RFN","OFL-1.1","OFL-1.1-RFN","OFL-1.1-no-RFN","OGC-1.0","OGDL-Taiwan-1.0","OGL-Canada-2.0","OGL-UK-1.0","OGL-UK-2.0","OGL-UK-3.0","OGTSL","OLDAP-1.1","OLDAP-1.2","OLDAP-1.3","OLDAP-1.4","OLDAP-2.0","OLDAP-2.0.1","OLDAP-2.1","OLDAP-2.2","OLDAP-2.2.1","OLDAP-2.2.2","OLDAP-2.3","OLDAP-2.4","OLDAP-2.5","OLDAP-2.6","OLDAP-2.7","OLDAP-2.8","OLFL-1.3","OML","OPL-1.0","OPL-UK-3.0","OPUBL-1.0","OSET-PL-2.1","OSL-1.0","OSL-1.1","OSL-2.0","OSL-2.1","OSL-3.0","OpenPBS-2.3","OpenSSL","OpenSSL-standalone","OpenVision","PADL","PDDL-1.0","PHP-3.0","PHP-3.01","PPL","PSF-2.0","Parity-6.0.0","Parity-7.0.0","Pixar","Plexus","PolyForm-Noncommercial-1.0.0","PolyForm-Small-Business-1.0.0","PostgreSQL","Python-2.0","Python-2.0.1","QPL-1.0","QPL-1.0-INRIA-2004","Qhull","RHeCos-1.1","RPL-1.1","RPL-1.5","RPSL-1.0","RSA-MD","RSCPL","Rdisc","Ruby","Ruby-pty","SAX-PD","SAX-PD-2.0","SCEA","SGI-B-1.0","SGI-B-1.1","SGI-B-2.0","SGI-OpenGL","SGP4","SHL-0.5","SHL-0.51","SISSL","SISSL-1.2","SL","SMAIL-GPL","SMLNJ","SMPPL","SNIA","SPL-1.0","SSH-OpenSSH","SSH-short","SSLeay-standalone","SSPL-1.0","SWL","Saxpath","SchemeReport","Sendmail","Sendmail-8.23","Sendmail-Open-Source-1.1","SimPL-2.0","Sleepycat","Soundex","Spencer-86","Spencer-94","Spencer-99","SugarCRM-1.1.3","Sun-PPP","Sun-PPP-2000","SunPro","Symlinks","TAPR-OHL-1.0","TCL","TCP-wrappers","TGPPL-1.0","TMate","TORQUE-1.1","TOSL","TPDL","TPL-1.0","TTWL","TTYP0","TU-Berlin-1.0","TU-Berlin-2.0","TermReadKey","ThirdEye","TrustedQSL","UCAR","UCL-1.0","UMich-Merit","UPL-1.0","URT-RLE","Ubuntu-font-1.0","Unicode-3.0","Unicode-DFS-2015","Unicode-DFS-2016","Unicode-TOU","UnixCrypt","Unlicense","VOSTROM","VSL-1.0","Vim","W3C","W3C-19980720","W3C-20150513","WTFPL","Watcom-1.0","Widget-Workshop","Wsuipa","X11","X11-distribute-modifications-variant","X11-swapped","XFree86-1.1","XSkat","Xdebug-1.03","Xerox","Xfig","Xnet","YPL-1.0","YPL-1.1","ZPL-1.1","ZPL-2.0","ZPL-2.1","Zed","Zeeff","Zend-2.0","Zimbra-1.3","Zimbra-1.4","Zlib","any-OSI","any-OSI-perl-modules","bcrypt-Solar-Designer","blessing","bzip2-1.0.6","check-cvs","checkmk","copyleft-next-0.3.0","copyleft-next-0.3.1","curl","cve-tou","diffmark","dtoa","dvipdfm","eGenix","etalab-2.0","fwlw","gSOAP-1.3b","generic-xts","gnuplot","gtkbook","hdparm","iMatix","libpng-2.0","libselinux-1.0","libtiff","libutil-David-Nugent","lsof","magaz","mailprio","metamail","mpi-permissive","mpich2","mplus","pkgconf","pnmstitch","psfrag","psutils","python-ldap","radvd","snprintf","softSurfer","ssh-keyscan","swrule","threeparttable","ulem","w3m","wwl","xinetd","xkeyboard-config-Zinoviev","xlock","xpp","xzoom","zlib-acknowledgement"];

const require$$1 = ["AGPL-1.0","AGPL-3.0","BSD-2-Clause-FreeBSD","BSD-2-Clause-NetBSD","GFDL-1.1","GFDL-1.2","GFDL-1.3","GPL-1.0","GPL-2.0","GPL-2.0-with-GCC-exception","GPL-2.0-with-autoconf-exception","GPL-2.0-with-bison-exception","GPL-2.0-with-classpath-exception","GPL-2.0-with-font-exception","GPL-3.0","GPL-3.0-with-GCC-exception","GPL-3.0-with-autoconf-exception","LGPL-2.0","LGPL-2.1","LGPL-3.0","Net-SNMP","Nunit","StandardML-NJ","bzip2-1.0.5","eCos-2.0","wxWindows"];

const require$$2 = ["389-exception","Asterisk-exception","Autoconf-exception-2.0","Autoconf-exception-3.0","Autoconf-exception-generic","Autoconf-exception-generic-3.0","Autoconf-exception-macro","Bison-exception-1.24","Bison-exception-2.2","Bootloader-exception","Classpath-exception-2.0","CLISP-exception-2.0","cryptsetup-OpenSSL-exception","DigiRule-FOSS-exception","eCos-exception-2.0","Fawkes-Runtime-exception","FLTK-exception","fmt-exception","Font-exception-2.0","freertos-exception-2.0","GCC-exception-2.0","GCC-exception-2.0-note","GCC-exception-3.1","Gmsh-exception","GNAT-exception","GNOME-examples-exception","GNU-compiler-exception","gnu-javamail-exception","GPL-3.0-interface-exception","GPL-3.0-linking-exception","GPL-3.0-linking-source-exception","GPL-CC-1.0","GStreamer-exception-2005","GStreamer-exception-2008","i2p-gpl-java-exception","KiCad-libraries-exception","LGPL-3.0-linking-exception","libpri-OpenH323-exception","Libtool-exception","Linux-syscall-note","LLGPL","LLVM-exception","LZMA-exception","mif-exception","OCaml-LGPL-linking-exception","OCCT-exception-1.0","OpenJDK-assembly-exception-1.0","openvpn-openssl-exception","PS-or-PDF-font-exception-20170817","QPL-1.0-INRIA-2004-exception","Qt-GPL-exception-1.0","Qt-LGPL-exception-1.1","Qwt-exception-1.0","SANE-exception","SHL-2.0","SHL-2.1","stunnel-exception","SWI-exception","Swift-exception","Texinfo-exception","u-boot-exception-2.0","UBDL-exception","Universal-FOSS-exception-1.0","vsftpd-openssl-exception","WxWindows-exception-3.1","x11vnc-openssl-exception"];

var scan;
var hasRequiredScan;

function requireScan () {
	if (hasRequiredScan) return scan;
	hasRequiredScan = 1;

	var licenses = []
	  .concat(require$$1$1)
	  .concat(require$$1);
	var exceptions = require$$2;

	scan = function (source) {
	  var index = 0;

	  function hasMore () {
	    return index < source.length
	  }

	  // `value` can be a regexp or a string.
	  // If it is recognized, the matching source string is returned and
	  // the index is incremented. Otherwise `undefined` is returned.
	  function read (value) {
	    if (value instanceof RegExp) {
	      var chars = source.slice(index);
	      var match = chars.match(value);
	      if (match) {
	        index += match[0].length;
	        return match[0]
	      }
	    } else {
	      if (source.indexOf(value, index) === index) {
	        index += value.length;
	        return value
	      }
	    }
	  }

	  function skipWhitespace () {
	    read(/[ ]*/);
	  }

	  function operator () {
	    var string;
	    var possibilities = ['WITH', 'AND', 'OR', '(', ')', ':', '+'];
	    for (var i = 0; i < possibilities.length; i++) {
	      string = read(possibilities[i]);
	      if (string) {
	        break
	      }
	    }

	    if (string === '+' && index > 1 && source[index - 2] === ' ') {
	      throw new Error('Space before `+`')
	    }

	    return string && {
	      type: 'OPERATOR',
	      string: string
	    }
	  }

	  function idstring () {
	    return read(/[A-Za-z0-9-.]+/)
	  }

	  function expectIdstring () {
	    var string = idstring();
	    if (!string) {
	      throw new Error('Expected idstring at offset ' + index)
	    }
	    return string
	  }

	  function documentRef () {
	    if (read('DocumentRef-')) {
	      var string = expectIdstring();
	      return { type: 'DOCUMENTREF', string: string }
	    }
	  }

	  function licenseRef () {
	    if (read('LicenseRef-')) {
	      var string = expectIdstring();
	      return { type: 'LICENSEREF', string: string }
	    }
	  }

	  function identifier () {
	    var begin = index;
	    var string = idstring();

	    if (licenses.indexOf(string) !== -1) {
	      return {
	        type: 'LICENSE',
	        string: string
	      }
	    } else if (exceptions.indexOf(string) !== -1) {
	      return {
	        type: 'EXCEPTION',
	        string: string
	      }
	    }

	    index = begin;
	  }

	  // Tries to read the next token. Returns `undefined` if no token is
	  // recognized.
	  function parseToken () {
	    // Ordering matters
	    return (
	      operator() ||
	      documentRef() ||
	      licenseRef() ||
	      identifier()
	    )
	  }

	  var tokens = [];
	  while (hasMore()) {
	    skipWhitespace();
	    if (!hasMore()) {
	      break
	    }

	    var token = parseToken();
	    if (!token) {
	      throw new Error('Unexpected `' + source[index] +
	                      '` at offset ' + index)
	    }

	    tokens.push(token);
	  }
	  return tokens
	};
	return scan;
}

var parse;
var hasRequiredParse;

function requireParse () {
	if (hasRequiredParse) return parse;
	hasRequiredParse = 1;

	// The ABNF grammar in the spec is totally ambiguous.
	//
	// This parser follows the operator precedence defined in the
	// `Order of Precedence and Parentheses` section.

	parse = function (tokens) {
	  var index = 0;

	  function hasMore () {
	    return index < tokens.length
	  }

	  function token () {
	    return hasMore() ? tokens[index] : null
	  }

	  function next () {
	    if (!hasMore()) {
	      throw new Error()
	    }
	    index++;
	  }

	  function parseOperator (operator) {
	    var t = token();
	    if (t && t.type === 'OPERATOR' && operator === t.string) {
	      next();
	      return t.string
	    }
	  }

	  function parseWith () {
	    if (parseOperator('WITH')) {
	      var t = token();
	      if (t && t.type === 'EXCEPTION') {
	        next();
	        return t.string
	      }
	      throw new Error('Expected exception after `WITH`')
	    }
	  }

	  function parseLicenseRef () {
	    // TODO: Actually, everything is concatenated into one string
	    // for backward-compatibility but it could be better to return
	    // a nice structure.
	    var begin = index;
	    var string = '';
	    var t = token();
	    if (t.type === 'DOCUMENTREF') {
	      next();
	      string += 'DocumentRef-' + t.string + ':';
	      if (!parseOperator(':')) {
	        throw new Error('Expected `:` after `DocumentRef-...`')
	      }
	    }
	    t = token();
	    if (t.type === 'LICENSEREF') {
	      next();
	      string += 'LicenseRef-' + t.string;
	      return { license: string }
	    }
	    index = begin;
	  }

	  function parseLicense () {
	    var t = token();
	    if (t && t.type === 'LICENSE') {
	      next();
	      var node = { license: t.string };
	      if (parseOperator('+')) {
	        node.plus = true;
	      }
	      var exception = parseWith();
	      if (exception) {
	        node.exception = exception;
	      }
	      return node
	    }
	  }

	  function parseParenthesizedExpression () {
	    var left = parseOperator('(');
	    if (!left) {
	      return
	    }

	    var expr = parseExpression();

	    if (!parseOperator(')')) {
	      throw new Error('Expected `)`')
	    }

	    return expr
	  }

	  function parseAtom () {
	    return (
	      parseParenthesizedExpression() ||
	      parseLicenseRef() ||
	      parseLicense()
	    )
	  }

	  function makeBinaryOpParser (operator, nextParser) {
	    return function parseBinaryOp () {
	      var left = nextParser();
	      if (!left) {
	        return
	      }

	      if (!parseOperator(operator)) {
	        return left
	      }

	      var right = parseBinaryOp();
	      if (!right) {
	        throw new Error('Expected expression')
	      }
	      return {
	        left: left,
	        conjunction: operator.toLowerCase(),
	        right: right
	      }
	    }
	  }

	  var parseAnd = makeBinaryOpParser('AND', parseAtom);
	  var parseExpression = makeBinaryOpParser('OR', parseAnd);

	  var node = parseExpression();
	  if (!node || hasMore()) {
	    throw new Error('Syntax error')
	  }
	  return node
	};
	return parse;
}

var spdxExpressionParse;
var hasRequiredSpdxExpressionParse;

function requireSpdxExpressionParse () {
	if (hasRequiredSpdxExpressionParse) return spdxExpressionParse;
	hasRequiredSpdxExpressionParse = 1;

	var scan = requireScan();
	var parse = requireParse();

	spdxExpressionParse = function (source) {
	  return parse(scan(source))
	};
	return spdxExpressionParse;
}

/*
Copyright spdx-correct.js contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

var spdxCorrect;
var hasRequiredSpdxCorrect;

function requireSpdxCorrect () {
	if (hasRequiredSpdxCorrect) return spdxCorrect;
	hasRequiredSpdxCorrect = 1;
	var parse = requireSpdxExpressionParse();
	var spdxLicenseIds = require$$1$1;

	function valid (string) {
	  try {
	    parse(string);
	    return true
	  } catch (error) {
	    return false
	  }
	}

	// Sorting function that orders the given array of transpositions such
	// that a transposition with the longer pattern comes before a transposition
	// with a shorter pattern. This is to prevent e.g. the transposition
	// ["General Public License", "GPL"] from matching to "Lesser General Public License"
	// before a longer and more accurate transposition ["Lesser General Public License", "LGPL"]
	// has a chance to be recognized.
	function sortTranspositions(a, b) {
	  var length = b[0].length - a[0].length;
	  if (length !== 0) return length
	  return a[0].toUpperCase().localeCompare(b[0].toUpperCase())
	}

	// Common transpositions of license identifier acronyms
	var transpositions = [
	  ['APGL', 'AGPL'],
	  ['Gpl', 'GPL'],
	  ['GLP', 'GPL'],
	  ['APL', 'Apache'],
	  ['ISD', 'ISC'],
	  ['GLP', 'GPL'],
	  ['IST', 'ISC'],
	  ['Claude', 'Clause'],
	  [' or later', '+'],
	  [' International', ''],
	  ['GNU', 'GPL'],
	  ['GUN', 'GPL'],
	  ['+', ''],
	  ['GNU GPL', 'GPL'],
	  ['GNU LGPL', 'LGPL'],
	  ['GNU/GPL', 'GPL'],
	  ['GNU GLP', 'GPL'],
	  ['GNU LESSER GENERAL PUBLIC LICENSE', 'LGPL'],
	  ['GNU Lesser General Public License', 'LGPL'],
	  ['GNU LESSER GENERAL PUBLIC LICENSE', 'LGPL-2.1'],
	  ['GNU Lesser General Public License', 'LGPL-2.1'],
	  ['LESSER GENERAL PUBLIC LICENSE', 'LGPL'],
	  ['Lesser General Public License', 'LGPL'],
	  ['LESSER GENERAL PUBLIC LICENSE', 'LGPL-2.1'],
	  ['Lesser General Public License', 'LGPL-2.1'],
	  ['GNU General Public License', 'GPL'],
	  ['Gnu public license', 'GPL'],
	  ['GNU Public License', 'GPL'],
	  ['GNU GENERAL PUBLIC LICENSE', 'GPL'],
	  ['MTI', 'MIT'],
	  ['Mozilla Public License', 'MPL'],
	  ['Universal Permissive License', 'UPL'],
	  ['WTH', 'WTF'],
	  ['WTFGPL', 'WTFPL'],
	  ['-License', '']
	].sort(sortTranspositions);

	var TRANSPOSED = 0;
	var CORRECT = 1;

	// Simple corrections to nearly valid identifiers.
	var transforms = [
	  // e.g. 'mit'
	  function (argument) {
	    return argument.toUpperCase()
	  },
	  // e.g. 'MIT '
	  function (argument) {
	    return argument.trim()
	  },
	  // e.g. 'M.I.T.'
	  function (argument) {
	    return argument.replace(/\./g, '')
	  },
	  // e.g. 'Apache- 2.0'
	  function (argument) {
	    return argument.replace(/\s+/g, '')
	  },
	  // e.g. 'CC BY 4.0''
	  function (argument) {
	    return argument.replace(/\s+/g, '-')
	  },
	  // e.g. 'LGPLv2.1'
	  function (argument) {
	    return argument.replace('v', '-')
	  },
	  // e.g. 'Apache 2.0'
	  function (argument) {
	    return argument.replace(/,?\s*(\d)/, '-$1')
	  },
	  // e.g. 'GPL 2'
	  function (argument) {
	    return argument.replace(/,?\s*(\d)/, '-$1.0')
	  },
	  // e.g. 'Apache Version 2.0'
	  function (argument) {
	    return argument
	      .replace(/,?\s*(V\.|v\.|V|v|Version|version)\s*(\d)/, '-$2')
	  },
	  // e.g. 'Apache Version 2'
	  function (argument) {
	    return argument
	      .replace(/,?\s*(V\.|v\.|V|v|Version|version)\s*(\d)/, '-$2.0')
	  },
	  // e.g. 'ZLIB'
	  function (argument) {
	    return argument[0].toUpperCase() + argument.slice(1)
	  },
	  // e.g. 'MPL/2.0'
	  function (argument) {
	    return argument.replace('/', '-')
	  },
	  // e.g. 'Apache 2'
	  function (argument) {
	    return argument
	      .replace(/\s*V\s*(\d)/, '-$1')
	      .replace(/(\d)$/, '$1.0')
	  },
	  // e.g. 'GPL-2.0', 'GPL-3.0'
	  function (argument) {
	    if (argument.indexOf('3.0') !== -1) {
	      return argument + '-or-later'
	    } else {
	      return argument + '-only'
	    }
	  },
	  // e.g. 'GPL-2.0-'
	  function (argument) {
	    return argument + 'only'
	  },
	  // e.g. 'GPL2'
	  function (argument) {
	    return argument.replace(/(\d)$/, '-$1.0')
	  },
	  // e.g. 'BSD 3'
	  function (argument) {
	    return argument.replace(/(-| )?(\d)$/, '-$2-Clause')
	  },
	  // e.g. 'BSD clause 3'
	  function (argument) {
	    return argument.replace(/(-| )clause(-| )(\d)/, '-$3-Clause')
	  },
	  // e.g. 'New BSD license'
	  function (argument) {
	    return argument.replace(/\b(Modified|New|Revised)(-| )?BSD((-| )License)?/i, 'BSD-3-Clause')
	  },
	  // e.g. 'Simplified BSD license'
	  function (argument) {
	    return argument.replace(/\bSimplified(-| )?BSD((-| )License)?/i, 'BSD-2-Clause')
	  },
	  // e.g. 'Free BSD license'
	  function (argument) {
	    return argument.replace(/\b(Free|Net)(-| )?BSD((-| )License)?/i, 'BSD-2-Clause-$1BSD')
	  },
	  // e.g. 'Clear BSD license'
	  function (argument) {
	    return argument.replace(/\bClear(-| )?BSD((-| )License)?/i, 'BSD-3-Clause-Clear')
	  },
	  // e.g. 'Old BSD License'
	  function (argument) {
	    return argument.replace(/\b(Old|Original)(-| )?BSD((-| )License)?/i, 'BSD-4-Clause')
	  },
	  // e.g. 'BY-NC-4.0'
	  function (argument) {
	    return 'CC-' + argument
	  },
	  // e.g. 'BY-NC'
	  function (argument) {
	    return 'CC-' + argument + '-4.0'
	  },
	  // e.g. 'Attribution-NonCommercial'
	  function (argument) {
	    return argument
	      .replace('Attribution', 'BY')
	      .replace('NonCommercial', 'NC')
	      .replace('NoDerivatives', 'ND')
	      .replace(/ (\d)/, '-$1')
	      .replace(/ ?International/, '')
	  },
	  // e.g. 'Attribution-NonCommercial'
	  function (argument) {
	    return 'CC-' +
	      argument
	        .replace('Attribution', 'BY')
	        .replace('NonCommercial', 'NC')
	        .replace('NoDerivatives', 'ND')
	        .replace(/ (\d)/, '-$1')
	        .replace(/ ?International/, '') +
	      '-4.0'
	  }
	];

	var licensesWithVersions = spdxLicenseIds
	  .map(function (id) {
	    var match = /^(.*)-\d+\.\d+$/.exec(id);
	    return match
	      ? [match[0], match[1]]
	      : [id, null]
	  })
	  .reduce(function (objectMap, item) {
	    var key = item[1];
	    objectMap[key] = objectMap[key] || [];
	    objectMap[key].push(item[0]);
	    return objectMap
	  }, {});

	var licensesWithOneVersion = Object.keys(licensesWithVersions)
	  .map(function makeEntries (key) {
	    return [key, licensesWithVersions[key]]
	  })
	  .filter(function identifySoleVersions (item) {
	    return (
	      // Licenses has just one valid version suffix.
	      item[1].length === 1 &&
	      item[0] !== null &&
	      // APL will be considered Apache, rather than APL-1.0
	      item[0] !== 'APL'
	    )
	  })
	  .map(function createLastResorts (item) {
	    return [item[0], item[1][0]]
	  });

	licensesWithVersions = undefined;

	// If all else fails, guess that strings containing certain substrings
	// meant to identify certain licenses.
	var lastResorts = [
	  ['UNLI', 'Unlicense'],
	  ['WTF', 'WTFPL'],
	  ['2 CLAUSE', 'BSD-2-Clause'],
	  ['2-CLAUSE', 'BSD-2-Clause'],
	  ['3 CLAUSE', 'BSD-3-Clause'],
	  ['3-CLAUSE', 'BSD-3-Clause'],
	  ['AFFERO', 'AGPL-3.0-or-later'],
	  ['AGPL', 'AGPL-3.0-or-later'],
	  ['APACHE', 'Apache-2.0'],
	  ['ARTISTIC', 'Artistic-2.0'],
	  ['Affero', 'AGPL-3.0-or-later'],
	  ['BEER', 'Beerware'],
	  ['BOOST', 'BSL-1.0'],
	  ['BSD', 'BSD-2-Clause'],
	  ['CDDL', 'CDDL-1.1'],
	  ['ECLIPSE', 'EPL-1.0'],
	  ['FUCK', 'WTFPL'],
	  ['GNU', 'GPL-3.0-or-later'],
	  ['LGPL', 'LGPL-3.0-or-later'],
	  ['GPLV1', 'GPL-1.0-only'],
	  ['GPL-1', 'GPL-1.0-only'],
	  ['GPLV2', 'GPL-2.0-only'],
	  ['GPL-2', 'GPL-2.0-only'],
	  ['GPL', 'GPL-3.0-or-later'],
	  ['MIT +NO-FALSE-ATTRIBS', 'MITNFA'],
	  ['MIT', 'MIT'],
	  ['MPL', 'MPL-2.0'],
	  ['X11', 'X11'],
	  ['ZLIB', 'Zlib']
	].concat(licensesWithOneVersion).sort(sortTranspositions);

	var SUBSTRING = 0;
	var IDENTIFIER = 1;

	var validTransformation = function (identifier) {
	  for (var i = 0; i < transforms.length; i++) {
	    var transformed = transforms[i](identifier).trim();
	    if (transformed !== identifier && valid(transformed)) {
	      return transformed
	    }
	  }
	  return null
	};

	var validLastResort = function (identifier) {
	  var upperCased = identifier.toUpperCase();
	  for (var i = 0; i < lastResorts.length; i++) {
	    var lastResort = lastResorts[i];
	    if (upperCased.indexOf(lastResort[SUBSTRING]) > -1) {
	      return lastResort[IDENTIFIER]
	    }
	  }
	  return null
	};

	var anyCorrection = function (identifier, check) {
	  for (var i = 0; i < transpositions.length; i++) {
	    var transposition = transpositions[i];
	    var transposed = transposition[TRANSPOSED];
	    if (identifier.indexOf(transposed) > -1) {
	      var corrected = identifier.replace(
	        transposed,
	        transposition[CORRECT]
	      );
	      var checked = check(corrected);
	      if (checked !== null) {
	        return checked
	      }
	    }
	  }
	  return null
	};

	spdxCorrect = function (identifier, options) {
	  options = options || {};
	  var upgrade = options.upgrade === undefined ? true : !!options.upgrade;
	  function postprocess (value) {
	    return upgrade ? upgradeGPLs(value) : value
	  }
	  var validArugment = (
	    typeof identifier === 'string' &&
	    identifier.trim().length !== 0
	  );
	  if (!validArugment) {
	    throw Error('Invalid argument. Expected non-empty string.')
	  }
	  identifier = identifier.trim();
	  if (valid(identifier)) {
	    return postprocess(identifier)
	  }
	  var noPlus = identifier.replace(/\+$/, '').trim();
	  if (valid(noPlus)) {
	    return postprocess(noPlus)
	  }
	  var transformed = validTransformation(identifier);
	  if (transformed !== null) {
	    return postprocess(transformed)
	  }
	  transformed = anyCorrection(identifier, function (argument) {
	    if (valid(argument)) {
	      return argument
	    }
	    return validTransformation(argument)
	  });
	  if (transformed !== null) {
	    return postprocess(transformed)
	  }
	  transformed = validLastResort(identifier);
	  if (transformed !== null) {
	    return postprocess(transformed)
	  }
	  transformed = anyCorrection(identifier, validLastResort);
	  if (transformed !== null) {
	    return postprocess(transformed)
	  }
	  return null
	};

	function upgradeGPLs (value) {
	  if ([
	    'GPL-1.0', 'LGPL-1.0', 'AGPL-1.0',
	    'GPL-2.0', 'LGPL-2.0', 'AGPL-2.0',
	    'LGPL-2.1'
	  ].indexOf(value) !== -1) {
	    return value + '-only'
	  } else if ([
	    'GPL-1.0+', 'GPL-2.0+', 'GPL-3.0+',
	    'LGPL-2.0+', 'LGPL-2.1+', 'LGPL-3.0+',
	    'AGPL-1.0+', 'AGPL-3.0+'
	  ].indexOf(value) !== -1) {
	    return value.replace(/\+$/, '-or-later')
	  } else if (['GPL-3.0', 'LGPL-3.0', 'AGPL-3.0'].indexOf(value) !== -1) {
	    return value + '-or-later'
	  } else {
	    return value
	  }
	}
	return spdxCorrect;
}

var validateNpmPackageLicense;
var hasRequiredValidateNpmPackageLicense;

function requireValidateNpmPackageLicense () {
	if (hasRequiredValidateNpmPackageLicense) return validateNpmPackageLicense;
	hasRequiredValidateNpmPackageLicense = 1;
	var parse = requireSpdxExpressionParse();
	var correct = requireSpdxCorrect();

	var genericWarning = (
	  'license should be ' +
	  'a valid SPDX license expression (without "LicenseRef"), ' +
	  '"UNLICENSED", or ' +
	  '"SEE LICENSE IN <filename>"'
	);

	var fileReferenceRE = /^SEE LICEN[CS]E IN (.+)$/;

	function startsWith(prefix, string) {
	  return string.slice(0, prefix.length) === prefix;
	}

	function usesLicenseRef(ast) {
	  if (ast.hasOwnProperty('license')) {
	    var license = ast.license;
	    return (
	      startsWith('LicenseRef', license) ||
	      startsWith('DocumentRef', license)
	    );
	  } else {
	    return (
	      usesLicenseRef(ast.left) ||
	      usesLicenseRef(ast.right)
	    );
	  }
	}

	validateNpmPackageLicense = function(argument) {
	  var ast;

	  try {
	    ast = parse(argument);
	  } catch (e) {
	    var match;
	    if (
	      argument === 'UNLICENSED' ||
	      argument === 'UNLICENCED'
	    ) {
	      return {
	        validForOldPackages: true,
	        validForNewPackages: true,
	        unlicensed: true
	      };
	    } else if (match = fileReferenceRE.exec(argument)) {
	      return {
	        validForOldPackages: true,
	        validForNewPackages: true,
	        inFile: match[1]
	      };
	    } else {
	      var result = {
	        validForOldPackages: false,
	        validForNewPackages: false,
	        warnings: [genericWarning]
	      };
	      if (argument.trim().length !== 0) {
	        var corrected = correct(argument);
	        if (corrected) {
	          result.warnings.push(
	            'license is similar to the valid expression "' + corrected + '"'
	          );
	        }
	      }
	      return result;
	    }
	  }

	  if (usesLicenseRef(ast)) {
	    return {
	      validForNewPackages: false,
	      validForOldPackages: false,
	      spdx: true,
	      warnings: [genericWarning]
	    };
	  } else {
	    return {
	      validForNewPackages: true,
	      validForOldPackages: true,
	      spdx: true
	    };
	  }
	};
	return validateNpmPackageLicense;
}

var normalizeData_1;
var hasRequiredNormalizeData;

function requireNormalizeData () {
	if (hasRequiredNormalizeData) return normalizeData_1;
	hasRequiredNormalizeData = 1;
	// Originally normalize-package-data

	const url = require$$0$2;
	const hostedGitInfo = requireLib$d();
	const validateLicense = requireValidateNpmPackageLicense();

	const typos = {
	  dependancies: 'dependencies',
	  dependecies: 'dependencies',
	  depdenencies: 'dependencies',
	  devEependencies: 'devDependencies',
	  depends: 'dependencies',
	  'dev-dependencies': 'devDependencies',
	  devDependences: 'devDependencies',
	  devDepenencies: 'devDependencies',
	  devdependencies: 'devDependencies',
	  repostitory: 'repository',
	  repo: 'repository',
	  prefereGlobal: 'preferGlobal',
	  hompage: 'homepage',
	  hampage: 'homepage',
	  autohr: 'author',
	  autor: 'author',
	  contributers: 'contributors',
	  publicationConfig: 'publishConfig',
	  script: 'scripts',
	};

	const isEmail = str => str.includes('@') && (str.indexOf('@') < str.lastIndexOf('.'));

	// Extracts description from contents of a readme file in markdown format
	function extractDescription (description) {
	  // the first block of text before the first heading that isn't the first line heading
	  const lines = description.trim().split('\n');
	  let start = 0;
	  // skip initial empty lines and lines that start with #
	  while (lines[start]?.trim().match(/^(#|$)/)) {
	    start++;
	  }
	  let end = start + 1;
	  // keep going till we get to the end or an empty line
	  while (end < lines.length && lines[end].trim()) {
	    end++;
	  }
	  return lines.slice(start, end).join(' ').trim()
	}

	function stringifyPerson (person) {
	  if (typeof person !== 'string') {
	    const name = person.name || '';
	    const u = person.url || person.web;
	    const wrappedUrl = u ? (' (' + u + ')') : '';
	    const e = person.email || person.mail;
	    const wrappedEmail = e ? (' <' + e + '>') : '';
	    person = name + wrappedEmail + wrappedUrl;
	  }
	  const matchedName = person.match(/^([^(<]+)/);
	  const matchedUrl = person.match(/\(([^()]+)\)/);
	  const matchedEmail = person.match(/<([^<>]+)>/);
	  const parsed = {};
	  if (matchedName?.[0].trim()) {
	    parsed.name = matchedName[0].trim();
	  }
	  if (matchedEmail) {
	    parsed.email = matchedEmail[1];
	  }
	  if (matchedUrl) {
	    parsed.url = matchedUrl[1];
	  }
	  return parsed
	}

	function normalizeData (data, changes) {
	  // fixDescriptionField
	  if (data.description && typeof data.description !== 'string') {
	    changes?.push(`'description' field should be a string`);
	    delete data.description;
	  }
	  if (data.readme && !data.description && data.readme !== 'ERROR: No README data found!') {
	    data.description = extractDescription(data.readme);
	  }
	  if (data.description === undefined) {
	    delete data.description;
	  }
	  if (!data.description) {
	    changes?.push('No description');
	  }

	  // fixModulesField
	  if (data.modules) {
	    changes?.push(`modules field is deprecated`);
	    delete data.modules;
	  }

	  // fixFilesField
	  const files = data.files;
	  if (files && !Array.isArray(files)) {
	    changes?.push(`Invalid 'files' member`);
	    delete data.files;
	  } else if (data.files) {
	    data.files = data.files.filter(function (file) {
	      if (!file || typeof file !== 'string') {
	        changes?.push(`Invalid filename in 'files' list: ${file}`);
	        return false
	      } else {
	        return true
	      }
	    });
	  }

	  // fixManField
	  if (data.man && typeof data.man === 'string') {
	    data.man = [data.man];
	  }

	  // fixBugsField
	  if (!data.bugs && data.repository?.url) {
	    const hosted = hostedGitInfo.fromUrl(data.repository.url);
	    if (hosted && hosted.bugs()) {
	      data.bugs = { url: hosted.bugs() };
	    }
	  } else if (data.bugs) {
	    if (typeof data.bugs === 'string') {
	      if (isEmail(data.bugs)) {
	        data.bugs = { email: data.bugs };
	        /* eslint-disable-next-line node/no-deprecated-api */
	      } else if (url.parse(data.bugs).protocol) {
	        data.bugs = { url: data.bugs };
	      } else {
	        changes?.push(`Bug string field must be url, email, or {email,url}`);
	      }
	    } else {
	      for (const k in data.bugs) {
	        if (['web', 'name'].includes(k)) {
	          changes?.push(`bugs['${k}'] should probably be bugs['url'].`);
	          data.bugs.url = data.bugs[k];
	          delete data.bugs[k];
	        }
	      }
	      const oldBugs = data.bugs;
	      data.bugs = {};
	      if (oldBugs.url) {
	        /* eslint-disable-next-line node/no-deprecated-api */
	        if (typeof (oldBugs.url) === 'string' && url.parse(oldBugs.url).protocol) {
	          data.bugs.url = oldBugs.url;
	        } else {
	          changes?.push('bugs.url field must be a string url. Deleted.');
	        }
	      }
	      if (oldBugs.email) {
	        if (typeof (oldBugs.email) === 'string' && isEmail(oldBugs.email)) {
	          data.bugs.email = oldBugs.email;
	        } else {
	          changes?.push('bugs.email field must be a string email. Deleted.');
	        }
	      }
	    }
	    if (!data.bugs.email && !data.bugs.url) {
	      delete data.bugs;
	      changes?.push('Normalized value of bugs field is an empty object. Deleted.');
	    }
	  }
	  // fixKeywordsField
	  if (typeof data.keywords === 'string') {
	    data.keywords = data.keywords.split(/,\s+/);
	  }
	  if (data.keywords && !Array.isArray(data.keywords)) {
	    delete data.keywords;
	    changes?.push(`keywords should be an array of strings`);
	  } else if (data.keywords) {
	    data.keywords = data.keywords.filter(function (kw) {
	      if (typeof kw !== 'string' || !kw) {
	        changes?.push(`keywords should be an array of strings`);
	        return false
	      } else {
	        return true
	      }
	    });
	  }
	  // fixBundleDependenciesField
	  const bdd = 'bundledDependencies';
	  const bd = 'bundleDependencies';
	  if (data[bdd] && !data[bd]) {
	    data[bd] = data[bdd];
	    delete data[bdd];
	  }
	  if (data[bd] && !Array.isArray(data[bd])) {
	    changes?.push(`Invalid 'bundleDependencies' list. Must be array of package names`);
	    delete data[bd];
	  } else if (data[bd]) {
	    data[bd] = data[bd].filter(function (filtered) {
	      if (!filtered || typeof filtered !== 'string') {
	        changes?.push(`Invalid bundleDependencies member: ${filtered}`);
	        return false
	      } else {
	        if (!data.dependencies) {
	          data.dependencies = {};
	        }
	        if (!Object.prototype.hasOwnProperty.call(data.dependencies, filtered)) {
	          changes?.push(`Non-dependency in bundleDependencies: ${filtered}`);
	          data.dependencies[filtered] = '*';
	        }
	        return true
	      }
	    });
	  }
	  // fixHomepageField
	  if (!data.homepage && data.repository && data.repository.url) {
	    const hosted = hostedGitInfo.fromUrl(data.repository.url);
	    if (hosted) {
	      data.homepage = hosted.docs();
	    }
	  }
	  if (data.homepage) {
	    if (typeof data.homepage !== 'string') {
	      changes?.push('homepage field must be a string url. Deleted.');
	      delete data.homepage;
	    } else {
	      /* eslint-disable-next-line node/no-deprecated-api */
	      if (!url.parse(data.homepage).protocol) {
	        data.homepage = 'http://' + data.homepage;
	      }
	    }
	  }
	  // fixReadmeField
	  if (!data.readme) {
	    changes?.push('No README data');
	    data.readme = 'ERROR: No README data found!';
	  }
	  // fixLicenseField
	  const license = data.license || data.licence;
	  if (!license) {
	    changes?.push('No license field.');
	  } else if (typeof (license) !== 'string' || license.length < 1 || license.trim() === '') {
	    changes?.push('license should be a valid SPDX license expression');
	  } else if (!validateLicense(license).validForNewPackages) {
	    changes?.push('license should be a valid SPDX license expression');
	  }
	  // fixPeople
	  if (data.author) {
	    data.author = stringifyPerson(data.author);
	  }
	  ['maintainers', 'contributors'].forEach(function (set) {
	    if (!Array.isArray(data[set])) {
	      return
	    }
	    data[set] = data[set].map(stringifyPerson);
	  });
	  // fixTypos
	  for (const d in typos) {
	    if (Object.prototype.hasOwnProperty.call(data, d)) {
	      changes?.push(`${d} should probably be ${typos[d]}.`);
	    }
	  }
	}

	normalizeData_1 = { normalizeData };
	return normalizeData_1;
}

var normalize_1;
var hasRequiredNormalize;

function requireNormalize () {
	if (hasRequiredNormalize) return normalize_1;
	hasRequiredNormalize = 1;
	const valid = requireValid();
	const clean = requireClean();
	const fs = require$$5;
	const path = require$$2$2;
	const { log } = requireLib$c();

	/**
	 * @type {import('hosted-git-info')}
	 */
	let _hostedGitInfo;
	function lazyHostedGitInfo () {
	  if (!_hostedGitInfo) {
	    _hostedGitInfo = requireLib$d();
	  }
	  return _hostedGitInfo
	}

	/**
	 * @type {import('glob').glob}
	 */
	let _glob;
	function lazyLoadGlob () {
	  if (!_glob) {
	    _glob = requireCommonjs$2().glob;
	  }
	  return _glob
	}

	// used to be npm-normalize-package-bin
	function normalizePackageBin (pkg, changes) {
	  if (pkg.bin) {
	    if (typeof pkg.bin === 'string' && pkg.name) {
	      changes?.push('"bin" was converted to an object');
	      pkg.bin = { [pkg.name]: pkg.bin };
	    } else if (Array.isArray(pkg.bin)) {
	      changes?.push('"bin" was converted to an object');
	      pkg.bin = pkg.bin.reduce((acc, k) => {
	        acc[path.basename(k)] = k;
	        return acc
	      }, {});
	    }
	    if (typeof pkg.bin === 'object') {
	      for (const binKey in pkg.bin) {
	        if (typeof pkg.bin[binKey] !== 'string') {
	          delete pkg.bin[binKey];
	          changes?.push(`removed invalid "bin[${binKey}]"`);
	          continue
	        }
	        const base = path.basename(secureAndUnixifyPath(binKey));
	        if (!base) {
	          delete pkg.bin[binKey];
	          changes?.push(`removed invalid "bin[${binKey}]"`);
	          continue
	        }

	        const binTarget = secureAndUnixifyPath(pkg.bin[binKey]);

	        if (!binTarget) {
	          delete pkg.bin[binKey];
	          changes?.push(`removed invalid "bin[${binKey}]"`);
	          continue
	        }

	        if (base !== binKey) {
	          delete pkg.bin[binKey];
	          changes?.push(`"bin[${binKey}]" was renamed to "bin[${base}]"`);
	        }
	        if (binTarget !== pkg.bin[binKey]) {
	          changes?.push(`"bin[${base}]" script name was cleaned`);
	        }
	        pkg.bin[base] = binTarget;
	      }

	      if (Object.keys(pkg.bin).length === 0) {
	        changes?.push('empty "bin" was removed');
	        delete pkg.bin;
	      }

	      return pkg
	    }
	  }
	  delete pkg.bin;
	}

	function normalizePackageMan (pkg, changes) {
	  if (pkg.man) {
	    const mans = [];
	    for (const man of (Array.isArray(pkg.man) ? pkg.man : [pkg.man])) {
	      if (typeof man !== 'string') {
	        changes?.push(`removed invalid "man [${man}]"`);
	      } else {
	        mans.push(secureAndUnixifyPath(man));
	      }
	    }

	    if (!mans.length) {
	      changes?.push('empty "man" was removed');
	    } else {
	      pkg.man = mans;
	      return pkg
	    }
	  }
	  delete pkg.man;
	}

	function isCorrectlyEncodedName (spec) {
	  return !spec.match(/[/@\s+%:]/) &&
	    spec === encodeURIComponent(spec)
	}

	function isValidScopedPackageName (spec) {
	  if (spec.charAt(0) !== '@') {
	    return false
	  }

	  const rest = spec.slice(1).split('/');
	  if (rest.length !== 2) {
	    return false
	  }

	  return rest[0] && rest[1] &&
	    rest[0] === encodeURIComponent(rest[0]) &&
	    rest[1] === encodeURIComponent(rest[1])
	}

	function unixifyPath (ref) {
	  return ref.replace(/\\|:/g, '/')
	}

	function secureAndUnixifyPath (ref) {
	  const secured = unixifyPath(path.join('.', path.join('/', unixifyPath(ref))));
	  return secured.startsWith('./') ? '' : secured
	}

	// We don't want the `changes` array in here by default because this is a hot
	// path for parsing packuments during install.  So the calling method passes it
	// in if it wants to track changes.
	const normalize = async (pkg, { strict, steps, root, changes, allowLegacyCase }) => {
	  if (!pkg.content) {
	    throw new Error('Can not normalize without content')
	  }
	  const data = pkg.content;
	  const scripts = data.scripts || {};
	  const pkgId = `${data.name ?? ''}@${data.version ?? ''}`;

	  // name and version are load bearing so we have to clean them up first
	  if (steps.includes('fixNameField') || steps.includes('normalizeData')) {
	    if (!data.name && !strict) {
	      changes?.push('Missing "name" field was set to an empty string');
	      data.name = '';
	    } else {
	      if (typeof data.name !== 'string') {
	        throw new Error('name field must be a string.')
	      }
	      if (!strict) {
	        const name = data.name.trim();
	        if (data.name !== name) {
	          changes?.push(`Whitespace was trimmed from "name"`);
	          data.name = name;
	        }
	      }

	      if (data.name.startsWith('.') ||
	        !(isValidScopedPackageName(data.name) || isCorrectlyEncodedName(data.name)) ||
	        (strict && (!allowLegacyCase) && data.name !== data.name.toLowerCase()) ||
	        data.name.toLowerCase() === 'node_modules' ||
	        data.name.toLowerCase() === 'favicon.ico') {
	        throw new Error('Invalid name: ' + JSON.stringify(data.name))
	      }
	    }
	  }

	  if (steps.includes('fixVersionField') || steps.includes('normalizeData')) {
	    // allow "loose" semver 1.0 versions in non-strict mode
	    // enforce strict semver 2.0 compliance in strict mode
	    const loose = !strict;
	    if (!data.version) {
	      data.version = '';
	    } else {
	      if (!valid(data.version, loose)) {
	        throw new Error(`Invalid version: "${data.version}"`)
	      }
	      const version = clean(data.version, loose);
	      if (version !== data.version) {
	        changes?.push(`"version" was cleaned and set to "${version}"`);
	        data.version = version;
	      }
	    }
	  }
	  // remove attributes that start with "_"
	  if (steps.includes('_attributes')) {
	    for (const key in data) {
	      if (key.startsWith('_')) {
	        changes?.push(`"${key}" was removed`);
	        delete pkg.content[key];
	      }
	    }
	  }

	  // build the "_id" attribute
	  if (steps.includes('_id')) {
	    if (data.name && data.version) {
	      changes?.push(`"_id" was set to ${pkgId}`);
	      data._id = pkgId;
	    }
	  }

	  // fix bundledDependencies typo
	  // normalize bundleDependencies
	  if (steps.includes('bundledDependencies')) {
	    if (data.bundleDependencies === undefined && data.bundledDependencies !== undefined) {
	      data.bundleDependencies = data.bundledDependencies;
	    }
	    changes?.push(`Deleted incorrect "bundledDependencies"`);
	    delete data.bundledDependencies;
	  }
	  // expand "bundleDependencies: true or translate from object"
	  if (steps.includes('bundleDependencies')) {
	    const bd = data.bundleDependencies;
	    if (bd === false && !steps.includes('bundleDependenciesDeleteFalse')) {
	      changes?.push(`"bundleDependencies" was changed from "false" to "[]"`);
	      data.bundleDependencies = [];
	    } else if (bd === true) {
	      changes?.push(`"bundleDependencies" was auto-populated from "dependencies"`);
	      data.bundleDependencies = Object.keys(data.dependencies || {});
	    } else if (bd && typeof bd === 'object') {
	      if (!Array.isArray(bd)) {
	        changes?.push(`"bundleDependencies" was changed from an object to an array`);
	        data.bundleDependencies = Object.keys(bd);
	      }
	    } else if ('bundleDependencies' in data) {
	      changes?.push(`"bundleDependencies" was removed`);
	      delete data.bundleDependencies;
	    }
	  }

	  // it was once common practice to list deps both in optionalDependencies and
	  // in dependencies, to support npm versions that did not know about
	  // optionalDependencies.  This is no longer a relevant need, so duplicating
	  // the deps in two places is unnecessary and excessive.
	  if (steps.includes('optionalDedupe')) {
	    if (data.dependencies &&
	      data.optionalDependencies && typeof data.optionalDependencies === 'object') {
	      for (const name in data.optionalDependencies) {
	        changes?.push(`optionalDependencies."${name}" was removed`);
	        delete data.dependencies[name];
	      }
	      if (!Object.keys(data.dependencies).length) {
	        changes?.push(`Empty "optionalDependencies" was removed`);
	        delete data.dependencies;
	      }
	    }
	  }

	  // add "install" attribute if any "*.gyp" files exist
	  if (steps.includes('gypfile')) {
	    if (!scripts.install && !scripts.preinstall && data.gypfile !== false) {
	      const files = await lazyLoadGlob()('*.gyp', { cwd: pkg.path });
	      if (files.length) {
	        scripts.install = 'node-gyp rebuild';
	        data.scripts = scripts;
	        data.gypfile = true;
	        changes?.push(`"scripts.install" was set to "node-gyp rebuild"`);
	        changes?.push(`"gypfile" was set to "true"`);
	      }
	    }
	  }

	  // add "start" attribute if "server.js" exists
	  if (steps.includes('serverjs') && !scripts.start) {
	    try {
	      await fs.access(path.join(pkg.path, 'server.js'));
	      scripts.start = 'node server.js';
	      data.scripts = scripts;
	      changes?.push('"scripts.start" was set to "node server.js"');
	    } catch {
	      // do nothing
	    }
	  }

	  // strip "node_modules/.bin" from scripts entries
	  // remove invalid scripts entries (non-strings)
	  if ((steps.includes('scripts') || steps.includes('scriptpath')) && data.scripts !== undefined) {
	    const spre = /^(\.[/\\])?node_modules[/\\].bin[\\/]/;
	    if (typeof data.scripts === 'object') {
	      for (const name in data.scripts) {
	        if (typeof data.scripts[name] !== 'string') {
	          delete data.scripts[name];
	          changes?.push(`Invalid scripts."${name}" was removed`);
	        } else if (steps.includes('scriptpath') && spre.test(data.scripts[name])) {
	          data.scripts[name] = data.scripts[name].replace(spre, '');
	          changes?.push(`scripts entry "${name}" was fixed to remove node_modules/.bin reference`);
	        }
	      }
	    } else {
	      changes?.push(`Removed invalid "scripts"`);
	      delete data.scripts;
	    }
	  }

	  if (steps.includes('funding')) {
	    if (data.funding && typeof data.funding === 'string') {
	      data.funding = { url: data.funding };
	      changes?.push(`"funding" was changed to an object with a url attribute`);
	    }
	  }

	  // populate "authors" attribute
	  if (steps.includes('authors') && !data.contributors) {
	    try {
	      const authorData = await fs.readFile(path.join(pkg.path, 'AUTHORS'), 'utf8');
	      const authors = authorData.split(/\r?\n/g)
	        .map(line => line.replace(/^\s*#.*$/, '').trim())
	        .filter(line => line);
	      data.contributors = authors;
	      changes?.push('"contributors" was auto-populated with the contents of the "AUTHORS" file');
	    } catch {
	      // do nothing
	    }
	  }

	  // populate "readme" attribute
	  if (steps.includes('readme') && !data.readme) {
	    const mdre = /\.m?a?r?k?d?o?w?n?$/i;
	    const files = await lazyLoadGlob()('{README,README.*}', {
	      cwd: pkg.path,
	      nocase: true,
	      mark: true,
	    });
	    let readmeFile;
	    for (const file of files) {
	      // don't accept directories.
	      if (!file.endsWith(path.sep)) {
	        if (file.match(mdre)) {
	          readmeFile = file;
	          break
	        }
	        if (file.endsWith('README')) {
	          readmeFile = file;
	        }
	      }
	    }
	    if (readmeFile) {
	      const readmeData = await fs.readFile(path.join(pkg.path, readmeFile), 'utf8');
	      data.readme = readmeData;
	      data.readmeFilename = readmeFile;
	      changes?.push(`"readme" was set to the contents of ${readmeFile}`);
	      changes?.push(`"readmeFilename" was set to ${readmeFile}`);
	    }
	    if (!data.readme) {
	      data.readme = 'ERROR: No README data found!';
	    }
	  }

	  // expand directories.man
	  if (steps.includes('mans')) {
	    if (data.directories?.man && !data.man) {
	      const manDir = secureAndUnixifyPath(data.directories.man);
	      const cwd = path.resolve(pkg.path, manDir);
	      const files = await lazyLoadGlob()('**/*.[0-9]', { cwd });
	      data.man = files.map(man =>
	        path.relative(pkg.path, path.join(cwd, man)).split(path.sep).join('/')
	      );
	    }
	    normalizePackageMan(data, changes);
	  }

	  if (steps.includes('bin') || steps.includes('binDir') || steps.includes('binRefs')) {
	    normalizePackageBin(data, changes);
	  }

	  // expand "directories.bin"
	  if (steps.includes('binDir') && data.directories?.bin && !data.bin) {
	    const binsDir = path.resolve(pkg.path, secureAndUnixifyPath(data.directories.bin));
	    const bins = await lazyLoadGlob()('**', { cwd: binsDir });
	    data.bin = bins.reduce((acc, binFile) => {
	      if (binFile && !binFile.startsWith('.')) {
	        const binName = path.basename(binFile);
	        acc[binName] = path.join(data.directories.bin, binFile);
	      }
	      return acc
	    }, {});
	    // *sigh*
	    normalizePackageBin(data, changes);
	  }

	  // populate "gitHead" attribute
	  if (steps.includes('gitHead') && !data.gitHead) {
	    const git = requireLib$4();
	    const gitRoot = await git.find({ cwd: pkg.path, root });
	    let head;
	    if (gitRoot) {
	      try {
	        head = await fs.readFile(path.resolve(gitRoot, '.git/HEAD'), 'utf8');
	      } catch (err) {
	      // do nothing
	      }
	    }
	    let headData;
	    if (head) {
	      if (head.startsWith('ref: ')) {
	        const headRef = head.replace(/^ref: /, '').trim();
	        const headFile = path.resolve(gitRoot, '.git', headRef);
	        try {
	          headData = await fs.readFile(headFile, 'utf8');
	          headData = headData.replace(/^ref: /, '').trim();
	        } catch (err) {
	          // do nothing
	        }
	        if (!headData) {
	          const packFile = path.resolve(gitRoot, '.git/packed-refs');
	          try {
	            let refs = await fs.readFile(packFile, 'utf8');
	            if (refs) {
	              refs = refs.split('\n');
	              for (let i = 0; i < refs.length; i++) {
	                const match = refs[i].match(/^([0-9a-f]{40}) (.+)$/);
	                if (match && match[2].trim() === headRef) {
	                  headData = match[1];
	                  break
	                }
	              }
	            }
	          } catch {
	            // do nothing
	          }
	        }
	      } else {
	        headData = head.trim();
	      }
	    }
	    if (headData) {
	      data.gitHead = headData;
	    }
	  }

	  // populate "types" attribute
	  if (steps.includes('fillTypes')) {
	    const index = data.main || 'index.js';

	    if (typeof index !== 'string') {
	      throw new TypeError('The "main" attribute must be of type string.')
	    }

	    // TODO exports is much more complicated than this in verbose format
	    // We need to support for instance

	    // "exports": {
	    //   ".": [
	    //     {
	    //       "default": "./lib/npm.js"
	    //     },
	    //     "./lib/npm.js"
	    //   ],
	    //   "./package.json": "./package.json"
	    // },
	    // as well as conditional exports

	    // if (data.exports && typeof data.exports === 'string') {
	    //   index = data.exports
	    // }

	    // if (data.exports && data.exports['.']) {
	    //   index = data.exports['.']
	    //   if (typeof index !== 'string') {
	    //   }
	    // }
	    const extless = path.join(path.dirname(index), path.basename(index, path.extname(index)));
	    const dts = `./${extless}.d.ts`;
	    const hasDTSFields = 'types' in data || 'typings' in data;
	    if (!hasDTSFields) {
	      try {
	        await fs.access(path.join(pkg.path, dts));
	        data.types = dts.split(path.sep).join('/');
	      } catch {
	        // do nothing
	      }
	    }
	  }

	  // "normalizeData" from "read-package-json", which was just a call through to
	  // "normalize-package-data".  We only call the "fixer" functions because
	  // outside of that it was also clobbering _id (which we already conditionally
	  // do) and also adding the gypfile script (which we also already
	  // conditionally do)

	  // Some steps are isolated so we can do a limited subset of these in `fix`
	  if (steps.includes('fixRepositoryField') || steps.includes('normalizeData')) {
	    if (data.repositories) {
	      changes?.push(`"repository" was set to the first entry in "repositories" (${data.repository})`);
	      data.repository = data.repositories[0];
	    }
	    if (data.repository) {
	      if (typeof data.repository === 'string') {
	        changes?.push('"repository" was changed from a string to an object');
	        data.repository = {
	          type: 'git',
	          url: data.repository,
	        };
	      }
	      if (data.repository.url) {
	        const hosted = lazyHostedGitInfo().fromUrl(data.repository.url);
	        let r;
	        if (hosted) {
	          if (hosted.getDefaultRepresentation() === 'shortcut') {
	            r = hosted.https();
	          } else {
	            r = hosted.toString();
	          }
	          if (r !== data.repository.url) {
	            changes?.push(`"repository.url" was normalized to "${r}"`);
	            data.repository.url = r;
	          }
	        }
	      }
	    }
	  }

	  if (steps.includes('fixDependencies') || steps.includes('normalizeData')) {
	    // peerDependencies?
	    // devDependencies is meaningless here, it's ignored on an installed package
	    for (const type of ['dependencies', 'devDependencies', 'optionalDependencies']) {
	      if (data[type]) {
	        let secondWarning = true;
	        if (typeof data[type] === 'string') {
	          changes?.push(`"${type}" was converted from a string into an object`);
	          data[type] = data[type].trim().split(/[\n\r\s\t ,]+/);
	          secondWarning = false;
	        }
	        if (Array.isArray(data[type])) {
	          if (secondWarning) {
	            changes?.push(`"${type}" was converted from an array into an object`);
	          }
	          const o = {};
	          for (const d of data[type]) {
	            if (typeof d === 'string') {
	              const dep = d.trim().split(/(:?[@\s><=])/);
	              const dn = dep.shift();
	              const dv = dep.join('').replace(/^@/, '').trim();
	              o[dn] = dv;
	            }
	          }
	          data[type] = o;
	        }
	      }
	    }
	    // normalize-package-data used to put optional dependencies BACK into
	    // dependencies here, we no longer do this

	    for (const deps of ['dependencies', 'devDependencies']) {
	      if (deps in data) {
	        if (!data[deps] || typeof data[deps] !== 'object') {
	          changes?.push(`Removed invalid "${deps}"`);
	          delete data[deps];
	        } else {
	          for (const d in data[deps]) {
	            const r = data[deps][d];
	            if (typeof r !== 'string') {
	              changes?.push(`Removed invalid "${deps}.${d}"`);
	              delete data[deps][d];
	            }
	            const hosted = lazyHostedGitInfo().fromUrl(data[deps][d])?.toString();
	            if (hosted && hosted !== data[deps][d]) {
	              changes?.push(`Normalized git reference to "${deps}.${d}"`);
	              data[deps][d] = hosted.toString();
	            }
	          }
	        }
	      }
	    }
	  }

	  // TODO some of this is duplicated in other steps here, a future breaking change may be able to remove the duplicates involved in this step
	  if (steps.includes('normalizeData')) {
	    const { normalizeData } = requireNormalizeData();
	    normalizeData(data, changes);
	  }

	  // Warn if the bin references don't point to anything.  This might be better
	  // in normalize-package-data if it had access to the file path.
	  if (steps.includes('binRefs') && data.bin instanceof Object) {
	    for (const key in data.bin) {
	      try {
	        await fs.access(path.resolve(pkg.path, data.bin[key]));
	      } catch {
	        log.warn('package-json', pkgId, `No bin file found at ${data.bin[key]}`);
	        // XXX: should a future breaking change delete bin entries that cannot be accessed?
	      }
	    }
	  }
	};

	normalize_1 = normalize;
	return normalize_1;
}

var readPackage_1;
var hasRequiredReadPackage;

function requireReadPackage () {
	if (hasRequiredReadPackage) return readPackage_1;
	hasRequiredReadPackage = 1;
	// This is JUST the code needed to open a package.json file and parse it.
	// It's isolated out so that code needing to parse a package.json file can do so in the same way as this module does, without needing to require the whole module, or needing to require the underlying parsing library.

	const { readFile } = require$$0$3;
	const parseJSON = requireLib$a();

	async function read (filename) {
	  try {
	    const data = await readFile(filename, 'utf8');
	    return data
	  } catch (err) {
	    err.message = `Could not read package.json: ${err}`;
	    throw err
	  }
	}

	function parse (data) {
	  try {
	    const content = parseJSON(data);
	    return content
	  } catch (err) {
	    err.message = `Invalid package.json: ${err}`;
	    throw err
	  }
	}

	// This is what most external libs will use.
	// PackageJson will call read and parse separately
	async function readPackage (filename) {
	  const data = await read(filename);
	  const content = parse(data);
	  return content
	}

	readPackage_1 = {
	  read,
	  parse,
	  readPackage,
	};
	return readPackage_1;
}

/**
 * arbitrary sort order for package.json largely pulled from:
 * https://github.com/keithamus/sort-package-json/blob/main/defaultRules.md
 *
 * cross checked with:
 * https://github.com/npm/types/blob/main/types/index.d.ts#L104
 * https://docs.npmjs.com/cli/configuring-npm/package-json
 */

var sort;
var hasRequiredSort;

function requireSort () {
	if (hasRequiredSort) return sort;
	hasRequiredSort = 1;
	function packageSort (json) {
	  const {
	    name,
	    version,
	    private: isPrivate,
	    description,
	    keywords,
	    homepage,
	    bugs,
	    repository,
	    funding,
	    license,
	    author,
	    maintainers,
	    contributors,
	    type,
	    imports,
	    exports,
	    main,
	    browser,
	    types,
	    bin,
	    man,
	    directories,
	    files,
	    workspaces,
	    scripts,
	    config,
	    dependencies,
	    devDependencies,
	    peerDependencies,
	    peerDependenciesMeta,
	    optionalDependencies,
	    bundledDependencies,
	    bundleDependencies,
	    engines,
	    os,
	    cpu,
	    publishConfig,
	    devEngines,
	    licenses,
	    overrides,
	    ...rest
	  } = json;

	  return {
	    ...(typeof name !== 'undefined' ? { name } : {}),
	    ...(typeof version !== 'undefined' ? { version } : {}),
	    ...(typeof isPrivate !== 'undefined' ? { private: isPrivate } : {}),
	    ...(typeof description !== 'undefined' ? { description } : {}),
	    ...(typeof keywords !== 'undefined' ? { keywords } : {}),
	    ...(typeof homepage !== 'undefined' ? { homepage } : {}),
	    ...(typeof bugs !== 'undefined' ? { bugs } : {}),
	    ...(typeof repository !== 'undefined' ? { repository } : {}),
	    ...(typeof funding !== 'undefined' ? { funding } : {}),
	    ...(typeof license !== 'undefined' ? { license } : {}),
	    ...(typeof author !== 'undefined' ? { author } : {}),
	    ...(typeof maintainers !== 'undefined' ? { maintainers } : {}),
	    ...(typeof contributors !== 'undefined' ? { contributors } : {}),
	    ...(typeof type !== 'undefined' ? { type } : {}),
	    ...(typeof imports !== 'undefined' ? { imports } : {}),
	    ...(typeof exports !== 'undefined' ? { exports } : {}),
	    ...(typeof main !== 'undefined' ? { main } : {}),
	    ...(typeof browser !== 'undefined' ? { browser } : {}),
	    ...(typeof types !== 'undefined' ? { types } : {}),
	    ...(typeof bin !== 'undefined' ? { bin } : {}),
	    ...(typeof man !== 'undefined' ? { man } : {}),
	    ...(typeof directories !== 'undefined' ? { directories } : {}),
	    ...(typeof files !== 'undefined' ? { files } : {}),
	    ...(typeof workspaces !== 'undefined' ? { workspaces } : {}),
	    ...(typeof scripts !== 'undefined' ? { scripts } : {}),
	    ...(typeof config !== 'undefined' ? { config } : {}),
	    ...(typeof dependencies !== 'undefined' ? { dependencies } : {}),
	    ...(typeof devDependencies !== 'undefined' ? { devDependencies } : {}),
	    ...(typeof peerDependencies !== 'undefined' ? { peerDependencies } : {}),
	    ...(typeof peerDependenciesMeta !== 'undefined' ? { peerDependenciesMeta } : {}),
	    ...(typeof optionalDependencies !== 'undefined' ? { optionalDependencies } : {}),
	    ...(typeof bundledDependencies !== 'undefined' ? { bundledDependencies } : {}),
	    ...(typeof bundleDependencies !== 'undefined' ? { bundleDependencies } : {}),
	    ...(typeof engines !== 'undefined' ? { engines } : {}),
	    ...(typeof os !== 'undefined' ? { os } : {}),
	    ...(typeof cpu !== 'undefined' ? { cpu } : {}),
	    ...(typeof publishConfig !== 'undefined' ? { publishConfig } : {}),
	    ...(typeof devEngines !== 'undefined' ? { devEngines } : {}),
	    ...(typeof licenses !== 'undefined' ? { licenses } : {}),
	    ...(typeof overrides !== 'undefined' ? { overrides } : {}),
	    ...rest,
	  }
	}

	sort = {
	  packageSort,
	};
	return sort;
}

var lib$3;
var hasRequiredLib$3;

function requireLib$3 () {
	if (hasRequiredLib$3) return lib$3;
	hasRequiredLib$3 = 1;
	const { readFile, writeFile } = require$$5;
	const { resolve } = require$$2$2;
	const parseJSON = requireLib$a();

	const updateDeps = requireUpdateDependencies();
	const updateScripts = requireUpdateScripts();
	const updateWorkspaces = requireUpdateWorkspaces();
	const normalize = requireNormalize();
	const { read, parse } = requireReadPackage();
	const { packageSort } = requireSort();

	// a list of handy specialized helper functions that take
	// care of special cases that are handled by the npm cli
	const knownSteps = new Set([
	  updateDeps,
	  updateScripts,
	  updateWorkspaces,
	]);

	// list of all keys that are handled by "knownSteps" helpers
	const knownKeys = new Set([
	  ...updateDeps.knownKeys,
	  'scripts',
	  'workspaces',
	]);

	class PackageJson {
	  static normalizeSteps = Object.freeze([
	    '_id',
	    '_attributes',
	    'bundledDependencies',
	    'bundleDependencies',
	    'optionalDedupe',
	    'scripts',
	    'funding',
	    'bin',
	  ])

	  // npm pkg fix
	  static fixSteps = Object.freeze([
	    'binRefs',
	    'bundleDependencies',
	    'bundleDependenciesFalse',
	    'fixNameField',
	    'fixVersionField',
	    'fixRepositoryField',
	    'fixDependencies',
	    'devDependencies',
	    'scriptpath',
	  ])

	  static prepareSteps = Object.freeze([
	    '_id',
	    '_attributes',
	    'bundledDependencies',
	    'bundleDependencies',
	    'bundleDependenciesDeleteFalse',
	    'gypfile',
	    'serverjs',
	    'scriptpath',
	    'authors',
	    'readme',
	    'mans',
	    'binDir',
	    'gitHead',
	    'fillTypes',
	    'normalizeData',
	    'binRefs',
	  ])

	  // create a new empty package.json, so we can save at the given path even
	  // though we didn't start from a parsed file
	  static async create (path, opts = {}) {
	    const p = new PackageJson();
	    await p.create(path);
	    if (opts.data) {
	      return p.update(opts.data)
	    }
	    return p
	  }

	  // Loads a package.json at given path and JSON parses
	  static async load (path, opts = {}) {
	    const p = new PackageJson();
	    // Avoid try/catch if we aren't going to create
	    if (!opts.create) {
	      return p.load(path)
	    }

	    try {
	      return await p.load(path)
	    } catch (err) {
	      if (!err.message.startsWith('Could not read package.json')) {
	        throw err
	      }
	      return await p.create(path)
	    }
	  }

	  // npm pkg fix
	  static async fix (path, opts) {
	    const p = new PackageJson();
	    await p.load(path, true);
	    return p.fix(opts)
	  }

	  // read-package-json compatible behavior
	  static async prepare (path, opts) {
	    const p = new PackageJson();
	    await p.load(path, true);
	    return p.prepare(opts)
	  }

	  // read-package-json-fast compatible behavior
	  static async normalize (path, opts) {
	    const p = new PackageJson();
	    await p.load(path);
	    return p.normalize(opts)
	  }

	  #path
	  #manifest
	  #readFileContent = ''
	  #canSave = true

	  // Load content from given path
	  async load (path, parseIndex) {
	    this.#path = path;
	    let parseErr;
	    try {
	      this.#readFileContent = await read(this.filename);
	    } catch (err) {
	      if (!parseIndex) {
	        throw err
	      }
	      parseErr = err;
	    }

	    if (parseErr) {
	      const indexFile = resolve(this.path, 'index.js');
	      let indexFileContent;
	      try {
	        indexFileContent = await readFile(indexFile, 'utf8');
	      } catch (err) {
	        throw parseErr
	      }
	      try {
	        this.fromComment(indexFileContent);
	      } catch (err) {
	        throw parseErr
	      }
	      // This wasn't a package.json so prevent saving
	      this.#canSave = false;
	      return this
	    }

	    return this.fromJSON(this.#readFileContent)
	  }

	  // Load data from a JSON string/buffer
	  fromJSON (data) {
	    this.#manifest = parse(data);
	    return this
	  }

	  fromContent (data) {
	    this.#manifest = data;
	    this.#canSave = false;
	    return this
	  }

	  // Load data from a comment
	  // /**package { "name": "foo", "version": "1.2.3", ... } **/
	  fromComment (data) {
	    data = data.split(/^\/\*\*package(?:\s|$)/m);

	    if (data.length < 2) {
	      throw new Error('File has no package in comments')
	    }
	    data = data[1];
	    data = data.split(/\*\*\/$/m);

	    if (data.length < 2) {
	      throw new Error('File has no package in comments')
	    }
	    data = data[0];
	    data = data.replace(/^\s*\*/mg, '');

	    this.#manifest = parseJSON(data);
	    return this
	  }

	  get content () {
	    return this.#manifest
	  }

	  get path () {
	    return this.#path
	  }

	  get filename () {
	    if (this.path) {
	      return resolve(this.path, 'package.json')
	    }
	    return undefined
	  }

	  create (path) {
	    this.#path = path;
	    this.#manifest = {};
	    return this
	  }

	  // This should be the ONLY way to set content in the manifest
	  update (content) {
	    if (!this.content) {
	      throw new Error('Can not update without content.  Please `load` or `create`')
	    }

	    for (const step of knownSteps) {
	      this.#manifest = step({ content, originalContent: this.content });
	    }

	    // unknown properties will just be overwitten
	    for (const [key, value] of Object.entries(content)) {
	      if (!knownKeys.has(key)) {
	        this.content[key] = value;
	      }
	    }

	    return this
	  }

	  async save ({ sort } = {}) {
	    if (!this.#canSave) {
	      throw new Error('No package.json to save to')
	    }
	    const {
	      [Symbol.for('indent')]: indent,
	      [Symbol.for('newline')]: newline,
	      ...rest
	    } = this.content;

	    const format = indent === undefined ? '  ' : indent;
	    const eol = newline === undefined ? '\n' : newline;

	    const content = sort ? packageSort(rest) : rest;

	    const fileContent = `${
	      JSON.stringify(content, null, format)
	    }\n`
	      .replace(/\n/g, eol);

	    if (fileContent.trim() !== this.#readFileContent.trim()) {
	      const written = await writeFile(this.filename, fileContent);
	      this.#readFileContent = fileContent;
	      return written
	    }
	  }

	  async normalize (opts = {}) {
	    if (!opts.steps) {
	      opts.steps = this.constructor.normalizeSteps;
	    }
	    await normalize(this, opts);
	    return this
	  }

	  async prepare (opts = {}) {
	    if (!opts.steps) {
	      opts.steps = this.constructor.prepareSteps;
	    }
	    await normalize(this, opts);
	    return this
	  }

	  async fix (opts = {}) {
	    // This one is not overridable
	    opts.steps = this.constructor.fixSteps;
	    await normalize(this, opts);
	    return this
	  }
	}

	lib$3 = PackageJson;
	return lib$3;
}

var lib$2;
var hasRequiredLib$2;

function requireLib$2 () {
	if (hasRequiredLib$2) return lib$2;
	hasRequiredLib$2 = 1;
	const { basename, dirname } = require$$0;

	const getName = (parent, base) =>
	  parent.charAt(0) === '@' ? `${parent}/${base}` : base;

	lib$2 = dir => dir ? getName(basename(dirname(dir)), basename(dir))
	  : false;
	return lib$2;
}

var lib$1;
var hasRequiredLib$1;

function requireLib$1 () {
	if (hasRequiredLib$1) return lib$1;
	hasRequiredLib$1 = 1;
	const path = require$$0;

	const getName = requireLib$2();
	const { minimatch } = requireCommonjs$3();
	const pkgJson = requireLib$3();
	const { glob } = requireCommonjs$2();

	function appendNegatedPatterns (allPatterns) {
	  const patterns = [];
	  const negatedPatterns = [];
	  for (let pattern of allPatterns) {
	    const excl = pattern.match(/^!+/);
	    if (excl) {
	      pattern = pattern.slice(excl[0].length);
	    }

	    // strip off any / or ./ from the start of the pattern.  /foo => foo
	    pattern = pattern.replace(/^\.?\/+/, '');

	    // an odd number of ! means a negated pattern.  !!foo ==> foo
	    const negate = excl && excl[0].length % 2 === 1;
	    if (negate) {
	      negatedPatterns.push(pattern);
	    } else {
	      // remove negated patterns that appeared before this pattern to avoid
	      // ignoring paths that were matched afterwards
	      // e.g: ['packages/**', '!packages/b/**', 'packages/b/a']
	      // in the above list, the last pattern overrides the negated pattern
	      // right before it. In effect, the above list would become:
	      // ['packages/**', 'packages/b/a']
	      // The order matters here which is why we must do it inside the loop
	      // as opposed to doing it all together at the end.
	      for (let i = 0; i < negatedPatterns.length; ++i) {
	        const negatedPattern = negatedPatterns[i];
	        if (minimatch(pattern, negatedPattern)) {
	          negatedPatterns.splice(i, 1);
	        }
	      }
	      patterns.push(pattern);
	    }
	  }

	  // use the negated patterns to eagerly remove all the patterns that
	  // can be removed to avoid unnecessary crawling
	  for (const negated of negatedPatterns) {
	    for (const pattern of minimatch.match(patterns, negated)) {
	      patterns.splice(patterns.indexOf(pattern), 1);
	    }
	  }
	  return { patterns, negatedPatterns }
	}

	function getPatterns (workspaces) {
	  const workspacesDeclaration =
	    Array.isArray(workspaces.packages)
	      ? workspaces.packages
	      : workspaces;

	  if (!Array.isArray(workspacesDeclaration)) {
	    throw getError({
	      message: 'workspaces config expects an Array',
	      code: 'EWORKSPACESCONFIG',
	    })
	  }

	  return appendNegatedPatterns(workspacesDeclaration)
	}

	function getPackageName (pkg, pathname) {
	  return pkg.name || getName(pathname)
	}

	// make sure glob pattern only matches folders
	function getGlobPattern (pattern) {
	  pattern = pattern.replace(/\\/g, '/');
	  return pattern.endsWith('/')
	    ? pattern
	    : `${pattern}/`
	}

	function getError ({ Type = TypeError, message, code }) {
	  return Object.assign(new Type(message), { code })
	}

	function reverseResultMap (map) {
	  return new Map(Array.from(map, item => item.reverse()))
	}

	async function mapWorkspaces (opts = {}) {
	  if (!opts || !opts.pkg) {
	    throw getError({
	      message: 'mapWorkspaces missing pkg info',
	      code: 'EMAPWORKSPACESPKG',
	    })
	  }
	  if (!opts.cwd) {
	    opts.cwd = process.cwd();
	  }

	  const { workspaces = [] } = opts.pkg;
	  const { patterns, negatedPatterns } = getPatterns(workspaces);
	  const results = new Map();

	  if (!patterns.length && !negatedPatterns.length) {
	    return results
	  }

	  const seen = new Map();
	  const getGlobOpts = () => ({
	    ...opts,
	    ignore: [
	      ...opts.ignore || [],
	      '**/node_modules/**',
	      // just ignore the negated patterns to avoid unnecessary crawling
	      ...negatedPatterns,
	    ],
	  });

	  let matches = await glob(patterns.map((p) => getGlobPattern(p)), getGlobOpts());
	  // preserves glob@8 behavior
	  matches = matches.sort((a, b) => a.localeCompare(b, 'en'));

	  // we must preserve the order of results according to the given list of
	  // workspace patterns
	  const orderedMatches = [];
	  for (const pattern of patterns) {
	    orderedMatches.push(...matches.filter((m) => {
	      return minimatch(m, pattern, { partial: true, windowsPathsNoEscape: true })
	    }));
	  }

	  for (const match of orderedMatches) {
	    let pkg;
	    try {
	      pkg = await pkgJson.normalize(path.join(opts.cwd, match));
	    } catch (err) {
	      if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {
	        continue
	      } else {
	        throw err
	      }
	    }

	    const name = getPackageName(pkg.content, pkg.path);

	    let seenPackagePathnames = seen.get(name);
	    if (!seenPackagePathnames) {
	      seenPackagePathnames = new Set();
	      seen.set(name, seenPackagePathnames);
	    }
	    seenPackagePathnames.add(pkg.path);
	  }

	  const errorMessageArray = ['must not have multiple workspaces with the same name'];
	  for (const [packageName, seenPackagePathnames] of seen) {
	    if (seenPackagePathnames.size > 1) {
	      addDuplicateErrorMessages(errorMessageArray, packageName, seenPackagePathnames);
	    } else {
	      results.set(packageName, seenPackagePathnames.values().next().value);
	    }
	  }

	  if (errorMessageArray.length > 1) {
	    throw getError({
	      Type: Error,
	      message: errorMessageArray.join('\n'),
	      code: 'EDUPLICATEWORKSPACE',
	    })
	  }

	  return results
	}

	function addDuplicateErrorMessages (messageArray, packageName, packagePathnames) {
	  messageArray.push(
	    `package '${packageName}' has conflicts in the following paths:`
	  );

	  for (const packagePathname of packagePathnames) {
	    messageArray.push(
	      '    ' + packagePathname
	    );
	  }
	}

	mapWorkspaces.virtual = function (opts = {}) {
	  if (!opts || !opts.lockfile) {
	    throw getError({
	      message: 'mapWorkspaces.virtual missing lockfile info',
	      code: 'EMAPWORKSPACESLOCKFILE',
	    })
	  }
	  if (!opts.cwd) {
	    opts.cwd = process.cwd();
	  }

	  const { packages = {} } = opts.lockfile;
	  const { workspaces = [] } = packages[''] || {};
	  // uses a pathname-keyed map in order to negate the exact items
	  const results = new Map();
	  const { patterns, negatedPatterns } = getPatterns(workspaces);
	  if (!patterns.length && !negatedPatterns.length) {
	    return results
	  }
	  negatedPatterns.push('**/node_modules/**');

	  const packageKeys = Object.keys(packages);
	  for (const pattern of negatedPatterns) {
	    for (const packageKey of minimatch.match(packageKeys, pattern)) {
	      packageKeys.splice(packageKeys.indexOf(packageKey), 1);
	    }
	  }

	  for (const pattern of patterns) {
	    for (const packageKey of minimatch.match(packageKeys, pattern)) {
	      const packagePathname = path.join(opts.cwd, packageKey);
	      const name = getPackageName(packages[packageKey], packagePathname);
	      results.set(packagePathname, name);
	    }
	  }

	  // Invert pathname-keyed to a proper name-to-pathnames Map
	  return reverseResultMap(results)
	};

	lib$1 = mapWorkspaces;
	return lib$1;
}

var lib;
var hasRequiredLib;

function requireLib () {
	if (hasRequiredLib) return lib;
	hasRequiredLib = 1;
	// TODO: set the scope config from package.json or explicit cli config
	const { walkUp } = requireCommonjs();
	const ini = requireIni();
	const nopt = requireNopt();
	const { log, time } = requireLib$c();

	const { resolve, dirname, join } = require$$2$2;
	const { homedir } = require$$1$3;
	const {
	  readFile,
	  writeFile,
	  chmod,
	  unlink,
	  stat,
	  mkdir,
	} = require$$5;

	const fileExists = (...p) => stat(resolve(...p))
	  .then((st) => st.isFile())
	  .catch(() => false);

	const dirExists = (...p) => stat(resolve(...p))
	  .then((st) => st.isDirectory())
	  .catch(() => false);

	const hasOwnProperty = (obj, key) =>
	  Object.prototype.hasOwnProperty.call(obj, key);

	const typeDefs = requireTypeDefs();
	const nerfDart = requireNerfDart();
	const envReplace = requireEnvReplace();
	const parseField = requireParseField();
	const setEnvs = requireSetEnvs();

	// types that can be saved back to
	const confFileTypes = new Set([
	  'global',
	  'user',
	  'project',
	]);

	const confTypes = new Set([
	  'default',
	  'builtin',
	  ...confFileTypes,
	  'env',
	  'cli',
	]);

	class Config {
	  #loaded = false
	  #flatten
	  // populated the first time we flatten the object
	  #flatOptions = null

	  static get typeDefs () {
	    return typeDefs
	  }

	  constructor ({
	    definitions,
	    shorthands,
	    flatten,
	    npmPath,

	    // options just to override in tests, mostly
	    env = process.env,
	    argv = process.argv,
	    platform = process.platform,
	    execPath = process.execPath,
	    cwd = process.cwd(),
	    excludeNpmCwd = false,
	  }) {
	    // turn the definitions into nopt's weirdo syntax
	    this.definitions = definitions;
	    const types = {};
	    const defaults = {};
	    this.deprecated = {};
	    for (const [key, def] of Object.entries(definitions)) {
	      defaults[key] = def.default;
	      types[key] = def.type;
	      if (def.deprecated) {
	        this.deprecated[key] = def.deprecated.trim().replace(/\n +/, '\n');
	      }
	    }

	    this.#flatten = flatten;
	    this.types = types;
	    this.shorthands = shorthands;
	    this.defaults = defaults;

	    this.npmPath = npmPath;
	    this.npmBin = join(this.npmPath, 'bin/npm-cli.js');
	    this.argv = argv;
	    this.env = env;
	    this.execPath = execPath;
	    this.platform = platform;
	    this.cwd = cwd;
	    this.excludeNpmCwd = excludeNpmCwd;

	    // set when we load configs
	    this.globalPrefix = null;
	    this.localPrefix = null;
	    this.localPackage = null;

	    // defaults to env.HOME, but will always be *something*
	    this.home = null;

	    // set up the prototype chain of config objects
	    const wheres = [...confTypes];
	    this.data = new Map();
	    let parent = null;
	    for (const where of wheres) {
	      this.data.set(where, parent = new ConfigData(parent));
	    }

	    this.data.set = () => {
	      throw new Error('cannot change internal config data structure')
	    };
	    this.data.delete = () => {
	      throw new Error('cannot change internal config data structure')
	    };

	    this.sources = new Map([]);

	    this.list = [];
	    for (const { data } of this.data.values()) {
	      this.list.unshift(data);
	    }
	    Object.freeze(this.list);

	    this.#loaded = false;
	  }

	  get loaded () {
	    return this.#loaded
	  }

	  get prefix () {
	    return this.#get('global') ? this.globalPrefix : this.localPrefix
	  }

	  // return the location where key is found.
	  find (key) {
	    if (!this.loaded) {
	      throw new Error('call config.load() before reading values')
	    }

	    // have to look in reverse order
	    const entries = [...this.data.entries()];
	    for (let i = entries.length - 1; i > -1; i--) {
	      const [where, { data }] = entries[i];
	      if (hasOwnProperty(data, key)) {
	        return where
	      }
	    }
	    return null
	  }

	  get (key, where) {
	    if (!this.loaded) {
	      throw new Error('call config.load() before reading values')
	    }
	    return this.#get(key, where)
	  }

	  // we need to get values sometimes, so use this internal one to do so
	  // while in the process of loading.
	  #get (key, where = null) {
	    if (where !== null && !confTypes.has(where)) {
	      throw new Error('invalid config location param: ' + where)
	    }
	    const { data } = this.data.get(where || 'cli');
	    return where === null || hasOwnProperty(data, key) ? data[key] : undefined
	  }

	  set (key, val, where = 'cli') {
	    if (!this.loaded) {
	      throw new Error('call config.load() before setting values')
	    }
	    if (!confTypes.has(where)) {
	      throw new Error('invalid config location param: ' + where)
	    }
	    this.#checkDeprecated(key);
	    const { data, raw } = this.data.get(where);
	    data[key] = val;
	    if (['global', 'user', 'project'].includes(where)) {
	      raw[key] = val;
	    }

	    // this is now dirty, the next call to this.valid will have to check it
	    this.data.get(where)[_valid] = null;

	    // the flat options are invalidated, regenerate next time they're needed
	    this.#flatOptions = null;
	  }

	  get flat () {
	    if (this.#flatOptions) {
	      return this.#flatOptions
	    }

	    // create the object for flat options passed to deps
	    const timeEnd = time.start('config:load:flatten');
	    this.#flatOptions = {};
	    // walk from least priority to highest
	    for (const { data } of this.data.values()) {
	      this.#flatten(data, this.#flatOptions);
	    }
	    this.#flatOptions.nodeBin = this.execPath;
	    this.#flatOptions.npmBin = this.npmBin;
	    timeEnd();

	    return this.#flatOptions
	  }

	  delete (key, where = 'cli') {
	    if (!this.loaded) {
	      throw new Error('call config.load() before deleting values')
	    }
	    if (!confTypes.has(where)) {
	      throw new Error('invalid config location param: ' + where)
	    }
	    const { data, raw } = this.data.get(where);
	    delete data[key];
	    if (['global', 'user', 'project'].includes(where)) {
	      delete raw[key];
	    }
	  }

	  async load () {
	    if (this.loaded) {
	      throw new Error('attempting to load npm config multiple times')
	    }

	    // first load the defaults, which sets the global prefix
	    this.loadDefaults();

	    // next load the builtin config, as this sets new effective defaults
	    await this.loadBuiltinConfig();

	    // cli and env are not async, and can set the prefix, relevant to project
	    this.loadCLI();
	    this.loadEnv();

	    // next project config, which can affect userconfig location
	    await this.loadProjectConfig();

	    // then user config, which can affect globalconfig location
	    await this.loadUserConfig();

	    // last but not least, global config file
	    await this.loadGlobalConfig();

	    // set this before calling setEnvs, so that we don't have to share
	    // private attributes, as that module also does a bunch of get operations
	    this.#loaded = true;

	    // set proper globalPrefix now that everything is loaded
	    this.globalPrefix = this.get('prefix');

	    this.setEnvs();
	  }

	  loadDefaults () {
	    this.loadGlobalPrefix();
	    this.loadHome();

	    const defaultsObject = {
	      ...this.defaults,
	      prefix: this.globalPrefix,
	    };

	    try {
	      defaultsObject['npm-version'] = commonjsRequire(join(this.npmPath, 'package.json')).version;
	    } catch {
	      // in some weird state where the passed in npmPath does not have a package.json
	      // this will never happen in npm, but is guarded here in case this is consumed
	      // in other ways + tests
	    }

	    this.#loadObject(defaultsObject, 'default', 'default values');

	    const { data } = this.data.get('default');

	    // if the prefix is set on cli, env, or userconfig, then we need to
	    // default the globalconfig file to that location, instead of the default
	    // global prefix.  It's weird that `npm get globalconfig --prefix=/foo`
	    // returns `/foo/etc/npmrc`, but better to not change it at this point.
	    // define a custom getter, but turn into a normal prop
	    // if we set it.  otherwise it can't be set on child objects
	    Object.defineProperty(data, 'globalconfig', {
	      get: () => resolve(this.#get('prefix'), 'etc/npmrc'),
	      set (value) {
	        Object.defineProperty(data, 'globalconfig', {
	          value,
	          configurable: true,
	          writable: true,
	          enumerable: true,
	        });
	      },
	      configurable: true,
	      enumerable: true,
	    });
	  }

	  loadHome () {
	    this.home = this.env.HOME || homedir();
	  }

	  loadGlobalPrefix () {
	    if (this.globalPrefix) {
	      throw new Error('cannot load default global prefix more than once')
	    }

	    if (this.env.PREFIX) {
	      this.globalPrefix = this.env.PREFIX;
	    } else if (this.platform === 'win32') {
	      // c:\node\node.exe --> prefix=c:\node\
	      this.globalPrefix = dirname(this.execPath);
	    } else {
	      // /usr/local/bin/node --> prefix=/usr/local
	      this.globalPrefix = dirname(dirname(this.execPath));

	      // destdir only is respected on Unix
	      if (this.env.DESTDIR) {
	        this.globalPrefix = join(this.env.DESTDIR, this.globalPrefix);
	      }
	    }
	  }

	  loadEnv () {
	    const conf = Object.create(null);
	    for (const [envKey, envVal] of Object.entries(this.env)) {
	      if (!/^npm_config_/i.test(envKey) || envVal === '') {
	        continue
	      }
	      let key = envKey.slice('npm_config_'.length);
	      if (!key.startsWith('//')) { // don't normalize nerf-darted keys
	        key = key.replace(/(?!^)_/g, '-') // don't replace _ at the start of the key
	          .toLowerCase();
	      }
	      conf[key] = envVal;
	    }
	    this.#loadObject(conf, 'env', 'environment');
	  }

	  loadCLI () {
	    nopt.invalidHandler = (k, val, type) =>
	      this.invalidHandler(k, val, type, 'command line options', 'cli');
	    const conf = nopt(this.types, this.shorthands, this.argv);
	    nopt.invalidHandler = null;
	    this.parsedArgv = conf.argv;
	    delete conf.argv;
	    this.#loadObject(conf, 'cli', 'command line options');
	  }

	  get valid () {
	    for (const [where, { valid }] of this.data.entries()) {
	      if (valid === false || valid === null && !this.validate(where)) {
	        return false
	      }
	    }
	    return true
	  }

	  validate (where) {
	    if (!where) {
	      let valid = true;
	      const authProblems = [];

	      for (const entryWhere of this.data.keys()) {
	        // no need to validate our defaults, we know they're fine
	        // cli was already validated when parsed the first time
	        if (entryWhere === 'default' || entryWhere === 'builtin' || entryWhere === 'cli') {
	          continue
	        }
	        const ret = this.validate(entryWhere);
	        valid = valid && ret;

	        if (['global', 'user', 'project'].includes(entryWhere)) {
	          // after validating everything else, we look for old auth configs we no longer support
	          // if these keys are found, we build up a list of them and the appropriate action and
	          // attach it as context on the thrown error

	          // first, keys that should be removed
	          for (const key of ['_authtoken', '-authtoken']) {
	            if (this.get(key, entryWhere)) {
	              authProblems.push({ action: 'delete', key, where: entryWhere });
	            }
	          }

	          // NOTE we pull registry without restricting to the current 'where' because we want to
	          // suggest scoping things to the registry they would be applied to, which is the default
	          // regardless of where it was defined
	          const nerfedReg = nerfDart(this.get('registry'));
	          // keys that should be nerfed but currently are not
	          for (const key of ['_auth', '_authToken', 'username', '_password']) {
	            if (this.get(key, entryWhere)) {
	              // username and _password must both exist in the same file to be recognized correctly
	              if (key === 'username' && !this.get('_password', entryWhere)) {
	                authProblems.push({ action: 'delete', key, where: entryWhere });
	              } else if (key === '_password' && !this.get('username', entryWhere)) {
	                authProblems.push({ action: 'delete', key, where: entryWhere });
	              } else {
	                authProblems.push({
	                  action: 'rename',
	                  from: key,
	                  to: `${nerfedReg}:${key}`,
	                  where: entryWhere,
	                });
	              }
	            }
	          }
	        }
	      }

	      if (authProblems.length) {
	        const { ErrInvalidAuth } = requireErrors$1();
	        throw new ErrInvalidAuth(authProblems)
	      }

	      return valid
	    } else {
	      const obj = this.data.get(where);
	      obj[_valid] = true;

	      nopt.invalidHandler = (k, val, type) =>
	        this.invalidHandler(k, val, type, obj.source, where);

	      nopt.clean(obj.data, this.types, typeDefs);

	      nopt.invalidHandler = null;
	      return obj[_valid]
	    }
	  }

	  // fixes problems identified by validate(), accepts the 'problems' property from a thrown
	  // ErrInvalidAuth to avoid having to check everything again
	  repair (problems) {
	    if (!problems) {
	      try {
	        this.validate();
	      } catch (err) {
	        // coverage skipped here because we don't need to test re-throwing an error
	        // istanbul ignore next
	        if (err.code !== 'ERR_INVALID_AUTH') {
	          throw err
	        }

	        problems = err.problems;
	      } finally {
	        if (!problems) {
	          problems = [];
	        }
	      }
	    }

	    for (const problem of problems) {
	      // coverage disabled for else branch because it doesn't do anything and shouldn't
	      // istanbul ignore else
	      if (problem.action === 'delete') {
	        this.delete(problem.key, problem.where);
	      } else if (problem.action === 'rename') {
	        const raw = this.data.get(problem.where).raw?.[problem.from];
	        const calculated = this.get(problem.from, problem.where);
	        this.set(problem.to, raw || calculated, problem.where);
	        this.delete(problem.from, problem.where);
	      }
	    }
	  }

	  // Returns true if the value is coming directly from the source defined
	  // in default definitions, if the current value for the key config is
	  // coming from any other different source, returns false
	  isDefault (key) {
	    const [defaultType, ...types] = [...confTypes];
	    const defaultData = this.data.get(defaultType).data;

	    return hasOwnProperty(defaultData, key)
	      && types.every(type => {
	        const typeData = this.data.get(type).data;
	        return !hasOwnProperty(typeData, key)
	      })
	  }

	  invalidHandler (k, val, type, source, where) {
	    const typeDescription = requireTypeDescription();
	    log.warn(
	      'invalid config',
	      k + '=' + JSON.stringify(val),
	      `set in ${source}`
	    );
	    this.data.get(where)[_valid] = false;

	    if (Array.isArray(type)) {
	      if (type.includes(typeDefs.url.type)) {
	        type = typeDefs.url.type;
	      } else {
	        /* istanbul ignore if - no actual configs matching this, but
	         * path types SHOULD be handled this way, like URLs, for the
	         * same reason */
	        if (type.includes(typeDefs.path.type)) {
	          type = typeDefs.path.type;
	        }
	      }
	    }

	    const typeDesc = typeDescription(type);
	    const mustBe = typeDesc
	      .filter(m => m !== undefined && m !== Array);
	    const msg = 'Must be' + this.#getOneOfKeywords(mustBe, typeDesc);
	    const desc = mustBe.length === 1 ? mustBe[0]
	      : [...new Set(mustBe.map(n => typeof n === 'string' ? n : JSON.stringify(n)))].join(', ');
	    log.warn('invalid config', msg, desc);
	  }

	  #getOneOfKeywords (mustBe, typeDesc) {
	    let keyword;
	    if (mustBe.length === 1 && typeDesc.includes(Array)) {
	      keyword = ' one or more';
	    } else if (mustBe.length > 1 && typeDesc.includes(Array)) {
	      keyword = ' one or more of:';
	    } else if (mustBe.length > 1) {
	      keyword = ' one of:';
	    } else {
	      keyword = '';
	    }
	    return keyword
	  }

	  #loadObject (obj, where, source, er = null) {
	    // obj is the raw data read from the file
	    const conf = this.data.get(where);
	    if (conf.source) {
	      const m = `double-loading "${where}" configs from ${source}, ` +
	        `previously loaded from ${conf.source}`;
	      throw new Error(m)
	    }

	    if (this.sources.has(source)) {
	      const m = `double-loading config "${source}" as "${where}", ` +
	        `previously loaded as "${this.sources.get(source)}"`;
	      throw new Error(m)
	    }

	    conf.source = source;
	    this.sources.set(source, where);
	    if (er) {
	      conf.loadError = er;
	      if (er.code !== 'ENOENT') {
	        log.verbose('config', `error loading ${where} config`, er);
	      }
	    } else {
	      conf.raw = obj;
	      for (const [key, value] of Object.entries(obj)) {
	        const k = envReplace(key, this.env);
	        const v = this.parseField(value, k);
	        if (where !== 'default') {
	          this.#checkDeprecated(k);
	          if (this.definitions[key]?.exclusive) {
	            for (const exclusive of this.definitions[key].exclusive) {
	              if (!this.isDefault(exclusive)) {
	                throw new TypeError(`--${key} can not be provided when using --${exclusive}`)
	              }
	            }
	          }
	        }
	        conf.data[k] = v;
	      }
	    }
	  }

	  #checkDeprecated (key) {
	    // XXX(npm9+) make this throw an error
	    if (this.deprecated[key]) {
	      log.warn('config', key, this.deprecated[key]);
	    }
	  }

	  // Parse a field, coercing it to the best type available.
	  parseField (f, key, listElement = false) {
	    return parseField(f, key, this, listElement)
	  }

	  async #loadFile (file, type) {
	    // only catch the error from readFile, not from the loadObject call
	    log.silly('config', `load:file:${file}`);
	    await readFile(file, 'utf8').then(
	      data => {
	        const parsedConfig = ini.parse(data);
	        if (type === 'project' && parsedConfig.prefix) {
	          // Log error if prefix is mentioned in project .npmrc
	          /* eslint-disable-next-line max-len */
	          log.error('config', `prefix cannot be changed from project config: ${file}.`);
	        }
	        return this.#loadObject(parsedConfig, type, file)
	      },
	      er => this.#loadObject(null, type, file, er)
	    );
	  }

	  loadBuiltinConfig () {
	    return this.#loadFile(resolve(this.npmPath, 'npmrc'), 'builtin')
	  }

	  async loadProjectConfig () {
	    // the localPrefix can be set by the CLI config, but otherwise is
	    // found by walking up the folder tree. either way, we load it before
	    // we return to make sure localPrefix is set
	    await this.loadLocalPrefix();

	    // if we have not detected a local package json yet, try now that we
	    // have a local prefix
	    if (this.localPackage == null) {
	      this.localPackage = await fileExists(this.localPrefix, 'package.json');
	    }

	    if (this.#get('global') === true || this.#get('location') === 'global') {
	      this.data.get('project').source = '(global mode enabled, ignored)';
	      this.sources.set(this.data.get('project').source, 'project');
	      return
	    }

	    const projectFile = resolve(this.localPrefix, '.npmrc');
	    // if we're in the ~ directory, and there happens to be a node_modules
	    // folder (which is not TOO uncommon, it turns out), then we can end
	    // up loading the "project" config where the "userconfig" will be,
	    // which causes some calamaties.  So, we only load project config if
	    // it doesn't match what the userconfig will be.
	    if (projectFile !== this.#get('userconfig')) {
	      return this.#loadFile(projectFile, 'project')
	    } else {
	      this.data.get('project').source = '(same as "user" config, ignored)';
	      this.sources.set(this.data.get('project').source, 'project');
	    }
	  }

	  async loadLocalPrefix () {
	    const cliPrefix = this.#get('prefix', 'cli');
	    if (cliPrefix) {
	      this.localPrefix = cliPrefix;
	      return
	    }

	    const cliWorkspaces = this.#get('workspaces', 'cli');
	    const isGlobal = this.#get('global') || this.#get('location') === 'global';

	    for (const p of walkUp(this.cwd)) {
	      // HACK: this is an option set in tests to stop the local prefix from being set
	      // on tests that are created inside the npm repo
	      if (this.excludeNpmCwd && p === this.npmPath) {
	        break
	      }

	      const hasPackageJson = await fileExists(p, 'package.json');

	      if (!this.localPrefix && (hasPackageJson || await dirExists(p, 'node_modules'))) {
	        this.localPrefix = p;
	        this.localPackage = hasPackageJson;

	        // if workspaces are disabled, or we're in global mode, return now
	        if (cliWorkspaces === false || isGlobal) {
	          return
	        }

	        // otherwise, continue the loop
	        continue
	      }

	      if (this.localPrefix && hasPackageJson) {
	        const pkgJson = requireLib$3();
	        // if we already set localPrefix but this dir has a package.json
	        // then we need to see if `p` is a workspace root by reading its package.json
	        // however, if reading it fails then we should just move on
	        const { content: pkg } = await pkgJson.normalize(p).catch(() => ({ content: {} }));
	        if (!pkg?.workspaces) {
	          continue
	        }

	        const mapWorkspaces = requireLib$1();
	        const workspaces = await mapWorkspaces({ cwd: p, pkg });
	        for (const w of workspaces.values()) {
	          if (w === this.localPrefix) {
	            // see if there's a .npmrc file in the workspace, if so log a warning
	            if (await fileExists(this.localPrefix, '.npmrc')) {
	              log.warn('config', `ignoring workspace config at ${this.localPrefix}/.npmrc`);
	            }

	            // set the workspace in the default layer, which allows it to be overridden easily
	            const { data } = this.data.get('default');
	            data.workspace = [this.localPrefix];
	            this.localPrefix = p;
	            this.localPackage = hasPackageJson;
	            log.info('config', `found workspace root at ${this.localPrefix}`);
	            // we found a root, so we return now
	            return
	          }
	        }
	      }
	    }

	    if (!this.localPrefix) {
	      this.localPrefix = this.cwd;
	    }
	  }

	  loadUserConfig () {
	    return this.#loadFile(this.#get('userconfig'), 'user')
	  }

	  loadGlobalConfig () {
	    return this.#loadFile(this.#get('globalconfig'), 'global')
	  }

	  async save (where) {
	    if (!this.loaded) {
	      throw new Error('call config.load() before saving')
	    }
	    if (!confFileTypes.has(where)) {
	      throw new Error('invalid config location param: ' + where)
	    }

	    const conf = this.data.get(where);
	    conf[_loadError] = null;

	    if (where === 'user') {
	      // if email is nerfed, then we want to de-nerf it
	      const nerfed = nerfDart(this.get('registry'));
	      const email = this.get(`${nerfed}:email`, 'user');
	      if (email) {
	        this.delete(`${nerfed}:email`, 'user');
	        this.set('email', email, 'user');
	      }
	    }

	    // We need the actual raw data before we called parseField so that we are
	    // saving the same content back to the file
	    const iniData = ini.stringify(conf.raw).trim() + '\n';
	    if (!iniData.trim()) {
	      // ignore the unlink error (eg, if file doesn't exist)
	      await unlink(conf.source).catch(() => {});
	      return
	    }
	    const dir = dirname(conf.source);
	    await mkdir(dir, { recursive: true });
	    await writeFile(conf.source, iniData, 'utf8');
	    const mode = where === 'user' ? 0o600 : 0o666;
	    await chmod(conf.source, mode);
	  }

	  clearCredentialsByURI (uri, level = 'user') {
	    const nerfed = nerfDart(uri);
	    const def = nerfDart(this.get('registry'));
	    if (def === nerfed) {
	      this.delete(`-authtoken`, level);
	      this.delete(`_authToken`, level);
	      this.delete(`_authtoken`, level);
	      this.delete(`_auth`, level);
	      this.delete(`_password`, level);
	      this.delete(`username`, level);
	      // de-nerf email if it's nerfed to the default registry
	      const email = this.get(`${nerfed}:email`, level);
	      if (email) {
	        this.set('email', email, level);
	      }
	    }
	    this.delete(`${nerfed}:_authToken`, level);
	    this.delete(`${nerfed}:_auth`, level);
	    this.delete(`${nerfed}:_password`, level);
	    this.delete(`${nerfed}:username`, level);
	    this.delete(`${nerfed}:email`, level);
	    this.delete(`${nerfed}:certfile`, level);
	    this.delete(`${nerfed}:keyfile`, level);
	  }

	  setCredentialsByURI (uri, { token, username, password, certfile, keyfile }) {
	    const nerfed = nerfDart(uri);

	    // field that hasn't been used as documented for a LONG time,
	    // and as of npm 7.10.0, isn't used at all.  We just always
	    // send auth if we have it, only to the URIs under the nerf dart.
	    this.delete(`${nerfed}:always-auth`, 'user');

	    this.delete(`${nerfed}:email`, 'user');
	    if (certfile && keyfile) {
	      this.set(`${nerfed}:certfile`, certfile, 'user');
	      this.set(`${nerfed}:keyfile`, keyfile, 'user');
	      // cert/key may be used in conjunction with other credentials, thus no `else`
	    }
	    if (token) {
	      this.set(`${nerfed}:_authToken`, token, 'user');
	      this.delete(`${nerfed}:_password`, 'user');
	      this.delete(`${nerfed}:username`, 'user');
	    } else if (username || password) {
	      if (!username) {
	        throw new Error('must include username')
	      }
	      if (!password) {
	        throw new Error('must include password')
	      }
	      this.delete(`${nerfed}:_authToken`, 'user');
	      this.set(`${nerfed}:username`, username, 'user');
	      // note: not encrypted, no idea why we bothered to do this, but oh well
	      // protects against shoulder-hacks if password is memorable, I guess?
	      const encoded = Buffer.from(password, 'utf8').toString('base64');
	      this.set(`${nerfed}:_password`, encoded, 'user');
	    } else if (!certfile || !keyfile) {
	      throw new Error('No credentials to set.')
	    }
	  }

	  // this has to be a bit more complicated to support legacy data of all forms
	  getCredentialsByURI (uri) {
	    const nerfed = nerfDart(uri);
	    const def = nerfDart(this.get('registry'));
	    const creds = {};

	    // email is handled differently, it used to always be nerfed and now it never should be
	    // if it's set nerfed to the default registry, then we copy it to the unnerfed key
	    // TODO: evaluate removing 'email' from the credentials object returned here
	    const email = this.get(`${nerfed}:email`) || this.get('email');
	    if (email) {
	      if (nerfed === def) {
	        this.set('email', email, 'user');
	      }
	      creds.email = email;
	    }

	    const certfileReg = this.get(`${nerfed}:certfile`);
	    const keyfileReg = this.get(`${nerfed}:keyfile`);
	    if (certfileReg && keyfileReg) {
	      creds.certfile = certfileReg;
	      creds.keyfile = keyfileReg;
	      // cert/key may be used in conjunction with other credentials, thus no `return`
	    }

	    const tokenReg = this.get(`${nerfed}:_authToken`);
	    if (tokenReg) {
	      creds.token = tokenReg;
	      return creds
	    }

	    const userReg = this.get(`${nerfed}:username`);
	    const passReg = this.get(`${nerfed}:_password`);
	    if (userReg && passReg) {
	      creds.username = userReg;
	      creds.password = Buffer.from(passReg, 'base64').toString('utf8');
	      const auth = `${creds.username}:${creds.password}`;
	      creds.auth = Buffer.from(auth, 'utf8').toString('base64');
	      return creds
	    }

	    const authReg = this.get(`${nerfed}:_auth`);
	    if (authReg) {
	      const authDecode = Buffer.from(authReg, 'base64').toString('utf8');
	      const authSplit = authDecode.split(':');
	      creds.username = authSplit.shift();
	      creds.password = authSplit.join(':');
	      creds.auth = authReg;
	      return creds
	    }

	    // at this point, nothing else is usable so just return what we do have
	    return creds
	  }

	  // set up the environment object we have with npm_config_* environs
	  // for all configs that are different from their default values, and
	  // set EDITOR and HOME.
	  setEnvs () {
	    setEnvs(this);
	  }
	}

	const _loadError = Symbol('loadError');
	const _valid = Symbol('valid');

	class ConfigData {
	  #data
	  #source = null
	  #raw = null
	  constructor (parent) {
	    this.#data = Object.create(parent && parent.data);
	    this.#raw = {};
	    this[_valid] = true;
	  }

	  get data () {
	    return this.#data
	  }

	  get valid () {
	    return this[_valid]
	  }

	  set source (s) {
	    if (this.#source) {
	      throw new Error('cannot set ConfigData source more than once')
	    }
	    this.#source = s;
	  }

	  get source () {
	    return this.#source
	  }

	  set loadError (e) {
	    if (this[_loadError] || (Object.keys(this.#raw).length)) {
	      throw new Error('cannot set ConfigData loadError after load')
	    }
	    this[_loadError] = e;
	  }

	  get loadError () {
	    return this[_loadError]
	  }

	  set raw (r) {
	    if (Object.keys(this.#raw).length || this[_loadError]) {
	      throw new Error('cannot set ConfigData raw after load')
	    }
	    this.#raw = r;
	  }

	  get raw () {
	    return this.#raw
	  }
	}

	lib = Config;
	return lib;
}

var libExports = requireLib();
const index = /*@__PURE__*/getDefaultExportFromCjs(libExports);

const index$1 = {
	__proto__: null,
	default: index
};

export { index$1 as i };
